[{"path":"index.html","id":"prefÃ¡cio","chapter":"PrefÃ¡cio","heading":"PrefÃ¡cio","text":"Atualmente na Ã¡rea das CiÃªncias AgrÃ¡rias identificam-se o uso de diversos sofwares para anÃ¡lise estatÃ­stica de dados originados em coletas de experimentos. Esta miscelÃ¢nea de sofwares pode confundir o pesquisador momento de escolher qual Ã© o software que serÃ¡ adotado para suas anÃ¡lises estatÃ­sticas, jÃ¡ que existem aqueles que devem ser adquiridas licenÃ§para uso e nem todos disponibilizam opÃ§Ãµes de todos os mÃ©todos de anÃ¡lise estatÃ­stica de dados.Dentre esses o Software R destaca-se por ser uma linguagem de programaÃ§Ã£o de cÃ³digo aberto open source basicamente destinado para computaÃ§Ã£o estatÃ­stica e grÃ¡ficos. Com proposta de organizaÃ§Ã£o de um curso e capacitaÃ§Ã£o de acadÃªmicos e professores envolvidos em PÃ³s-GraduaÃ§Ã£o na Ãrea de CiÃªncias AgrÃ¡rias, os Drs. Bruno Giacomini Sari e Tiago Olivoto propuseram-se elaborar um documento onde oferecem uma excelente apresentaÃ§Ã£o e introduÃ§Ã£o ao ambiente R, bem como diversas aplicaÃ§Ãµes de abordagens estatÃ­sticas em experimentos agrÃ­colas.Em sua segunda ediÃ§Ã£o ampliada e atualizada apresentam-se variaÃ§Ãµes nos tipos de tratamentos (qualitativos e quantitativos), variaÃ§Ãµes nos desdobramentos das interaÃ§Ãµes e variaÃ§Ãµes nas formas da casualizaÃ§Ã£o de experimentos bifatoriais. Uma breve abordagem ao uso de modelos lineares generalizados Ã© apresentada. TÃ©cnicas biomÃ©tricas voltadas ao melhoramento genÃ©tico vegetal como anÃ¡lise conjunta de experimentos, anÃ¡lise de estabilidade e associaÃ§Ãµes entre variÃ¡veis ou grupo de variÃ¡veis sÃ£o tambÃ©m abordadas. Todos os exemplos sÃ£o reproduzÃ­veis. expectativa Ã© de que este documento seja Ãºtil para aqueles pesquisadores que desejam utilizar este ambiente de programaÃ§Ã£o para realizaÃ§Ã£o de suas anÃ¡lises estatÃ­sticasParabenizamos os autores pela iniciativa e qualidade material oferecido.Alessandro Dalâ€™Col LÃºcioProfessor Titular, Setor de ExperimentaÃ§Ã£o VegetalDepartamento de FitotecniaCentro de CiÃªncias RuraisUniversidade Federal de Santa Maria","code":""},{"path":"index.html","id":"por-que-eu-deveria-ler-este-e-book","chapter":"PrefÃ¡cio","heading":"Por que eu deveria ler este e-Book","text":"Com uma disponibilidade cada vez maior de bons softwares estatÃ­sticos, escolha por um Ãºnico programa se torna uma tarefa difÃ­cil, atÃ© mesmo para alguÃ©m com vasta experiÃªncia na Ã¡rea de anÃ¡lise de dados de experimentos agronÃ´micos. O ambiente de programaÃ§Ã£o R Ã©, tambÃ©m, um poderoso software estatÃ­stico. Assim, inÃºmeras sÃ£o fontes com informaÃ§Ãµes relacionadas anÃ¡lise de dados, criaÃ§Ã£o de grÃ¡ficos, etc.grande maioria dos blogs12345678 relacionados ao software R estÃ£o na lÃ­ngua Inglesa e mesmo que nos tempos atuais esta nÃ£o seja uma questÃ£o limitante, materiais de qualidade em lÃ­ngua Portuguesa sÃ£o muito bem-vindos. Por exemplo, R-br9 Ã© lista Brasileira oficial de discussÃ£o programa R e tem o propÃ³sito de permitir troca de informaÃ§Ãµes entre os usuÃ¡rios de R (em portuguÃªs) e contÃ©m inÃºmeras dicas/discussÃµes sobre mais diversas Ã¡reas de estudo.Esta material, voltado para anÃ¡lise de dados de experimentos agronÃ´micos, apresenta teoria e aplicaÃ§Ã£o software R dos procedimentos mais utilizados na anÃ¡lise de experimentos agronÃ´micos. Assim, ela pode servir de referÃªncia para aqueles que querem realizar suas anÃ¡lises R, principalmente para os que ainda possuem pouca ou nenhuma experiÃªncia com este ambiente de programaÃ§Ã£o.","code":""},{"path":"index.html","id":"estrutura","chapter":"PrefÃ¡cio","heading":"Estrutura","text":"Este material contÃ©m 14 capÃ­tulos divididos em 3 principais partes. Na parte (CapÃ­tulos 1 5 ) o ambiente R Ã© apresentado. O CapÃ­tulo 1, apresenta uma breve introduÃ§Ã£o sobre os softwares R e RStudio, mostrando como instalar e carregar os pacotes necessÃ¡rios, alÃ©m de mostrar ao leitor como criar seu primeiro script. O CapÃ­tulo 2 apresenta os tipos de objetos. CapÃ­tulo 3, principais operaÃ§Ãµes matemÃ¡ticas sÃ£o mostradas. CapÃ­tulo 4 Ã© mostrado como loops podem ser Ãºteis para repetir um determinado cÃ³digo diversas vezes. CapÃ­tulo 5 Ã© mostrado os dados podem ser armazenados em objetos com diferentes classes.parte II (CapÃ­tulos 6 9) Ã© voltada para organizaÃ§Ã£o, manipulaÃ§Ã£o e apresentaÃ§Ã£o grÃ¡fica de dados. CapÃ­tulo 6 Ã© mostrado diversos formatos de dados podem ser carregados ambiente R. O CapÃ­tulo 7 trata da manimulaÃ§Ã£o dos dados, tais como adiÃ§Ã£o, seleÃ§Ã£o, resumo e combinaÃ§Ã£o de variÃ¡veis. O CapÃ­tulo 8 trata da apresentaÃ§Ã£o dos dados utilizando diversos tipos de grÃ¡ficos, tais como barra, histogramas e grÃ¡ficos de dispersÃ£o. O CapÃ­tulo 9 Ã© voltado para exportaÃ§Ã£o dos dados, tanto numÃ©rico quanto grÃ¡ficos.parte III (CapÃ­tulos 10 14) Ã© voltada para anÃ¡lise dos dados. O CapÃ­tulo 10 trata da anÃ¡lise de dados experimentais, incluindo estatÃ­stica bÃ¡sica, anÃ¡lise descritiva, anÃ¡lise de experimentos uni- e bi-fatoriais considerando os principais delineamentos, transformaÃ§Ãµes de dados, anÃ¡lise de covariÃ¢ncia bem como uma breve abordagem ao uso de modelos lineares generalizados. O CapÃ­tulo 11 Ã© voltado exclusivamente para anÃ¡lise de regressÃ£o linear e nÃ£o linear. O CapÃ­tulo 12 trata da associaÃ§Ã£o entre variÃ¡veis tais como correlaÃ§Ã£o linear, correlaÃ§Ã£o parcial e anÃ¡lise de trilha. CapÃ­tulo 13 anÃ¡lise multivariada de dados Ã© apresentada. Por fim â€“mas nÃ£o menos importanteâ€“ CapÃ­tulo 14 sÃ£o apresentados diversos modelos para anÃ¡lise de ensaios multi-ambientes, com Ãªnfase na aplicaÃ§Ã£o dos mÃ©todos AMMI10, BLUP11 e GGE12.","code":""},{"path":"index.html","id":"conjunto-de-dados","chapter":"PrefÃ¡cio","heading":"Conjunto de dados","text":"Exemplos reproduzÃ­veis sÃ£o muito importante para uma curva de aprendizado satisfatÃ³ria ambiente de programaÃ§Ã£o R. Os leitores podem interagir com os exemplos deste material ao lÃª-lo. Por exemplo, Ã© possÃ­vel, utilizando Ctrl+C, copiar uma programaÃ§Ã£o, colar em seu ambiente de trabalho utilizando Ctrl+V e saber imediatamente o que acontece se certos parÃ¢metros/argumentos de um modelo/anÃ¡lise forem alterados. Todos os dados utilizados estÃ£o disponÃ­veis repositÃ³rio digital deste e-book. Os dados sÃ£o carregados ambiente R utilizando funÃ§Ã£o import() pacote rio.","code":""},{"path":"sobre-os-autores.html","id":"sobre-os-autores","chapter":"Sobre os autores","heading":"Sobre os autores","text":"","code":""},{"path":"sobre-os-autores.html","id":"tiago-olivoto","chapter":"Sobre os autores","heading":"Tiago Olivoto","text":"Tiago Olivoto Ã© TÃ©cnico AgrÃ­cola pela Escola Estadual de EducaÃ§Ã£o BÃ¡sica Viadutos (2008), Engenheiro agrÃ´nomo pela Universidade Oeste de Santa Catarina (2014), Mestre em Agronomia: Agricultura e Ambiente pela Universidade Federal de Santa Maria (2017) e Doutor em Agronomia com Ãªnfase em Melhoramento GenÃ©tico Vegetal e ExperimentaÃ§Ã£o AgrÃ­cola pela Universidade Federal de Santa Maria (2020). Tem experiÃªncia profissional como TÃ©cnico AgrÃ­cola (2008-2011), consultor tÃ©cnico de vendas (2012-2013), na administraÃ§Ã£o pÃºblica e gestÃ£o de pessoas (2014-2015), atuando como SecretÃ¡rio Municipal da Agricultura e Meio Ambiente municÃ­pio de Cacique Doble-RS. Foi professor (bolsista) Instituto Federal de EducaÃ§Ã£o, CiÃªncia e Tecnologia Rio Grande Sul, edital nÂº 271, de 17 de julho de 2014, atuando na aÃ§Ã£o Bolsa-FormaÃ§Ã£o Programa Nacional de Acesso ao Ensino TÃ©cnico e Emprego (PRONATEC), na Unidade Remota de Cacique Doble. Atualmente Ã© Professor Ensino Superior Instituto de Desenvolvimento Educacional Alto Uruguai (IDEAU), sendo membro NÃºcleo Docente Estruturante curso de Agronomia. Ã‰ membro atuante da International Biometric Society (IBS), American Society Agronomy (ASA), Crop Science Society America (CSSA) e da Soil Science Society America (SSSA). Ã‰ integrante da comissÃ£o de Jovens Pesquisadores da RegiÃ£o Brasileira da Sociedade Internacional de Biometria, RBras, (JP-RBras) representando os estados RS, SC e PR. Atua tambÃ©m como revisor ad hoc em revistas cientÃ­ficas nacionais e internacionais, sendo membro Conselho Editorial da revista Genetics Molecular Research. Exerce atividades de pesquisa relacionadas ao planejamento, conduÃ§Ã£o e avaliaÃ§Ã£o de experimentos com culturas anuais, com Ãªnfase desenvolvimento e aperfeiÃ§oamento de mÃ©todos estatÃ­stico-experimentais para avaliaÃ§Ã£o de ensaios multi-ambientes em melhoramento genÃ©tico de plantas. Em seu currÃ­culo, os termos mais frequentes na contextualizaÃ§Ã£o da produÃ§Ã£o cientÃ­fica sÃ£o: anÃ¡lise de ensaios multi-ambientes, Ã­ndices multivariados, intervalo de confianÃ§para correlaÃ§Ã£o, planejamento de experimentos, seleÃ§Ã£o indireta, interaÃ§Ã£o genÃ³tipo-vs-ambiente, modelos mistos e parÃ¢metros genÃ©ticos. Tem experiÃªncia com os softwares GÃªnes, GEA-R, R, SAS e SPSS. Desenvolveu o pacote para sofware R metan https://tiagoolivoto.github.io/metan/, voltado para checagem, manipulaÃ§Ã£o, anÃ¡lise e apresentaÃ§Ã£o de dados de ensaios multi-ambientes.Sua mais recente pesquisa publicada em uma sÃ©rie de dois artigos diz respeito ao desenvolvimento de um novo Ã­ndice de estabilidade para anÃ¡lise de ensaios multi-ambientes, bem comoo desenvolvimento de um Ã­ndice multivariado, para seleÃ§Ã£o de genÃ³tipos baseado na estabilidade ou performance e estabilidade quando diversas variÃ¡veis sÃ£o analizadas.Como um usuÃ¡rio ativo de R, Tiago desenvolveu o pacote metan, acrÃ´nimo para multi-environment trial analysis. O cÃ³digo fonte pode ser encontrado em sua pÃ¡gina GitHub.ContatosE-mail: tiagoolivoto@gmail.com | Curriculo Lattes | GitHub | ORCID | Research Gate | ResearcherID","code":""},{"path":"sobre-os-autores.html","id":"bruno-giacomini-sari","chapter":"Sobre os autores","heading":"Bruno Giacomini Sari","text":"Possui graduaÃ§Ã£o (2012), mestrado (2015) e doutorado (2018) em Agronomia pela Universidade Federal de Santa Maria - UFSM. Atualmente realiza estÃ¡gio pÃ³s doutoral junto ao Programa de PÃ³s GraduaÃ§Ã£o em Agronomia da UFSM. Tem experiÃªncia na Ã¡rea de estatÃ­stica, com enfase em experimentaÃ§Ã£o agrÃ­cola, atuando nos seguintes temas: probabilidade, amostragem, planejamento experimental, anÃ¡lise de regressÃ£o linear e nÃ£o linear.ContatosE-mail: brunosari@hotmail.com | Curriculo Lattes | ORCID | Research Gate","code":""},{"path":"detalhes-importantes.html","id":"detalhes-importantes","chapter":"Detalhes importantes","heading":"Detalhes importantes","text":"Ambiente de criaÃ§Ã£oEste e-book foi escrito em RMarkdown, usando o pacote bookdown e hospedado na web em GitHub. versÃ£o pdf deste material pode ser baixada aqui.CÃ³digo fonteO cÃ³digo fonte deste e-book pode ser encontrado neste repositÃ³rio GitHub. Para informar qualquer problema, por favor, crie um pull request.LicenÃ§aEste trabalho estÃ¡ licenciado com uma LicenÃ§Creative Commons - AtribuiÃ§Ã£o-NÃ£oComercial-CompartilhaIgual 4.0 Internacional. O resumo legÃ­vel da licenÃ§afirma que vocÃª tem o direito de:Compartilhar â€” copiar e redistribuir o material em qualquer suporte ou formatoCompartilhar â€” copiar e redistribuir o material em qualquer suporte ou formatoAdaptar â€” remixar, transformar, e criar partir materialAdaptar â€” remixar, transformar, e criar partir materialAtribuiÃ§Ã£o â€” VocÃª deve dar o crÃ©dito apropriado, prover um link para licenÃ§e indicar se mudanÃ§foram feitas. VocÃª deve fazÃª-lo em qualquer circunstÃ¢ncia razoÃ¡vel, mas de nenhuma maneira que sugira que o licenciante apoia vocÃª ou o seu uso.AtribuiÃ§Ã£o â€” VocÃª deve dar o crÃ©dito apropriado, prover um link para licenÃ§e indicar se mudanÃ§foram feitas. VocÃª deve fazÃª-lo em qualquer circunstÃ¢ncia razoÃ¡vel, mas de nenhuma maneira que sugira que o licenciante apoia vocÃª ou o seu uso.De acordo com os termos seguintes\r\nNÃ£o Comercial â€” VocÃª nÃ£o pode usar o material para fins comerciais.\r\nCompartilhaIgual â€” Se vocÃª remixar, transformar, ou criar partir material, tem de distribuir suas contribuiÃ§Ãµes sob mesma licenÃ§que o original.\r\nSem restriÃ§Ãµes adicionais â€” VocÃª nÃ£o pode aplicar termos jurÃ­dicos ou medidas de carÃ¡ter tecnolÃ³gico que restrinjam legalmente outros de fazerem algo que licenÃ§permita.\r\nDe acordo com os termos seguintesNÃ£o Comercial â€” VocÃª nÃ£o pode usar o material para fins comerciais.NÃ£o Comercial â€” VocÃª nÃ£o pode usar o material para fins comerciais.CompartilhaIgual â€” Se vocÃª remixar, transformar, ou criar partir material, tem de distribuir suas contribuiÃ§Ãµes sob mesma licenÃ§que o original.CompartilhaIgual â€” Se vocÃª remixar, transformar, ou criar partir material, tem de distribuir suas contribuiÃ§Ãµes sob mesma licenÃ§que o original.Sem restriÃ§Ãµes adicionais â€” VocÃª nÃ£o pode aplicar termos jurÃ­dicos ou medidas de carÃ¡ter tecnolÃ³gico que restrinjam legalmente outros de fazerem algo que licenÃ§permita.Sem restriÃ§Ãµes adicionais â€” VocÃª nÃ£o pode aplicar termos jurÃ­dicos ou medidas de carÃ¡ter tecnolÃ³gico que restrinjam legalmente outros de fazerem algo que licenÃ§permita.","code":""},{"path":"intro.html","id":"intro","chapter":"CapÃ­tulo 1 IntroduÃ§Ã£o ao ambiente R","heading":"CapÃ­tulo 1 IntroduÃ§Ã£o ao ambiente R","text":"","code":""},{"path":"intro.html","id":"o-software-r","chapter":"CapÃ­tulo 1 IntroduÃ§Ã£o ao ambiente R","heading":"1.1 O software R","text":"O artigo R: Language Data Analysis Graphics13 marca o inÃ­cio de uma nova era processamento e anÃ¡lise de dados: o desenvolvimento software R. O R Ã© uma linguagem e ambiente estatÃ­stico que traz muitas vantagens para o usuÃ¡rio, embora elas nÃ£o sejam tÃ£o obvias inicialmente: () o R Ã© um Software Livre (livre sentido de liberdade) distribuÃ­sob LicenÃ§PÃºblica Geral14, podendo ser livremente copiado, distribuÃ­, e instalado em diversos computadores livremente. Isso contrasta com softwares comerciais, que tÃªm licenÃ§altamente restritivas, que nÃ£o permitem que cÃ³pias sejam distribuÃ­das ou instaladas em mais de um computador sem devida licenÃ§(que obviamente Ã© paga!); (ii) grande maioria dos Softwares livres sÃ£o grÃ¡tis, e o R nÃ£o Ã© uma exceÃ§Ã£o; (iii) os cÃ³digos-fontes R estÃ£o disponÃ­veis para os usuÃ¡rios, e atualmente sÃ£o gerenciados por um grupo chamado R Development Core Team15. vantagem de ter o cÃ³digo aberto Ã© que falhas podem ser detectadas e rapidamente corrigidas. Este sistema de revisÃ£o depende da participaÃ§Ã£o dos usuÃ¡rios. Em contraste, em muitos pacotes comerciais, falhas nÃ£o sÃ£o corrigidas atÃ© o lanÃ§amento da prÃ³xima versÃ£o, o que pode levar vÃ¡rios anos; (iv) o R fornece um interface de entrada por linha de comando (ELC).software R, todos os comandos sÃ£o digitados e o mouse Ã© pouco usado. Pode parecer antigo, pouco amigÃ¡vel ou atÃ© pobre em recursos visuais, mas isso faz com que nos deparemos com o melhor recurso R: sua flexibilidade. Para usuÃ¡rios familiarizados, linguagem R se torna clara e simples. Com poucos comandos, funÃ§Ãµes poderosas podem ser criadas e o usuÃ¡rio Ã© sempre consciente que foi pedido atravÃ©s da ELC (Meus dados, minhas anÃ¡lises!). Isso contrasta com pacotes que possuem uma interface amigÃ¡vel (Windows-based), mas escondem dinÃ¢mica dos cÃ¡lculos e, potencialmente, os seus erros. Finalmente, o R fornece uma ampla variedade de procedimentos estatÃ­sticos bÃ¡sicos ou que exigem grande esforÃ§o computacional (modelagem linear e nÃ£o linear, testes estatÃ­sticos clÃ¡ssicos, anÃ¡lise de sÃ©ries temporais, classificaÃ§Ã£o, agrupamento, etc.) e recursos grÃ¡ficos elegantes. Um dos pontos fortes de R Ã© facilidade com que grÃ¡ficos de qualidade podem ser produzidos, incluindo sÃ­mbolos matemÃ¡ticos e fÃ³rmulas, quando necessÃ¡rio. O software R estÃ¡ disponÃ­vel em uma ampla variedade de plataformas UNIX e sistemas similares (incluindo FreeBSD e Linux), Windows e MacOS.","code":""},{"path":"intro.html","id":"o-software-rstudio","chapter":"CapÃ­tulo 1 IntroduÃ§Ã£o ao ambiente R","heading":"1.2 O software RStudio","text":"Quem ja Ã© usuÃ¡rio de softwares por linhas de comando, como o SAS, provavelmente nÃ£o notou nenhuma grande diferenÃ§atÃ© aqui. Toda anÃ¡lise se resume Ã  seguinte sequÃªncia dados > cÃ³digos > saÃ­da. experiÃªncia usuÃ¡rio com o R, entanto, pode ser mais atrativa utilizando o RStudio16. O Rstudio Ã© um produto de cÃ³digo aberto disponÃ­vel publicamente em 28/02/2011 que estÃ¡ disponÃ­vel gratuitamente. Ele Ã© um ambiente de desenvolvimento integrado para R que inclui () janelas de ediÃ§Ã£o de texto partir das quais o cÃ³digo pode ser enviado para o console e/ou salvo sistema operacional, (ii) listas de objetos em sua Ã¡rea de trabalho, (iii) histÃ³rico infinito dos comandos facilmente pesquisÃ¡vel com capacidade de inserir, partir histÃ³rico, um comando console novamente; (iv); interface com o sistema operacional para acesso arquivos; (v) janela de ajuda com botÃµes de voltar e avanÃ§ar; (vi) download de pacotes. Apesar de todas estas capacidades, o RStudio Ã© muito fÃ¡cil de utilizar.Nesta seÃ§Ã£o serÃ£o abordados alguns aspectos bÃ¡sicos para que o usuÃ¡rio R possa desenvolver suas anÃ¡lises. SerÃ¡ dado enfoque para Ã¡reas bÃ¡sicas da interface, cujo conhecimento Ã© necessÃ¡rio para que um usuÃ¡rio inicante possa realizar sua primeira anÃ¡lisee. figura abaixo mostra principais janelas Rstudio, incluindo o script, o console, â€œÃ¡rea de trabalhoâ€ e o output para grÃ¡ficos.Interface RStudioAntes de iniciar anÃ¡lises, recomenda-se escolher um diretÃ³rio  onde devem estar os inputs (dados) e para onde serÃ£o enviados os outputs (grÃ¡ficos, arquivos .txt, .xlsx, etc) . Para selecionar o diretÃ³rio, basta seguir o caminho Session > Set Working Directory > Choosing diretory ou utilizar teclas de atalho Ctrl+Shift+H.Selecionando um diretÃ³rioPosteriormente, um R Script Ã© iniciado conforme figura abaixo ou pelas teclas de atalho Ctrl+Shift+N. canto inferior direito, alÃ©m de servir como output para grÃ¡ficos (Plots), tambÃ©m Ã© o local onde os pacotes utilizados nas anÃ¡lises (Packages) sÃ£o instalados e maiores informaÃ§Ãµes sobre funÃ§Ãµes podem ser encontradas (Help).","code":""},{"path":"intro.html","id":"meu-primeiro-script","chapter":"CapÃ­tulo 1 IntroduÃ§Ã£o ao ambiente R","heading":"1.3 Meu primeiro script","text":"Se vocÃª realizou o download software R pela primeira vez e achou um tanto quanto â€œestranhoâ€ o pequeno tamanho arquivo (~80 Mb), provalemente deve ter se perguntado como um software estatistico tÃ£o poderoso pode ser comprimido em um arquivo tÃ£o pequeno (pequeno em comparaÃ§Ã£o com os +20 GB SAS). resposta Ã© simples, somente os pacote bÃ¡sicos R sÃ£o baixados com o arquivo de instalaÃ§Ã£o. Na medida em que o usuÃ¡rio necessita realizar uma anÃ¡lise especÃ­fica, instalaÃ§Ã£o de um pacote que contÃ©m uma funÃ§Ã£o especÃ­fica para realizar tal anÃ¡lise Ã© necessÃ¡ria.instalaÃ§Ã£o dos pacotes pode ser realizada conforme figura abaixo, ou atravÃ©s da funÃ§Ã£o install.packages(). ApÃ³s instalaÃ§Ã£o pacote, o usuÃ¡rio deve utilizar funÃ§Ãµes require() ou library()  para que o pacote seja carregado e suas funÃ§Ãµes possam ser utilizadas. Quando o usuÃ¡rio tenta utilizar uma funÃ§Ã£o pertencente um determinado pacote e este pacote nÃ£o estÃ¡ instalado (ou carregado), um erro Ã© exibido.Instalando pacotesAjuda para funÃ§Ã£o nls()Como exemplo inicial, vamos tentar selecionar variÃ¡vel Sepal.Length conjunto de dados base iris utilizando funÃ§Ã£o abaixo. Cuidado, spoilers pacote dplyr! O primero passo Ã© criar um novo script, seguindo os seguintes passos: File > New File > R script ou utilizando o atalho Ctrl+Shift+N. Posteriormente, o seguinte cÃ³digo Ã© digitado e executado ao se selecionar linha cÃ³digo e clicar botÃ£o run ou utilizando o atalho Ctrl+Enter).Neste caso, um erro Ã© exibido pois o pacote dplyr nÃ£o estÃ¡ instalado ou carregado. Caso ele jÃ¡ estiver instalado, mensagem de erro acima Ã© superada ao carregar o pacote antes de executar funÃ§Ã£o.Caso o pacote dplyr nÃ£o esteja instalado, maneira mais fÃ¡cil de instalÃ¡-lo Ã© instalando coleÃ§Ã£o de pacotes tidyverse17 seguindo os passos da Figura 3 ou utilizando seguinte funÃ§Ã£o.O tidyverse Ã© uma coleÃ§Ã£o de pacotes R projetados para ciÃªncia de dados, contendo, dentre outros, seguintes pacotes que serÃ£o utilizados neste material.ggplot2, para criaÃ§Ã£o de grÃ¡ficos.dplyr, para manipulaÃ§Ã£o de dados.tidyr, para organizaÃ§Ã£o de dados.tibble, para criaÃ§Ã£o de tibbles.Praticamente todas rotinas realizadas R sÃ£o baseadas em bibliotecas de cÃ³digos. Com manipulaÃ§ao de pacotes nÃ£o seria diferente. O pacote pacman18 Ã© uma ferramenta de gerenciamento de pacotes R que combina funcionalidade das funÃ§Ãµes relacionadas Ã  biblioteca base em funÃ§Ãµes intuitivamente nomeadas, reduzindo o cÃ³digo para executar simultaneamente vÃ¡rias aÃ§Ãµes. Uma das funÃ§Ãµes mais Ãºteis pacote pacman Ã© p_load() (package load). Esta funÃ§Ã£o checa se um pacote estÃ¡ instalado e em caso afirmativo, carrega-o como se funÃ§Ã£o library() tivesse sido usada. Caso o pacote nÃ£o esteja instalado, ela primeiro o instala-rÃ¡ antes de carregÃ¡-lo. Vamos, agora, instalar (para quem estÃ¡ utilizando o R pela primeira vez) ou carregar (para quem jÃ¡ tem instalado) os pacotes utilizados neste material.Os pacotes disponibilizados software R estÃ£o em constante atualizaÃ§Ã£o. Utilizando funÃ§Ã£o packageStatus() e summary(packageStatus()) Ã© possÃ­vel verificar se os pacotes estÃ£o atualizados. Outra forma Ã© ir em Packages > Update que se encontra canto inferior direito conforme demonstrado na Figura 5.Verificando se hÃ¡ atualizaÃ§Ãµes disponÃ­veisPor fim, para citar os pacotes , recomenda-se verificar referÃªncia atravÃ©s da funÃ§Ã£o citation().   Para o pacote metan, por exemplo, referÃªncia oficial Ã© encontrada artigo que descreve o pacote (Olivoto LÃºcio 2020)","code":"\nSL <- select(iris, Sepal.Length)\nlibrary(dplyr)\nSL <- select(iris, Sepal.Length)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nlibrary(tidyverse)\nif (!require(\"pacman\")){\ninstall.packages(\"pacman\")\n}\nlibrary(pacman)\np_load(hnp, asbio, car, DT, dplyr, devtools, emmeans, effects, multcomp,\n       lme4, rio, ExpDes.pt, qqplotr, manipulate, metan, MASS, olsrr,\n       tidyverse, tibble, agricolae, psych, corrplot, pvclust, factoextra, ggfortify)\ncitation(\"metan\")\n# \n# Please, support this project by citing it in your publications!\n# \n#   Olivoto, T., and LÃºcio, A.D. (2020). metan: an R package for\n#   multi-environment trial analysis. Methods Ecol Evol. 11:783-789\n#   doi:10.1111/2041-210X.13384\n# \n# A BibTeX entry for LaTeX users is\n# \n#   @Article{Olivoto2020,\n#     author = {Tiago Olivoto and Alessandro Dal'Col L{'{u}}cio},\n#     title = {metan: an R package for multi-environment trial analysis},\n#     journal = {Methods in Ecology and Evolution},\n#     volume = {11},\n#     number = {6},\n#     pages = {783-789},\n#     year = {2020},\n#     doi = {10.1111/2041-210X.13384},\n#     url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13384},\n#     eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13384},\n#   }"},{"path":"objects.html","id":"objects","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"CapÃ­tulo 2 Tipos de objetos","text":"Os tipos de objeto mais utilizados na linguagem R sÃ£o: () vetores, (ii) matrizes, (iii) data frames, (iv) listas e (v) funÃ§Ãµes. Um enfoque maior serÃ¡ dado aos vetores, matrizes e data frames, pois estes sÃ£o amplamen\r\nte utilizados, inclusive nas anÃ¡lises mais simples.","code":""},{"path":"objects.html","id":"vetores","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.1 Vetores","text":"Existem quatro tipos principais de vetores: lÃ³gico, inteiro, duplo e caractere (que contÃ©m cadeias de caracteres). Vetores coletivamente inteiros e duplos sÃ£o conhecidos como vetores numÃ©ricos. Cada um dos quatro tipos primÃ¡rios possui uma sintaxe especial para criar um valor individual, um escalar.Vetores lÃ³gicos podem ser escritos por extenso (TRUE ou FASLSE) ou abreviados (T ou F).Vetores lÃ³gicos podem ser escritos por extenso (TRUE ou FASLSE) ou abreviados (T ou F).Vetores duplos podem ser especificadas em formato decimal (0.1234), cientÃ­fico (1.23e4).Vetores duplos podem ser especificadas em formato decimal (0.1234), cientÃ­fico (1.23e4).Vetores inteiros sÃ£o escritos de forma semelhante aos duplos, mas devem ser seguidos por L (1234L, 1e4L ou 0xcafeL) e nÃ£o podem conter valores fracionados.Vetores inteiros sÃ£o escritos de forma semelhante aos duplos, mas devem ser seguidos por L (1234L, 1e4L ou 0xcafeL) e nÃ£o podem conter valores fracionados.Caracteres sÃ£o cercadas por \" (\"dia\") ou â€™ ('noite').Caracteres sÃ£o cercadas por \" (\"dia\") ou â€™ ('noite').funÃ§Ã£o c()   combina valores que formam vetores19. Abaixo, Ã© demonstrado como vetores podem ser criados utilizando c().Os vetores foram armazenados em x1, x2 e x3 e ficaram armazenados como valores na Ã¡rea de trabalho como valores (values). Para que os valores sejam mostrados basta digitar console onde os vetores foram armazenados.Vetores tambÃ©m podem ser criados utilizando funÃ§Ãµes rep() e seq(), conforme mostrado abaixo.  funÃ§Ã£o c() tambÃ©m pode ser combinada com funÃ§Ãµes rep() e seq() para criar vetores mais complexos, como mostrado abaixo.Utilizando colchtes [] Ã© possÃ­vel selecionar um (ou um conjunto) de elementos de um vetor. Por exemplo:Em adiÃ§Ã£o ao uso de [], funÃ§Ãµes first(), last() e nth(),  sÃ£o utilizadas para selecionar o primeiro, o Ãºltimo e o -Ã©simo elemento de um vetor. principal vantagem Ã© que vocÃª pode fornecer um vetor secundÃ¡rio opcional que define ordem e fornecer um valor padrÃ£o ser usado quando entrada menor que o esperado.","code":"\nx1 <- c(1) # Escalar \nx2 <- c(1,2) # Vetor\nx3 <- c(1,2,3) # Vetor\nx3\n# [1] 1 2 3\nx3.1 <- c(\"um\",\"dois\",\"trÃªs\") # Vetor com caracteres\nrep(5, 10)\n#  [1] 5 5 5 5 5 5 5 5 5 5\nseq(1, 5)\n# [1] 1 2 3 4 5\nseq(1, 5, by = 0.5)\n# [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nseq(2, 20, by = 2)\n#  [1]  2  4  6  8 10 12 14 16 18 20\nx4 <- c(rep(1:4, each = 4))\nx5 <- seq(1:5)\nx6 <- c(rep(seq(1:5), each = 2))\nx6\n#  [1] 1 1 2 2 3 3 4 4 5 5\nx7 <- x6[1] # Seleciona o primeiro elemento do vetor \nx8 <- x6[4] #  Seleciona o quarto elemento do vetor \nx9 <- x6[c(1, 4, 8)] # Seleciona o primeiro, o quarto e o oitavo elemento\nx9\n# [1] 1 2 4\nx10 <- x6[1:4] # armazena uma sequÃªncia de elementos (primeiro ao quarto)\nx <- 1:10\nx <- runif(100, 0, 100)\nfirst(x)\n# [1] 46.14801\nlast(x)\n# [1] 10.7578\nnth(x, 23)\n# [1] 53.21046"},{"path":"objects.html","id":"matrizes","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.2 Matrizes","text":"matrizes  sÃ£o um conjunto de valores (ou variÃ¡veis) dispostos em linhas e colunas, e que formam um corpo delimitado por [ ]. matrizes sÃ£o geralmente representadas genericamente por \\({{\\boldsymbol{}}_{{\\boldsymbol{MxN}}}}\\), onde M e N represetam os nÃºmeros de linhas e colunas da matriz, respectivamente. matrizes podem ser facilmente construÃ­das utilizando funÃ§Ã£o matrix(). Alternativamente, funÃ§Ãµes cbind() e rbind() tambÃ©m podem ser utilizadas. primeira funÃ§Ã£o adiciona colunas matrizes, enquanto que segunda adiciona linhas. Veremos mais tarde que estas funÃ§Ãµes podem ser combinadas com outras funÃ§Ãµes para construÃ§Ã£o de data frames .funÃ§Ãµes cbind() e rbind()   podem ser utilizadas conjuntamente. NÃ£o queremos confundir sua cabeÃ§, mas se liÃ§Ã£o anterior foi entendida, prÃ³xima se torna fÃ¡cil.Com funÃ§Ã£o matrix()  podemos ter o mesmo resultado que o obtido com o uso das funÃ§Ãµes cbind() e rbind(). PorÃ©m, para utilizar funÃ§Ã£o matrix(), alguns argumentos devem ser declarados. Na funÃ§Ã£o matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,dimnames = NULL), os argumentos que devemos inicialmente conhecer sÃ£o o nrow, ncol e byrow. O primeiro indica o nÃºmero de linhas da matriz, o segundo nÃºmero de colunas e o terceiro indica como matriz Ã© preenchida. Por default, byrow Ã© FALSE, indicando que matrizes sÃ£o preenchidas por colunas. Se TRUE, o preenchimento ocorre por linhas.Para selecionar elementos, linhas e colunas da matriz com [ ] utiliza-se um sistema de coordenadas:","code":"\n## Usando cbind()\nx10 <- cbind(1,2,3,4,5) # ou x10 = cbind(1:5), 5 colunas com 1 elemento cada\nx11 <- cbind(c(1,2,3,4,5)) # ou x11 = cbind(c(1:5)), 1 coluna com 5 elementos cada \nx12 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10)) # 2 colunas de 5 elementos\nx12.1 <- cbind(x11,c(6:10))\nx13 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10),c(11,12,13,14,15)) # 3 colunas de 5 elementos\nx13.1 <- cbind(x12.1,c(11,12,13,14,15))\n## Usando rbind()\nx14 <- rbind(1,2,3,4,5) # x14 = x11, 5 linhas com 1 elemento cada \nx15 <- rbind(c(1,2,3,4,5)) # x15 = x10, 1 linha com 5 elementos cada\nx16 <- rbind(c(1,2,3,4,5),c(6,7,8,9,10)) # 2 linhas com 5 elementos cada\nx16.1 <- rbind(x15,c(6,7,8,9,10))\nx17 <- rbind(c(1,6),c(2,7),c(3,8),c(4,9),c(5,10)) # x16 = x12\n## Usando rbind() e cbind()\nx18 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10), rbind(11, 12, 13, 14, 15))  \nx18\n#      [,1] [,2] [,3]\n# [1,]    1    6   11\n# [2,]    2    7   12\n# [3,]    3    8   13\n# [4,]    4    9   14\n# [5,]    5   10   15\n## Usando matrix\nx19 <- matrix(1:15, nrow = 5, ncol = 3)\nx19\n#      [,1] [,2] [,3]\n# [1,]    1    6   11\n# [2,]    2    7   12\n# [3,]    3    8   13\n# [4,]    4    9   14\n# [5,]    5   10   15\nx20 <- matrix(1:15, nrow = 5, ncol = 3, byrow = TRUE)\nx20\n#      [,1] [,2] [,3]\n# [1,]    1    2    3\n# [2,]    4    5    6\n# [3,]    7    8    9\n# [4,]   10   11   12\n# [5,]   13   14   15\nx19[2, 3] # seleciona o elemento que estÃ¡ na linha 2 e coluna 3\n# [1] 12\nx19[, 2] # \",\" indica que todas as linhas serÃ£o selecionadas na coluna 2\n# [1]  6  7  8  9 10\nx19[1, ] # \",\" indica que todas as colunas serÃ£o selecionadas na linha 1\n# [1]  1  6 11"},{"path":"objects.html","id":"data-frame","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.3 Data Frame","text":"funÃ§Ã£o data.frame()  cria estruturas cujas colunas podem ser valores numÃ©ricos ou caracteres. Ã‰ uma estrutura muito utilizada em funÃ§Ãµes software R.Em x22 simulamos como muitos experimentos sÃ£o organizados momento de tabulaÃ§Ã£o dos dados (fatores nas colunas e variÃ¡veis nas linhas).","code":"\nx22 <- data.frame(\n      expand.grid(Ambiente = c(\"A1\", \"A2\"),\n                  Genotipo = c(\"G1\", \"G2\", \"G3\"),\n                  Rep = c(\"I\", \"II\", \"III\")),\n                  Y = rnorm(18, 50, 15))\nstr(x22)\n# 'data.frame': 18 obs. of  4 variables:\n#  $ Ambiente: Factor w/ 2 levels \"A1\",\"A2\": 1 2 1 2 1 2 1 2 1 2 ...\n#  $ Genotipo: Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 1 1 2 2 3 3 1 1 2 2 ...\n#  $ Rep     : Factor w/ 3 levels \"I\",\"II\",\"III\": 1 1 1 1 1 1 2 2 2 2 ...\n#  $ Y       : num  41.6 45.6 33.5 50.5 30.7 ..."},{"path":"objects.html","id":"tibbles","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.4 Tibbles","text":"Um tibble, ou tbl_df, Ã© uma versÃ£o moderna data.frame. Tibbles sÃ£o datas frames que nÃ£o alteram nomes ou tipos de variÃ¡veis, possuindo um mÃ©todo print() aprimorado, que facilita o uso com grandes conjuntos de dados contendo objetos complexos. VocÃª pode forÃ§ar um objeto de classe data.frame um de classe tibble utilizando as_tibble()  ou criar um partir de vetores individuais com tibble() . funÃ§Ã£o tibble(), diferente de data.frame() permite que vocÃª se refira Ã s variÃ¡veis que vocÃª acabou de criar. Ã‰ possÃ­vel, tambÃ©m, que um tibble tenha nomes de colunas que nÃ£o sejam nomes de variÃ¡veis R vÃ¡lidos. Por exemplo, elas podem nÃ£o comeÃ§ar com uma letra ou podem conter caracteres incomuns como um espaÃ§o. Para se referir essas variÃ¡veis, vocÃª precisa cercÃ¡-las com `. Neste documento, estrutura de dados padrÃ£o ser utilizada serÃ¡ tibble.","code":"\n# Convertendo um dataframe a um tibble\ntbl_x22 <- as_tibble(x22)\n# Tentando criar um dataframe\ndata.frame(x = 1:5,\n           y = 1,\n           z = x ^ 2 + y)\n# Criando um tibble\ntibble(x = 1:5,\n       y = x ^ 2,\n       `soma x + y` = x + y)\n# # A tibble: 5 x 3\n#       x     y `soma x + y`\n#   <int> <dbl>        <dbl>\n# 1     1     1            2\n# 2     2     4            6\n# 3     3     9           12\n# 4     4    16           20\n# 5     5    25           30"},{"path":"objects.html","id":"lista","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.5 Lista","text":"exemplo abaixo, serÃ¡ armazenado em uma lista  dois data-frames e uma matriz. Posteriomente, serÃ¡ selecionado matriz que estÃ¡ armazenada na lista:","code":"\nx23 <- list(x19, x22)\nx24 <- x23[[1]]"},{"path":"objects.html","id":"funÃ§Ãµes","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.6 FunÃ§Ãµes","text":"funÃ§Ãµes  sÃ£o base da linguagem R. AtravÃ©s de argumentos que sÃ£o indicados em funtion(), uma expressÃ£o (ou sÃ©rie de expressÃµes) Ã© resolvida e um valor (ou um conjunto de valores) Ã© retornado. Quando uma funÃ§Ã£o Ã© armazenada ambiente de trabalho, basta digitar o nome como o qual aquela funÃ§Ã£o foi gravada. Os argumentos podem ser inseridos na ordem em que aparecem na funÃ§Ã£o, sem especificar qual argumento aquele valor pertence. caso em que inserÃ§Ã£o dos argumentos Ã© diferente da ordem em que aparecem na funÃ§Ã£o, Ã© preciso identificar qual argumento aquele valor pertente. Note que Ã© possÃ­vel combinar valores numÃ©ricos e texto como argumentos e/ou resultados de funÃ§Ãµes.ExercÃ­cio 1O resultado da funÃ§Ã£o F2(2, 3) foi o mesmo da F2(y = 3, x =2)? Por quÃª?Por quÃª ocorreu um erro quando funÃ§Ã£o F3(20) foi rodada?O que tem de errado na execuÃ§Ã£o da funÃ§Ã£o elevar(12, eleva = \"cubico\")?Crie uma funÃ§Ã£o chamada mega que retorna os nÃºmeros serem apostados em jogo da Mega Sena, tendo como argumentos jogos, que define quantos jogos e numeros, que define quantos numeros serÃ£o escolhidos em cada aposta (6-15). Para cada jogo ordene os nÃºmeros em ordem crescente.Resposta","code":"\nF1 <- function(x){ # x Ã© o argumento da funÃ§Ã£o\n  a = 2 * x + 1\n  return(a) # retorna a\n}\n\nF2 <- function(x, y){ # dois argumentos na funÃ§Ã£o\n  a = 2 * x + 1\n  b = y\n  c = a + b\n  return(c) # retorna c\n}\n\nF3 <- function(x){\n  if(x > 10){\n    stop(\"O argumento x = \", x, \" Ã© invÃ¡lido. 'x' precisa ser maior que 10\")\n  }\n  a = ifelse(x<= 5, 2 * x + 1, 3 * x + 1)\n  return(a)\n}\n\nelevar <- function(x, eleva = \"quadrado\"){\n  if(!eleva %in% c(\"quadrado\", \"cubo\")){\n    stop(\"O argumento eleva = \",eleva, \" deve ser ou 'quadrado' ou 'cubo'\")\n  }\n  if(eleva == \"quadrado\"){\n  valor <- ifelse(x^2 >= 1000,\n                 paste(\"O resultado (\",x^2,\") tem mais que 3 dÃ­gitos\"),\n                 paste(\"O resultado (\",x^2,\") tem menos que 3 dÃ­gitos\"))\n  }\n  if(eleva == \"cubo\"){\n  valor <- ifelse(x^3 >= 1000,\n                 paste(\"O resultado (\",x^3,\") tem mais que 3 dÃ­gitos\"),\n                 paste(\"O resultado (\",x^3,\") tem menos que 3 dÃ­gitos\"))\n  }\n                 \n  return(valor)\n}\nF1(2)\nF2(2, 3)\nF2(y = 3, x =2)\nF3(1)\nF3(6)\nF3(20)\nelevar(12)\nelevar(12, eleva = \"cubico\")"},{"path":"objects.html","id":"identificando-as-classes-de-objetos","chapter":"CapÃ­tulo 2 Tipos de objetos","heading":"2.7 Identificando as classes de objetos","text":"Conforme visto anteriormente, Ã© possÃ­vel construir vÃ¡rias classes de objetos em linguagem R. Veremos mais adiante que muitas funÃ§Ãµes exigem classes especÃ­ficas como argumento, e por isso conhecÃª-los Ã© muito importante. FunÃ§Ãµes genÃ©ricas como class()  ou .objeto sÃ£o importantes para identificar qual tipo de classe tal objeto pertence.Algumas funÃ§Ãµes permitem forÃ§ar objetos uma classe especÃ­fica, como por exemplo, transformar um objeto de classe data.frame em um objeto de classe matrix. ","code":"\nclass(x19)\n# [1] \"matrix\" \"array\"\nclass(x22)\n# [1] \"data.frame\"\nclass(x24)\n# [1] \"matrix\" \"array\"\nis.matrix(x19)\n# [1] TRUE\nx22 <- as.matrix(x22)\nx19 <- as.data.frame(x19)"},{"path":"math.html","id":"math","chapter":"CapÃ­tulo 3 OperaÃ§Ãµes matemÃ¡ticas","heading":"CapÃ­tulo 3 OperaÃ§Ãµes matemÃ¡ticas","text":"operaÃ§Ãµes matemÃ¡ticas utilizam sÃ­mbolos que sÃ£o padrÃ£o em outros softwares estatÃ­sticos.Para multiplicaÃ§Ã£o de matrizes utiliza-se %*% ao envÃ©z de *. Note diferenÃ§exemplo abaixo.O resultado da multiplicaÃ§Ã£o da matriz x e y Ã© dado por:\\[\r\n    \\left( {\\begin{array}{*{20}{c}}{1 \\cdot 5 + 3 \\cdot 6}&{1 \\cdot 7 + 3 \\cdot 8}\\\\{2 \\cdot 5 + 4 \\cdot 6}&{2 \\cdot 7 + 4 \\cdot 8}\\end{array}} \\right) = \\left( {\\begin{array}{*{20}{c}}{23}&{31}\\\\{34}&{46}\\end{array}} \\right)\r\n\\]funÃ§Ã£o t()  Ã© utilizada para transposiÃ§Ã£o de matrizes e solve()  para inversÃ£o de matrizes. Vamos resolver o seguinte sistema de equaÃ§Ãµes retirado livro de Rencher Schaalje (2008) utilizando estes operadores.\\[\r\n    \\begin{array}{l}{x_1} + 2{x_2} = 4\\\\{x_1} - {x_2} = 1\\\\{x_1} + {x_2} = 3\\end{array}\r\n\\]Matricialmente o sistema acima Ã© dado por:\\[\\left[ {\\begin{array}{*{20}{c}}{\\begin{array}{*{20}{l}}1\\\\1\\\\1\\end{array}}&{\\begin{array}{*{20}{c}}2\\\\{ - 1}\\\\1\\end{array}}\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}{{x_1}}\\\\{{x_2}}\\end{array}} \\right] = \\left[ {\\begin{array}{*{20}{c}}4\\\\1\\\\3\\end{array}} \\right]\\]Esse sistema de equaÃ§Ãµes Ã© representado por \\({\\boldsymbol{AX}} = {\\boldsymbol{c}}\\) e tem como soluÃ§Ã£o \\({\\boldsymbol{X = }}{{\\boldsymbol{}}^{{\\boldsymbol{ - 1}}}}{\\boldsymbol{c}}\\)Considere um equaÃ§Ã£o linear mÃºltipla cuja variÃ¡vel dependente Ã© Y e variÃ¡veis independentes sÃ£o X1 e X2 (dados obtidos em Kutner et al. (2005)). O sistema de equaÃ§Ãµes Ã© representado matricialmente por \\[\r\n\\boldsymbol{X'X\\beta = X'Y}\r\n\\]que tem como soluÃ§Ã£o:\\[\r\n\\boldsymbol{\\hat\\beta} = \\boldsymbol{X'X}^{- 1}\\boldsymbol{X'Y}\r\n\\]O modelo ajustado Ã© dado por \\(\\hat Y = - 234.698704 + 1.074137{X_1} + 20.547108{X_2} + \\varepsilon\\). Combinando algumas funÃ§Ãµes vistas atÃ© agora, vamos criar um vetor de dados chamado PRED com os valores estimados pelo modelo acima. Em adiÃ§Ã£o, um vetor de resÃ­duos (RESID) serÃ¡ criado.funÃ§Ãµes det()  para calcular o determinante de uma matriz. JÃ¡ funÃ§Ã£o eigen()  retorna uma lista com os autovalores e autovetores da matriz. funÃ§Ã£o names()  indica o que contÃ©m objeto av, e usando $ Ã© possivel selecionar os autovalores ou os autovetores.funÃ§Ã£o diag()  extrai diagonal de uma matriz ou cria uma matriz onde diagonal sÃ£o os elementos declarados. Os prÃ³ximos comandos extraem diagonal de XX e criam uma matriz identidade, com 5 linhas e 5 colunas.","code":"\n1 + 1 # Soma\n2 - 1 # SubtraÃ§Ã£o\n2 * 2 # MultiplicaÃ§Ã£o\n1 + 2 * 2 ^ 2 # Primeiro eleva ao quadrado, depois multiplica e depois soma\n((1 + 2) * 2) ^ 2 # Primeiro soma e depois multiplica depois eleva ao quadrado\nsqrt(4) # Raiz quadrada\n4^2 # PotÃªncia\nlog(10) # Por default, o logarÃ­timo Ã© de base e (logarÃ­timo natural)\nlog(100, 10) # LogarÃ­timo de base 10\nexp(100) # exponencial\n(x <- matrix(1:4, ncol = 2))\n#      [,1] [,2]\n# [1,]    1    3\n# [2,]    2    4\n(y <- matrix(5:8, ncol = 2))\n#      [,1] [,2]\n# [1,]    5    7\n# [2,]    6    8\nx * y # Errado\n#      [,1] [,2]\n# [1,]    5   21\n# [2,]   12   32\nx %*% y # Certo\n#      [,1] [,2]\n# [1,]   23   31\n# [2,]   34   46\n\n(A <- matrix(c(1, 1, 1, 2, -1, 1), nrow = 3, ncol = 2))\n#      [,1] [,2]\n# [1,]    1    2\n# [2,]    1   -1\n# [3,]    1    1\nA1 <- MASS::ginv(A) # ObtÃ©m a inversa generalizada de A\nc <- c(4, 1, 3) # Vetor C\nX <- A1 %*% c\nX\n#      [,1]\n# [1,]    2\n# [2,]    1\nX0 <- rep(1, each = 10)\nX1 <- c(68.5, 45.2, 91.3, 47.8, 46.9, 66.1, 49.5, 52, 48.9, 38.4)\nX2 <- c(16.7, 16.8, 18.2, 16.3, 17.3, 18.2, 15.9, 17.2, 16.6, 16)\nY <- c(174.4,  164.4, 244.2, 154.6, 181.6, 207.5, 152.8, 163.2, 145.4, 137.2)\nX <- matrix(c(X0, X1, X2), nrow = 10, ncol = 3)\nX\n#       [,1] [,2] [,3]\n#  [1,]    1 68.5 16.7\n#  [2,]    1 45.2 16.8\n#  [3,]    1 91.3 18.2\n#  [4,]    1 47.8 16.3\n#  [5,]    1 46.9 17.3\n#  [6,]    1 66.1 18.2\n#  [7,]    1 49.5 15.9\n#  [8,]    1 52.0 17.2\n#  [9,]    1 48.9 16.6\n# [10,]    1 38.4 16.0\nB <- (solve(t(X) %*% X)) %*%  t(X) %*% Y\nB\n#             [,1]\n# [1,] -234.698704\n# [2,]    1.074137\n# [3,]   20.547108\nPRED <- B[1] + B[2] * X1 + B[3] * X2\nRESID <- Y - PRED\nmat <- matrix(c(0, 2, 4, 3, 5, 0, 2, 4, 4), nrow = 3)\ndetXX <- det(mat)\nav <- eigen(mat)\nnames(av)\n# [1] \"values\"  \"vectors\"\nav$values # Extrai os autovalores\n# [1]  8  2 -1\nav$vectors # Extrai os autovetores\n#           [,1]       [,2]       [,3]\n# [1,] 0.4082483 -0.3333333  0.7715167\n# [2,] 0.8164966 -0.6666667  0.1543033\n# [3,] 0.4082483  0.6666667 -0.6172134\ndiag(mat)\n# [1] 0 5 4\ndiag(x = 1, nrow = 4, ncol = 4)\n#      [,1] [,2] [,3] [,4]\n# [1,]    1    0    0    0\n# [2,]    0    1    0    0\n# [3,]    0    0    1    0\n# [4,]    0    0    0    1"},{"path":"loops.html","id":"loops","chapter":"CapÃ­tulo 4 Loops","heading":"CapÃ­tulo 4 Loops","text":"Reescrever um cÃ³digo muitas vezes por necessidade de repetir um determinado procedimento seria bastante trabalhoso, alÃ©m de precisarmos de mais tempo para isso. Por isso, o R tem algumas funÃ§Ãµes que fazem essas repetiÃ§Ãµes para nÃ³s. Isso Ã© muito comum e pode ser facilmente implementado pela funÃ§Ã£o (), () e repeat().\r\nfunÃ§Ã£o () repete o cÃ³digo indicado dentro de {} n vezes, sendo n o comprimento da sequÃªncia dentro dos parÃªnteses.funÃ§Ã£o () (que significa enquanto) repete o cÃ³digo dentro de {} enquanto alguma condiÃ§Ã£o verdadeira.Note que os dois Ãºltimos exemplos apresentam o mesmo resultado: o R vai retornar uma sequÃªncia sendo = 1:5, onde cada nÃºmero serÃ¡ o resultado da multiplicaÃ§Ã£o \\(\\times 2\\). caso (), precisamos mudar o valor de para que sequÃªncia continue atÃ© que condiÃ§Ã£o (<= 5) verdadeira. Em adiÃ§Ã£o, precisamos declarar variÃ¡vel (= 1) antes para que o R possa testar condiÃ§Ã£o expressa dentro dos parÃªnteses. caso (), sequÃªncia progride sem precisarmos fazer isso manualmente.Ãºltimo exemplo, utilizando repeat(), o R repetirÃ¡ o cÃ³digo dentro de {} sem condiÃ§Ãµes. Com isso, precisamos utilizar combinaÃ§Ã£o das funÃ§Ãµes () e break() para informar ao programa quando o cÃ³digo deve parar de ser repetido.Os Loops sÃ£o importantes em estudos que utilizam reamostragens para realizar anÃ¡lises estatÃ­sticas. AtravÃ©s de reamostragnes Ã© possÃ­vel construir intervalos de confianÃ§e testar hipÃ³teses nÃ£o paramÃ©tricas. Como exemplo, vamos demonstrar o teorema central limite20. Para isto, criamos uma funÃ§Ã£o (teor_lim())  que tem 4 argumentos: n o tamanho da amostra ser considerada, namostra, min, max sÃ£o os parÃ¢metros da distribuiÃ§Ã£o uniforme (veja ?runif). Para confecÃ§Ã£o dos dendrogramas, os pacotes ggplot2 e cowplot serÃ£o utilizados. Veja seÃ§Ã£o 1.6 para maiores informaÃ§Ãµes sobre confecÃ§Ã£o de grÃ¡ficos com estes pacotes. \r\nFigure 4.1: DemonstraÃ§Ã£o teorema central limite\r\n","code":"\nj <- 5\nfor (i in 1:j){\n  k <- i * 2\n  print(k)\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\ni <- 1\nwhile (i <= 5){\n  print(i * 2)\n  i <- i + 1\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\ni <- 1\nrepeat {\n  print(i * 2)\n  i <- i + 1\n  if(i > 5){\n    break()\n  }\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\nlibrary(ggplot2)\nlibrary(cowplot)\nteor_lim <- function(n, namostra, min, max){\n  set.seed(100)\n  media <- data.frame(matrix(ncol = 1, nrow = n))\n  names(media) = \"value\"\n  for(j in 1:n){\n    media[j, 1] = mean(runif(namostra, min, max))\n  }\n  return(media)\n}\n\nteorema_limite <- list(\n  n20 <- teor_lim(n = 20, namostra = 100, min = 0, max = 10),\n  n200 <- teor_lim(n = 200, namostra = 100, min = 0, max = 10),\n  n2000 <- teor_lim(n = 2000, namostra = 100, min = 0, max = 10),\n  n20000 <- teor_lim(n = 20000, namostra = 100, min = 0, max = 10)\n)\n\np <- lapply(teorema_limite, function(d){\n  ggplot(data = d, aes(x = value))+\n    geom_histogram(bins = 50,\n                   col = \"black\",\n                   size = 0.3,\n                   aes(fill = ..count..))+\n    theme(legend.position = \"none\")+\n    labs(x = \"MÃ©dia\", y = \"Contagem\")\n  \n})\nplot_grid(plotlist = p,\n          labels = names(teorema_limite),\n          vjust = 2.5,\n          hjust = c(-1.7, -1.5, -1, -1))"},{"path":"dataframe.html","id":"dataframe","chapter":"CapÃ­tulo 5 Construindo uma tabela","heading":"CapÃ­tulo 5 Construindo uma tabela","text":"Com o que foi visto atÃ© agora, Ã© possÃ­vel construir uma tabela para armazenar dados ou resultados gerados em uma anÃ¡lise. SÃ£o inumeras formas de fazer isso, porÃ©m vamos mostrar apenas uma (devido ao pouco tempo). Utilizaremos funÃ§Ãµes matrix()  e data.frame()  para tabela, e funÃ§Ã£o names() para dar nomeas colunas. construÃ§Ã£o de tabelas Ã© muito util para armazenar resultados na Ã¡rea de trabalho.Vimos anteriormente, entanto, que utilizaÃ§Ã£o de tibbles  Ã© recomendada. O mesmo conjunto de dados pode ser criado mais â€œelegantementeâ€ com funÃ§Ã£o abaixo","code":"\nEx.gen <- as.data.frame(matrix(0, ncol = 3, nrow = 20))\nnames(Ex.gen) = c(\"G1\",\"G2\",\"G3\")\nYeld1 <- rnorm(20,10,1) \n# gera 20 valores de uma distribuiÃ§Ã£o normal com mÃ©dia 10 e desvio padrÃ£o 1\nYeld2 <- rnorm(20,40,3)\n# gera 20 valores de uma distribuiÃ§Ã£o normal com mÃ©dia 40 e desvio padrÃ£o 3\nYeld3 <- rnorm(20,25,2.5)\n# gera 20 valores de uma distribuiÃ§Ã£o normal com mÃ©dia 25 e desvio padrÃ£o 2,5\nEx.gen$G1 <- Yeld1\nEx.gen$G2 <- Yeld2\nEx.gen$G3 <- Yeld3\nEx.gen2 <- tibble(G1 = rnorm(20,10,1),\n                  G2 = rnorm(20,40,3),\n                  G3 = rnorm(20,25,2.5))"},{"path":"entrada.html","id":"entrada","chapter":"CapÃ­tulo 6 Entrada de dados","heading":"CapÃ­tulo 6 Entrada de dados","text":"entrada de dados pode ser feita de vÃ¡rias maneiras, e em vÃ¡rios formatos. PorÃ©m daremos destaque formas mais comuns. forma mais simples (e mais trabalhosa) Ã© digitar os dados diretamente console, utilizando para isso funÃ§Ã£o scan().  Para importar dados digitados em extensÃ£o .txt utiliza-se funÃ§Ã£o read.table().  Por fim, para carregar arquivos em extensÃ£o .xlsx maneira mais simples Ã© utilizar o Import Dataset que encontra-se na Ã¡rea de trabalho.funÃ§Ã£o scan() Ã© muito trabalhosa e pouco utilizada. Caso o usuÃ¡rio queira carregar dados salvos em extensÃ£o .txt (bloco de notas), usando funÃ§Ã£o read.table(), deve-se ter o cuidado de mover o arquivo para o diretÃ³rio previamente indicado. Como colunas sÃ£o identificadas por espaÃ§os, Ã© importante que o usuÃ¡rio nÃ£o utilize nomes compostos cabeÃ§alho ou corpo da tabela. Quando isso ocorre, o R identifica o erro console atravÃ©s da mensagem Error read.table(\"Dados_1.txt\", header = TRUE) : columns column names.forma mais comum pesquisador digitar seus dados Ã© atravÃ©s de planilhas eletrÃ´nicas Excel. Para carregar esses dados, basta ir em Import Dataset na Ã¡rea de trabalho. O passo passo estÃ¡ descrito abaixo: Importando dados de planilhas eletrÃ´nicas Excel - Passo 1Importando dados de planilhas eletrÃ´nicas Excel - Passo 2Importando dados de planilhas eletrÃ´nicas Excel - Passo 3TambÃ©m Ã© possÃ­vel carregar dados jÃ¡ extistentes dentro software R. Geralmente, os pacotes contÃ©m dados que sÃ£o utilizados como exemplo de aplicaÃ§Ã£o das suas funÃ§Ãµes.","code":"\ndata <- scan() #Enter\n1: 10\n2: 20\n3: 30\n4: 40\n5: # Duplo enter\ndata\ndados <- read.table(\"data/Dados_1.txt\", header = TRUE)\n# Argumento header = TRUE indica a existÃªncia de cabeÃ§alho\ndados <- read.table(\"data/Dados_2.txt\", header = TRUE)\n# Argumento header = TRUE indica a existÃªncia de cabeÃ§alho\ndados\n#           Trat Rep      Sta     num    peso\n# 1 bcco_alb_imp   1 1132.789 0.00000 0.00000\n# 2 bcco_alb_imp   2 1199.039 0.06250 0.61875\n# 3 bcco_alb_imp   3 1265.189 0.06250 0.61875\n# 4 bcco_alb_imp   4 1337.789 0.16525 1.48125\nhead(iris) # head() limita os valores que serÃ£o impressos no console\n#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n# 1          5.1         3.5          1.4         0.2  setosa\n# 2          4.9         3.0          1.4         0.2  setosa\n# 3          4.7         3.2          1.3         0.2  setosa\n# 4          4.6         3.1          1.5         0.2  setosa\n# 5          5.0         3.6          1.4         0.2  setosa\n# 6          5.4         3.9          1.7         0.4  setosa"},{"path":"manipula.html","id":"manipula","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","text":"ApÃ³s seus dados estarem carregados ambiente R, eles provavelmente necessitarÃ£o de alguma manimulaÃ§Ã£o antes de serem utilizados em uma determinada anÃ¡lise. Esta manipulaÃ§Ã£o pode envolver operaÃ§Ãµes como exclusÃ£o de colunas, ordenamento de linhas com base em valores, criaÃ§Ã£o de covariÃ¡veis (que serÃ£o resultado de operaÃ§Ãµes com uma ou mais variÃ¡veis existentes), dentre muitas outras possibilidades. Felizmente, o pacote dplyr  permite que esta manimulaÃ§Ã£o seja relativamente fÃ¡cil, lÃ³gica (ponto de vista de digitaÃ§Ã£o de cÃ³digos) e rÃ¡pida, pois ele integra linguagem C++ em suas funÃ§Ãµes.O pacote dplyr Ã© uma gramÃ¡tica de manipulaÃ§Ã£o de dados. Nos rerferimos Ã  gramÃ¡tica aqui porque ele fornece funÃ§Ãµes nomeadas como verbos simples, relacionados Ã s tarefas mais comuns de manipulaÃ§Ã£o de dados, para ajudÃ¡-lo traduzir seus pensamentos em cÃ³digo. Este serÃ¡ o pacote utilizado para manipulaÃ§Ã£o dos dados decorrer deste material. De fato, maioria dos dados em R podem ser manipulados utilizando os seguintes â€œverbosâ€.filter() para selecionar linhas com base em seus valores.arrange() para reordenar linhas.select() e rename() para selecionar variÃ¡veis com base em seus nomes.mutate() e transmute() para adicionar novas variÃ¡veis que sÃ£o funÃ§Ãµes de variÃ¡veis existentes.summarise() para resumir vÃ¡rios valores para um Ãºnico valor.sample_n() e sample_frac() para obter amostras aleatÃ³rias.Anteriomente mencionamos que manipulaÃ§Ã£o dos dados com o pacote dplyr Ã© lÃ³gica ponto de vista da implementaÃ§Ã£o cÃ³digo. Isto sÃ³ Ã© possivel devido utilizaÃ§Ã£o operador %>% (forward-pipe operator), importado pacote magrittr. Basicamente, este operador capta o argumento resultante de uma funÃ§Ã£o Ã  sua esquerda e passa como input Ã  funÃ§Ã£o Ã  sua direita. NÃ£o Ã© nossso objetivo aqui discutir os benefÃ­cios da utilizaÃ§Ã£o deste operador, mas uma pequena demonstraÃ§Ã£o (com spoilers das funÃ§Ãµes pacote dplyr) serÃ¡ apresentada. Considere seguintes (e simples) operaÃ§Ãµes. Crie um data frame com 100 linhas com variÃ¡veis x e y contendo valores aleatÃ³rios. Adicione uma terceira variÃ¡veis (z) que serÃ¡ uma funÃ§Ã£o da multiplicaÃ§Ã£o de x e y, selecione apenas os valores de z menores que 10 e extraia raiz quadrada destes valores. Finalmente, compute mÃ©dia e armazene object mean_sqrt.Criando o conjunto de dadosUtilizando funÃ§Ãµes bases R (cÃ³digo massivo)Utilizando funÃ§Ãµes bases R (cÃ³digo mais limpo)Utilizando o operdor %>%utilizaÃ§Ã£o operador %>% parece nÃ£o trazer tantos benefÃ­cios neste exemplo, visto que objetivo aqui foi demonstrar como ele permite uma implementaÃ§Ã£o lÃ³gica das operaÃ§Ãµes realizadas, captando saÃ­da da funÃ§Ã£o diretamente Ã  esquerda (acima neste caso) e passando para prÃ³xima funÃ§Ã£o. Em operaÃ§Ãµes mais complexas, entanto, o %>% se mostrarÃ¡ muito mais Ãºtil.O pacote metan fornece funÃ§Ãµes Ãºteis para manipulaÃ§Ã£o de dados. Duas principais categorias de funÃ§Ãµes serÃ£o utilizadas neste material:UtilitÃ¡rios para lidar com linhas e colunasadd_cols(): adiciona uma ou mais colunas um conjunto de dados existente. Se colunas .ou .especificadas nÃ£o existirem, colunas serÃ£o anexadas final dos dados. Retorna um conjunto de dados com todas colunas originais em .data mais colunas declaradas em .... Em add_cols(), colunas em .data estÃ£o disponÃ­veis para expressÃµes. Portanto, Ã© possÃ­vel adicionar uma coluna com base nos dados existentes.\r\nadd_rows(): adiciona uma ou mais linhas um conjunto de dados existente. Se nÃ£o houver linhas especificadas .ou ., linhas serÃ£o anexadas final dos dados. Retorna um conjunto de dados com todas linhas originais em .data mais linhas declaradas em ....\r\nall_pairs(): obtÃ©m todos os pares possÃ­veis entre os nÃ­veis de um fator.\r\ncolnames_to_lower(): converte todos os nomes de coluna para minÃºsculas.\r\ncolnames_to_upper(): converte todos os nomes de coluna para maiÃºsculas.\r\ncolnames_to_title(): converte todos os nomes de coluna em maiÃºsculas.\r\ncolumn_exists(): verifica se existe uma coluna em um conjunto de dados. Retorne um valor lÃ³gico.\r\ncolumns_to_first(): move colunas para primeiras posiÃ§Ãµes em .data.\r\ncolumns_to_last(): move colunas para Ãºltimas posiÃ§Ãµes em .data.\r\nconcatenate(): concatena colunas de um conjunto de dados. Se drop = TRUE, variÃ¡veis existentes sÃ£o descartadas. Se pull = TRUE, variÃ¡vel concatenada Ã© extraÃ­da para um vetor. Isso Ã© especialmente Ãºtil ao usar concatenar para adicionar colunas um conjunto de dados com add_cols().\r\nget_levels(): obtÃ©m os nÃ­veis de um fator.\r\nget_level_size(): obtÃ©m o tamanho de cada nÃ­vel de um fator.\r\nremove_cols(): remove uma ou mais colunas de um conjunto de dados.\r\nremove_rows(): remove uma ou mais linhas de um conjunto de dados.\r\nreorder_cols(): reordena colunas em um conjunto de dados.\r\nselect_cols(): seleciona uma ou mais colunas de um conjunto de dados.\r\nselect_first_col(): seleciona primeira variÃ¡vel, possivelmente com um deslocamento.\r\nselect_last_col(): seleciona Ãºltima variÃ¡vel, possivelmente com um deslocamento.\r\nselect_numeric_cols(): selecione todas colunas numÃ©ricas de um conjunto de dados.\r\nselect_non_numeric_cols(): seleciona todas colunas nÃ£o numÃ©ricas de um conjunto de dados.\r\nselect_rows(): seleciona uma ou mais linhas de um conjunto de dados.\r\nUtilitÃ¡rios para lidar com nÃºmeros e stringsall_lower_case(): converte todas seqÃ¼Ãªncias nÃ£o numÃ©ricas de um conjunto de dados para minÃºsculas (â€œEnvâ€ para â€œenvâ€). all_upper_case(): converte todas seqÃ¼Ãªncias nÃ£o numÃ©ricas de um conjunto de dados para maiÃºsculas (por exemplo, â€œEnvâ€ para â€œENVâ€).all_title_case(): converta todas seqÃ¼Ãªncias nÃ£o numÃ©ricas de um conjunto de dados em maiÃºsculas e minÃºsculas (por exemplo, â€œENVâ€ para â€œEnvâ€).extract_number(): extrai o(s) nÃºmero(s) de uma sequÃªncia de caracteres.extract_string(): Extrai todas letras de uma sequÃªncia de caracteres, ignorando maiÃºsculas e minÃºsculas.find_text_in_num(): encontra caracteres de texto em uma sequÃªncia numÃ©rica e retorna o Ã­ndice de linha.has_text_in_num(): inspeciona colunas procurando por texto na sequÃªncia numÃ©rica e retorna um aviso se algum texto encontrado.remove_strings(): remove todas strings de uma variÃ¡vel.replace_number(): substitui os nÃºmeros por uma substituiÃ§Ã£o.replace_string(): substitui todas strings por uma substituiÃ§Ã£o, ignorando caixa.round_cols(): Arredonda uma coluna selecionada ou um conjunto de dados inteiro para nÃºmeros significativos.tidy_strings(): arruma seqÃ¼Ãªncias de caracteres, colunas nÃ£o numÃ©ricas ou quaisquer colunas selecionadas em um conjunto de dados colocando todas palavras em maiÃºsculas, substituindo qualquer espaÃ§o, tabulaÃ§Ã£o e caracteres de pontuaÃ§Ã£o por '_' e colocando '_' entre letras maiÃºsculas e minÃºsculas. Suponha que str = c(\"Env1\", \"env 1\", \"env.1\") (que por definiÃ§Ã£o deve representar um nÃ­vel Ãºnico nos ensaios de melhoramento de plantas, por exemplo, ambiente 1) seja submetido tidy_strings(str): o resultado serÃ¡ entÃ£o c(\"ENV_1\", \"ENV_1\", \"ENV_1\").O conjunto de dados maize serÃ¡ utilizado como exemplo para operaÃ§Ãµes de manipulaÃ§Ã£o de dados. Este arquivo em formato .xlsx estÃ¡ hospedado em https://github.com/TiagoOlivoto/e-bookr/tree/master/data e pode ser carregado ambiente R com funÃ§Ã£o import() pacote rio.","code":"\nset.seed(1)\ndata <- tibble(x = runif(100, 0, 10),\n               y = runif(100, 0, 10))\ndata$z <- data$x * data$y\ndf <- subset(data, z < 10)\ndf <- df[, 3]\nsqr_dat <- sqrt(df$z)\nmean_sqrt <- mean(sqr_dat)\nmean_sqrt\n# [1] 1.977507\ndata$z <- data$x * data$y\nmean_sqrt <- mean(sqrt(subset(data, z < 10)$z))\nmean_sqrt\n# [1] 1.977507\nmean_sqrt <- \n  data %>% \n  mutate(z = x * y) %>%\n  filter(z < 10) %>%\n  pull(z) %>%\n  sqrt() %>%\n  mean()\nmean_sqrt\n# [1] 1.977507\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nmaize <- import(url,\n                sheet = \"maize\", \n                setclass = \"tibble\")\ninspect(maize)\n# # A tibble: 10 x 9\n#    Variable Class     Missing Levels Valid_n   Min Median    Max Outlier\n#    <chr>    <chr>     <chr>   <chr>    <int> <dbl>  <dbl>  <dbl>   <dbl>\n#  1 AMB      character No      0          780  NA    NA     NA         NA\n#  2 HIB      character No      0          780  NA    NA     NA         NA\n#  3 REP      character No      0          780  NA    NA     NA         NA\n#  4 APLA     numeric   No      -          780   1     2.52   3.3        4\n#  5 AIES     numeric   No      -          780   0.5   1.38   2.39       1\n#  6 CESP     numeric   No      -          780   0.8  15.4   20.4       16\n#  7 DIES     numeric   No      -          780  36.4  50.0   59.7        1\n#  8 MGRA     numeric   No      -          780  58.5 174.   291.         0\n#  9 MMG      numeric   No      -          780 123.  344.   546.         6\n# 10 NGRA     numeric   No      -          780 147   517    903          9"},{"path":"manipula.html","id":"trabalhando-com-linhas-e-colunas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1 Trabalhando com linhas e colunas","text":"","code":""},{"path":"manipula.html","id":"selecionar-colunas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.1 Selecionar colunas","text":"funÃ§Ã£o select_cols() pode ser usada para selecionar colunas de um conjunto de dados.colunas numÃ©ricas podem ser selecionadas rapidamente usando funÃ§Ã£o select_numeric_cols(). colunas nÃ£o numÃ©ricas sÃ£o selecionadas com select_non_numeric_cols().Podemos selecionar primeira ou Ãºltima coluna rapidamente com select_first_col() e select_last_col(), respectivamente.Select helpers podem ser usados para selecionar variÃ¡veis que correspondem uma expressÃ£o. Isso significa que podemos usar uma funÃ§Ã£o para selecionar variÃ¡veis em vez de digitar seus prÃ³prios nomes. O metan reexporta os tidy select helpers e implementa os prÃ³prios select helpers com base em operaÃ§Ãµes com prefixos e sufixos (different_var(), intersect_var() e union_var()), tamanho dos nomes das variÃ¡veis (width_of(), width_gength_than() e width_less_than()) e tipo de letra (lower_case_only(), upper_case_only() e title_case_only()).Selecionando variÃ¡veis que comeÃ§com um prefixo.Se quisermos selecionar variÃ¡veis que comeÃ§com â€œCâ€, podemos usar:mas se quisermos selecionar aqueles que nÃ£o comeÃ§com â€œCâ€, basta adicionar â€œ-â€ logo antes de starts_with()Selecionando variÃ¡veis que terminam com um sufixo.Da mesma forma, se quisermos selecionar variÃ¡veis que terminam com â€œSâ€, podemos usar:Selecionando variÃ¡veis que comeÃ§com um prefixo E terminam com um sufixo â€œSâ€Agora, se quisermos selecionar variÃ¡veis que comeÃ§com â€œAâ€ e terminam com â€œSâ€, ou seja, interseÃ§Ã£o entre letra inicial â€œAâ€ e letra final â€œSâ€, podemos:Selecionando variÃ¡veis que comeÃ§com um prefixo OU terminam com um sufixo.TambÃ©m podemos obter uniÃ£o entre letra inicial â€œAâ€ e letra final â€œSâ€, ou seja, variÃ¡veis que comeÃ§com â€œAâ€ ou terminam com â€œSâ€.** Selecionando variÃ¡veis que comeÃ§com um prefixo E NÃƒO terminam com um sufixo.**TambÃ©m podemos obter diferenÃ§entre letra inicial â€œAâ€ e letra final â€œSâ€, ou seja, variÃ¡veis que comeÃ§com â€œCâ€ e nÃ£o terminam com â€œDâ€.Selecionando variÃ¡veis que contÃªm uma string literal.Se variÃ¡veis conjunto de dados tiverem um padrÃ£o com diferenÃ§entre um grupo de variÃ¡veis, podemos usar o cÃ³digo seguir para selecionar variÃ¡veis com um padrÃ£o. Primeiro, iremos alterar os nomes das variÃ¡veis PH, EH, EP e EL incluindo _PLANT para indicar que sÃ£o variÃ¡veis relacionadas Ã  planta. Em seguida, selecionaremos essas variÃ¡veis com funÃ§Ã£o contains().Selecionando variÃ¡veis que correspondem uma expressÃ£o regular.SeleÃ§Ãµes mais sofisticadas podem ser feitas usando matches(). Supondo que gostarÃ­amos de selecionar variÃ¡veis que comeÃ§com â€œAâ€ e tem segunda letra entre â€œAâ€ e â€œMâ€, usarÃ­amos algo como:Selecionando Ãºltima ou primeira variÃ¡vel, possivelmente com um deslocamento.Podemos selecionar n-Ã©sima primeira ou Ãºltima coluna com select_last_col() ou select_first_col().Selecione variÃ¡veis com um comprimento de nome especÃ­fico (quatro letras)**Selecione variÃ¡veis com largura menor que *n**.Selecione variÃ¡veis com largura maior que * n *.Selecione variÃ¡veis por tipo de letraVamos criar um conjunto de dados com nomes de colunas bagunÃ§ados.","code":"\nselect_cols(maize, AMB, HIB)\n# # A tibble: 780 x 2\n#    AMB   HIB  \n#    <chr> <chr>\n#  1 A1    H1   \n#  2 A1    H1   \n#  3 A1    H1   \n#  4 A1    H1   \n#  5 A1    H1   \n#  6 A1    H1   \n#  7 A1    H1   \n#  8 A1    H1   \n#  9 A1    H1   \n# 10 A1    H1   \n# # ... with 770 more rows\nselect_numeric_cols(maize)\n# # A tibble: 780 x 7\n#     APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1  2.45  2.39  16.9  52.1 228.   375.   609\n#  2  2.5   1.43  14.4  50.7 187.   437.   427\n#  3  2.69  1.52  16.5  54.7 230.   464.   497\n#  4  2.8   1.64  16.8  52.0 213.   408.   523\n#  5  2.62  1.55  15.9  51.6 224.   406.   551\n#  6  2.12  1.8   15    51.4 203.   383.   529\n#  7  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8  2.97  1.84  15    53.4 204.   387.   528\n#  9  3.1   1.78  13.6  50.8 187.   348.   538\n# 10  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nselect_non_numeric_cols(maize)\n# # A tibble: 780 x 3\n#    AMB   HIB   REP  \n#    <chr> <chr> <chr>\n#  1 A1    H1    I    \n#  2 A1    H1    I    \n#  3 A1    H1    I    \n#  4 A1    H1    I    \n#  5 A1    H1    I    \n#  6 A1    H1    II   \n#  7 A1    H1    II   \n#  8 A1    H1    II   \n#  9 A1    H1    II   \n# 10 A1    H1    II   \n# # ... with 770 more rows\nselect_first_col(maize)\n# # A tibble: 780 x 1\n#    AMB  \n#    <chr>\n#  1 A1   \n#  2 A1   \n#  3 A1   \n#  4 A1   \n#  5 A1   \n#  6 A1   \n#  7 A1   \n#  8 A1   \n#  9 A1   \n# 10 A1   \n# # ... with 770 more rows\nselect_last_col(maize)\n# # A tibble: 780 x 1\n#     NGRA\n#    <dbl>\n#  1   609\n#  2   427\n#  3   497\n#  4   523\n#  5   551\n#  6   529\n#  7   294\n#  8   528\n#  9   538\n# 10   582\n# # ... with 770 more rows\nselect_cols(maize, starts_with(\"C\"))\n# # A tibble: 780 x 1\n#     CESP\n#    <dbl>\n#  1  16.9\n#  2  14.4\n#  3  16.5\n#  4  16.8\n#  5  15.9\n#  6  15  \n#  7  10.9\n#  8  15  \n#  9  13.6\n# 10  16.3\n# # ... with 770 more rows\nselect_cols(maize, -starts_with(\"C\"))\n# # A tibble: 780 x 9\n#    AMB   HIB   REP    APLA  AIES  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  52.0 213.   408.   523\n#  5 A1    H1    I      2.62  1.55  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   53.9 250.   430.   582\n# # ... with 770 more rows\nselect_cols(maize, ends_with(\"S\"))\n# # A tibble: 780 x 2\n#     AIES  DIES\n#    <dbl> <dbl>\n#  1  2.39  52.1\n#  2  1.43  50.7\n#  3  1.52  54.7\n#  4  1.64  52.0\n#  5  1.55  51.6\n#  6  1.8   51.4\n#  7  1.78  41.9\n#  8  1.84  53.4\n#  9  1.78  50.8\n# 10  1.6   53.9\n# # ... with 770 more rows\nselect_cols(maize, intersect_var(\"A\", \"S\"))\n# # A tibble: 780 x 1\n#     AIES\n#    <dbl>\n#  1  2.39\n#  2  1.43\n#  3  1.52\n#  4  1.64\n#  5  1.55\n#  6  1.8 \n#  7  1.78\n#  8  1.84\n#  9  1.78\n# 10  1.6 \n# # ... with 770 more rows\nselect_cols(maize, union_var(\"A\", \"S\"))\n# # A tibble: 780 x 4\n#    AMB    APLA  AIES  DIES\n#    <chr> <dbl> <dbl> <dbl>\n#  1 A1     2.45  2.39  52.1\n#  2 A1     2.5   1.43  50.7\n#  3 A1     2.69  1.52  54.7\n#  4 A1     2.8   1.64  52.0\n#  5 A1     2.62  1.55  51.6\n#  6 A1     2.12  1.8   51.4\n#  7 A1     3.15  1.78  41.9\n#  8 A1     2.97  1.84  53.4\n#  9 A1     3.1   1.78  50.8\n# 10 A1     3.02  1.6   53.9\n# # ... with 770 more rows\nselect_cols(maize, difference_var(\"A\", \"S\"))\n# # A tibble: 780 x 2\n#    AMB    APLA\n#    <chr> <dbl>\n#  1 A1     2.45\n#  2 A1     2.5 \n#  3 A1     2.69\n#  4 A1     2.8 \n#  5 A1     2.62\n#  6 A1     2.12\n#  7 A1     3.15\n#  8 A1     2.97\n#  9 A1     3.1 \n# 10 A1     3.02\n# # ... with 770 more rows\ndata_vars <- \n  maize %>%\n  rename(APLA_PLANT = APLA,\n         AIES_PLANT = AIES)\nnames(data_vars)\n#  [1] \"AMB\"        \"HIB\"        \"REP\"        \"APLA_PLANT\" \"AIES_PLANT\"\n#  [6] \"CESP\"       \"DIES\"       \"MGRA\"       \"MMG\"        \"NGRA\"\n\nselect_cols(data_vars, contains(\"PLANT\"))\n# # A tibble: 780 x 2\n#    APLA_PLANT AIES_PLANT\n#         <dbl>      <dbl>\n#  1       2.45       2.39\n#  2       2.5        1.43\n#  3       2.69       1.52\n#  4       2.8        1.64\n#  5       2.62       1.55\n#  6       2.12       1.8 \n#  7       3.15       1.78\n#  8       2.97       1.84\n#  9       3.1        1.78\n# 10       3.02       1.6 \n# # ... with 770 more rows\nselect_cols(maize, matches(\"^A[A-M]\"))\n# # A tibble: 780 x 2\n#    AMB    AIES\n#    <chr> <dbl>\n#  1 A1     2.39\n#  2 A1     1.43\n#  3 A1     1.52\n#  4 A1     1.64\n#  5 A1     1.55\n#  6 A1     1.8 \n#  7 A1     1.78\n#  8 A1     1.84\n#  9 A1     1.78\n# 10 A1     1.6 \n# # ... with 770 more rows\nselect_first_col(data_vars)\n# # A tibble: 780 x 1\n#    AMB  \n#    <chr>\n#  1 A1   \n#  2 A1   \n#  3 A1   \n#  4 A1   \n#  5 A1   \n#  6 A1   \n#  7 A1   \n#  8 A1   \n#  9 A1   \n# 10 A1   \n# # ... with 770 more rows\nselect_last_col(data_vars)\n# # A tibble: 780 x 1\n#     NGRA\n#    <dbl>\n#  1   609\n#  2   427\n#  3   497\n#  4   523\n#  5   551\n#  6   529\n#  7   294\n#  8   528\n#  9   538\n# 10   582\n# # ... with 770 more rows\nselect_cols(data_vars, width_of(4))\n# # A tibble: 780 x 4\n#     CESP  DIES  MGRA  NGRA\n#    <dbl> <dbl> <dbl> <dbl>\n#  1  16.9  52.1 228.    609\n#  2  14.4  50.7 187.    427\n#  3  16.5  54.7 230.    497\n#  4  16.8  52.0 213.    523\n#  5  15.9  51.6 224.    551\n#  6  15    51.4 203.    529\n#  7  10.9  41.9  75.2   294\n#  8  15    53.4 204.    528\n#  9  13.6  50.8 187.    538\n# 10  16.3  53.9 250.    582\n# # ... with 770 more rows\nselect_cols(data_vars, width_less_than(4))\n# # A tibble: 780 x 4\n#    AMB   HIB   REP     MMG\n#    <chr> <chr> <chr> <dbl>\n#  1 A1    H1    I      375.\n#  2 A1    H1    I      437.\n#  3 A1    H1    I      464.\n#  4 A1    H1    I      408.\n#  5 A1    H1    I      406.\n#  6 A1    H1    II     383.\n#  7 A1    H1    II     256.\n#  8 A1    H1    II     387.\n#  9 A1    H1    II     348.\n# 10 A1    H1    II     430.\n# # ... with 770 more rows\nselect_cols(data_vars, width_greater_than(3))\n# # A tibble: 780 x 6\n#    APLA_PLANT AIES_PLANT  CESP  DIES  MGRA  NGRA\n#         <dbl>      <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1       2.45       2.39  16.9  52.1 228.    609\n#  2       2.5        1.43  14.4  50.7 187.    427\n#  3       2.69       1.52  16.5  54.7 230.    497\n#  4       2.8        1.64  16.8  52.0 213.    523\n#  5       2.62       1.55  15.9  51.6 224.    551\n#  6       2.12       1.8   15    51.4 203.    529\n#  7       3.15       1.78  10.9  41.9  75.2   294\n#  8       2.97       1.84  15    53.4 204.    528\n#  9       3.1        1.78  13.6  50.8 187.    538\n# 10       3.02       1.6   16.3  53.9 250.    582\n# # ... with 770 more rows\ndf <- head(maize, 3)\ncolnames(df) <- c (\"Amg\", \"hib\", \"Rep\", \"APLA\", \"AIES\", \"CESp\", \"dies\", \"Mgra\", \"mmG\", \"ngra\")\nselect_cols(df, lower_case_only())\n# # A tibble: 3 x 3\n#   hib    dies  ngra\n#   <chr> <dbl> <dbl>\n# 1 H1     52.1   609\n# 2 H1     50.7   427\n# 3 H1     54.7   497\nselect_cols(df, upper_case_only())\n# # A tibble: 3 x 2\n#    APLA  AIES\n#   <dbl> <dbl>\n# 1  2.45  2.39\n# 2  2.5   1.43\n# 3  2.69  1.52\nselect_cols(df, title_case_only())\n# # A tibble: 3 x 3\n#   Amg   Rep    Mgra\n#   <chr> <chr> <dbl>\n# 1 A1    I      228.\n# 2 A1    I      187.\n# 3 A1    I      230."},{"path":"manipula.html","id":"remover-linhas-ou-colunas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.2 Remover linhas ou colunas","text":"Podemos usar remove_cols() e remove_rows() para remover colunas e linhas, respectivamente.funÃ§Ãµes remove_rows_na() e remove_rows_na() sÃ£o usados para remover linhas e colunas com valores NA, respectivamente. ","code":"\nremove_cols(maize, AMB, HIB)\n# # A tibble: 780 x 8\n#    REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 II     2.12  1.8   15    51.4 203.   383.   529\n#  7 II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 II     2.97  1.84  15    53.4 204.   387.   528\n#  9 II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\ndata_with_na <- maize\ndata_with_na[c (1, 5, 10), c (3:5, 9:10)] <- NA\nremove_cols_na(data_with_na)\n# Warning: Column(s) REP, APLA, AIES, MMG, NGRA with NA values deleted.\n# # A tibble: 780 x 5\n#    AMB   HIB    CESP  DIES  MGRA\n#    <chr> <chr> <dbl> <dbl> <dbl>\n#  1 A1    H1     16.9  52.1 228. \n#  2 A1    H1     14.4  50.7 187. \n#  3 A1    H1     16.5  54.7 230. \n#  4 A1    H1     16.8  52.0 213. \n#  5 A1    H1     15.9  51.6 224. \n#  6 A1    H1     15    51.4 203. \n#  7 A1    H1     10.9  41.9  75.2\n#  8 A1    H1     15    53.4 204. \n#  9 A1    H1     13.6  50.8 187. \n# 10 A1    H1     16.3  53.9 250. \n# # ... with 770 more rows\nremove_rows_na(data_with_na)\n# Warning: Row(s) 1, 5, 10 with NA values deleted.\n# # A tibble: 777 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  2 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  3 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  4 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  5 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  6 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  7 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n#  8 A1    H1    III    2.69  1.52  15.6  49.5 195.   369.   529\n#  9 A1    H1    III    2.6   1.68  14.3  48.9 172.   344.   500\n# 10 A1    H1    III    2.82  1.52  18.4  54.3 255.   371.   689\n# # ... with 767 more rows"},{"path":"manipula.html","id":"ordenar-linhas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.3 Ordenar linhas","text":"funÃ§Ã£o arrange()  Ã© utilizada para ordenar linhas de um tibble (ou data.frames) com base em uma expressÃ£o envolvendo suas variÃ¡veis. Considerando funÃ§Ãµes que vimos atÃ© aqui, vamos computar mÃ©dia para MGRA, criar uma nova variÃ¡vel chamada Rank, qual corresponde ao ranqueamento dos hÃ­bridos para variÃ¡vel em questÃ£o e ordenar variÃ¡vel Rank em ordem crescente, onde o hÃ­brido com maior mÃ©dia ficarÃ¡ na primeira linha.ExercÃ­cio 3Considerando o exemplo anterior, ordene variÃ¡vel Rank em ordem decrescente.RespostaAo combinar funÃ§Ã£o group_by() com arrange() Ã© possÃ­vel realizar o ordenamento para cada nÃ­vel de um determinado fator. exemplo abaixo, variÃ¡vel APLA Ã© ordenada de maneira crescente para cada hÃ­brido.","code":"\nmaize %>%\n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA)) %>%\n  mutate(Rank = rank(MGRA_mean)) %>%\n  arrange(-Rank)\n# # A tibble: 13 x 3\n#    HIB   MGRA_mean  Rank\n#    <chr>     <dbl> <dbl>\n#  1 H6         188.    13\n#  2 H2         187.    12\n#  3 H4         184.    11\n#  4 H1         184.    10\n#  5 H5         184.     9\n#  6 H13        180.     8\n#  7 H7         171.     7\n#  8 H3         169.     6\n#  9 H11        167.     5\n# 10 H10        164.     4\n# 11 H8         160.     3\n# 12 H12        157.     2\n# 13 H9         153.     1\n\nmaize %>%\n  group_by(HIB) %>%\n  arrange(APLA, .by_group = TRUE)\n# # A tibble: 780 x 10\n# # Groups:   HIB [13]\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A3    H1    II     1.93  0.93  13    50.0 120.   276.   433\n#  2 A3    H1    I      2     1.05  19.9  53.3 253.   444.   570\n#  3 A3    H1    I      2.07  1.05  13.2  47.9 110.   293.   377\n#  4 A3    H1    II     2.08  0.94  12    47.6 103.   334.   309\n#  5 A3    H1    III    2.1   0.97  17.5  50.8 222.   423.   524\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A3    H1    I      2.12  1.03  18.5  52.0 214.   382.   560\n#  8 A3    H1    III    2.12  0.96  15    56.8 174.   339.   512\n#  9 A3    H1    I      2.13  1.05  11.6  47.0  89.5  300.   298\n# 10 A4    H1    I      2.13  1.1   12.8  47.6 144.   280.   516\n# # ... with 770 more rows"},{"path":"manipula.html","id":"selecionar-top-n-linhas-baseado-em-valor","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.4 Selecionar top n linhas baseado em valor","text":"funÃ§Ã£o top_n() Ã© usada para selecionar linhas superiores ou inferiores em cada grupo.","code":"\n# seleciona as duas linhas com o maior valor de MGRA\ntop_n(maize, 2, MGRA)\n# # A tibble: 2 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    I      2.92  1.64  18    56.0  289.  393.   734\n# 2 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n\n# seleciona as duas linhas com o menor valor de MGRA\ntop_n(maize, 2, -MGRA)\n# # A tibble: 2 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H9    III    2.72  1.54  11    42.8  58.5  295.   198\n# 2 A2    H8    I      1.92  0.63  12.1  39.7  59.5  243.   245\n\n# Maior produtividade em cada ambiente\nmaize %>%\n  group_by(AMB) %>%\n  top_n(1, MGRA)\n# # A tibble: 4 x 10\n# # Groups:   AMB [4]\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n# 2 A2    H6    III    3.18  1.62  19.2  53.0  270.  382.   708\n# 3 A3    H1    I      2     1.05  19.9  53.3  253.  444.   570\n# 4 A4    H10   I      2.65  1.47  14    50.3  287.  275.   493"},{"path":"manipula.html","id":"adicionar-novas-variÃ¡veis","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.5 Adicionar novas variÃ¡veis","text":"funÃ§Ã£o mutate()  Ã© utilizada quando se deseja adicionar novas variÃ¡veis conjunto de dados. Estas variÃ¡veis sÃ£o funÃ§Ãµes de variÃ¡veis existentes. Como exemplo, vamos criar uma nova variÃ¡vel chamada PRE_2 conjunto de dados maize, qual serÃ¡ razÃ£o entre AIES e APLA. Note que funÃ§Ã£o adiciona nova variÃ¡vel apÃ³s Ãºltima variÃ¡vel origina e mantÃ©m todas demais. Digamos que querÃ­amos adicionar nova variÃ¡vel criada apÃ³s variÃ¡vel REP, seguinte abordagem com o pacote dplyr deve ser usada.Com funÃ§Ã£o add_cols(), o mesmo resultado pode ser obtido com:ExercÃ­cio 2Crie uma variÃ¡vel chamada MGRA_kg qual serÃ¡ o resultado em quilogramas da massa de grÃ£os.Crie uma variÃ¡vel chamada MGRA_kg qual serÃ¡ o resultado em quilogramas da massa de grÃ£os.Selecione somente colunas HIB, AMB, REP e MGRA_Kg.Selecione somente colunas HIB, AMB, REP e MGRA_Kg.Selecione somente cinco linhas com maior valor de MGRA_Kg.Selecione somente cinco linhas com maior valor de MGRA_Kg.Resposta","code":"\n\nmaize %>% \n  mutate(PRE_2 = AIES/APLA) %>%\n  select(AMB, HIB, REP, PRE_2, everything())\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   PRE_2  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     0.976  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     0.572  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     0.565  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     0.586  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     0.592  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    0.849  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    0.565  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    0.620  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    0.574  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    0.530  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\n\nadd_cols(maize,\n         PRE_2 = AIES/APLA,\n         .after = \"REP\")\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   PRE_2  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     0.976  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     0.572  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     0.565  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     0.586  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     0.592  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    0.849  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    0.565  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    0.620  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    0.574  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    0.530  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"concatenar-colunas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.6 Concatenar colunas","text":"funÃ§Ã£o concatenate() pode ser usada para concatenar vÃ¡rias colunas de um conjunto de dados. concatenate() retorna um quadro de dados com todas colunas originais em .data mais variÃ¡vel concatenada, apÃ³s Ãºltima coluna. Para escolher posiÃ§Ã£o da nova variÃ¡vel, use o argumento .ou., como seguir.Para eliminar variÃ¡veis existentes e manter apenas coluna concatenada, use o argumento drop = TRUE. Para usar concatenate() dentro de uma determinada funÃ§Ã£o como add_cols() use o argumento pull = TRUE para extrair os resultados para um vetor.","code":"\nconcatenate(maize, AMB:REP, .after = \"REP\")\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   new_var   APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     A1_H1_I   2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     A1_H1_I   2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     A1_H1_I   2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     A1_H1_I   2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     A1_H1_I   2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    A1_H1_II  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    A1_H1_II  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    A1_H1_II  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    A1_H1_II  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    A1_H1_II  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nconcatenate(maize, AMB:REP, drop = TRUE)\n# # A tibble: 780 x 1\n#    new_var \n#    <chr>   \n#  1 A1_H1_I \n#  2 A1_H1_I \n#  3 A1_H1_I \n#  4 A1_H1_I \n#  5 A1_H1_I \n#  6 A1_H1_II\n#  7 A1_H1_II\n#  8 A1_H1_II\n#  9 A1_H1_II\n# 10 A1_H1_II\n# # ... with 770 more rows\nconcatenate(maize, AMB:REP, pull = TRUE) %>% head()\n# [1] \"A1_H1_I\"  \"A1_H1_I\"  \"A1_H1_I\"  \"A1_H1_I\"  \"A1_H1_I\"  \"A1_H1_II\""},{"path":"manipula.html","id":"formatar-nomes-de-coluna","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.7 Formatar nomes de coluna","text":"funÃ§Ãµes colnames_to_lower(), colnames_to_upper() e colnames_to_title() podem ser usados para converter nomes de colunas em maiÃºsculas, minÃºsculas ou em formato de tÃ­tulo, respectivamente .  ","code":"\ncolnames_to_lower(maize)\n# # A tibble: 780 x 10\n#    amb   hib   rep    apla  aies  cesp  dies  mgra   mmg  ngra\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\ncolnames_to_upper(maize)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\ncolnames_to_title(maize)\n# # A tibble: 780 x 10\n#    Amb   Hib   Rep    Apla  Aies  Cesp  Dies  Mgra   Mmg  Ngra\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"reordenando-colunas","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.8 Reordenando colunas","text":"funÃ§Ã£o reorder_cols() pode ser usada para reordenar colunas de um quadro de dados.Ã‰ possÃ­vel colocar colunas primeiro e Ãºltimo lugar rapidamente com columns_to_first() e columns_to_last(), respectivamente.","code":"\nreorder_cols(data_vars, contains(\"PLANT\"), .before = \"AMB\")\n# # A tibble: 780 x 10\n#    APLA_PLANT AIES_PLANT AMB   HIB   REP    CESP  DIES  MGRA   MMG  NGRA\n#         <dbl>      <dbl> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1       2.45       2.39 A1    H1    I      16.9  52.1 228.   375.   609\n#  2       2.5        1.43 A1    H1    I      14.4  50.7 187.   437.   427\n#  3       2.69       1.52 A1    H1    I      16.5  54.7 230.   464.   497\n#  4       2.8        1.64 A1    H1    I      16.8  52.0 213.   408.   523\n#  5       2.62       1.55 A1    H1    I      15.9  51.6 224.   406.   551\n#  6       2.12       1.8  A1    H1    II     15    51.4 203.   383.   529\n#  7       3.15       1.78 A1    H1    II     10.9  41.9  75.2  256.   294\n#  8       2.97       1.84 A1    H1    II     15    53.4 204.   387.   528\n#  9       3.1        1.78 A1    H1    II     13.6  50.8 187.   348.   538\n# 10       3.02       1.6  A1    H1    II     16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nreorder_cols(data_vars, AMB, HIB, .after = \"REP\")\n# # A tibble: 780 x 10\n#    REP   AMB   HIB   APLA_PLANT AIES_PLANT  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr>      <dbl>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 I     A1    H1          2.45       2.39  16.9  52.1 228.   375.   609\n#  2 I     A1    H1          2.5        1.43  14.4  50.7 187.   437.   427\n#  3 I     A1    H1          2.69       1.52  16.5  54.7 230.   464.   497\n#  4 I     A1    H1          2.8        1.64  16.8  52.0 213.   408.   523\n#  5 I     A1    H1          2.62       1.55  15.9  51.6 224.   406.   551\n#  6 II    A1    H1          2.12       1.8   15    51.4 203.   383.   529\n#  7 II    A1    H1          3.15       1.78  10.9  41.9  75.2  256.   294\n#  8 II    A1    H1          2.97       1.84  15    53.4 204.   387.   528\n#  9 II    A1    H1          3.1        1.78  13.6  50.8 187.   348.   538\n# 10 II    A1    H1          3.02       1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\ncolumn_to_first(maize, CESP, DIES)\n# # A tibble: 780 x 10\n#     CESP  DIES AMB   HIB   REP    APLA  AIES  MGRA   MMG  NGRA\n#    <dbl> <dbl> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1  16.9  52.1 A1    H1    I      2.45  2.39 228.   375.   609\n#  2  14.4  50.7 A1    H1    I      2.5   1.43 187.   437.   427\n#  3  16.5  54.7 A1    H1    I      2.69  1.52 230.   464.   497\n#  4  16.8  52.0 A1    H1    I      2.8   1.64 213.   408.   523\n#  5  15.9  51.6 A1    H1    I      2.62  1.55 224.   406.   551\n#  6  15    51.4 A1    H1    II     2.12  1.8  203.   383.   529\n#  7  10.9  41.9 A1    H1    II     3.15  1.78  75.2  256.   294\n#  8  15    53.4 A1    H1    II     2.97  1.84 204.   387.   528\n#  9  13.6  50.8 A1    H1    II     3.1   1.78 187.   348.   538\n# 10  16.3  53.9 A1    H1    II     3.02  1.6  250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"obtendo-nÃ­veis-de-fatores","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.9 Obtendo nÃ­veis de fatores","text":"Para obter os nÃ­veis e o tamanho dos nÃ­veis de um fator, funÃ§Ãµes get_levels() e get_level_size() pode ser usado.\r\nUtilizando funÃ§Ã£o case_when()  Ã© possÃ­vel criar uma variÃ¡vel baseado em um argumento () vetorizado. case_when() Ã© particularmente Ãºtil dentro da funÃ§Ã£o mutate() quando vocÃª quer criar uma nova variÃ¡vel que depende de uma combinaÃ§Ã£o complexa de variÃ¡veis existentes. exemplo abaixo, uma nova variÃ¡vel serÃ¡ criada, dependendo dos valores de APLA, AIES ou CESP","code":"\nget_levels(maize, AMB)\n# [1] \"A1\" \"A2\" \"A3\" \"A4\"\nget_level_size(maize, AMB)\n#  A1  A2  A3  A4 \n# 195 195 195 195\n\nmaize %>% \n  mutate(\n    CASE = case_when(\n      MGRA > 280 | APLA < 1.3 | NGRA > 820 ~  \"Selecionar\",\n      APLA > 2.3 ~ \"Alto\",\n      MGRA < 130 ~ \"Pouco produtivo\",\n      TRUE ~ \"Outro\"\n    )\n  ) %>% \n  select_non_numeric_cols()\n# # A tibble: 780 x 4\n#    AMB   HIB   REP   CASE \n#    <chr> <chr> <chr> <chr>\n#  1 A1    H1    I     Alto \n#  2 A1    H1    I     Alto \n#  3 A1    H1    I     Alto \n#  4 A1    H1    I     Alto \n#  5 A1    H1    I     Alto \n#  6 A1    H1    II    Outro\n#  7 A1    H1    II    Alto \n#  8 A1    H1    II    Alto \n#  9 A1    H1    II    Alto \n# 10 A1    H1    II    Alto \n# # ... with 770 more rows"},{"path":"manipula.html","id":"selecionar-linhas-com-base-em-seus-valores","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.1.10 Selecionar linhas com base em seus valores","text":"Utilizando funÃ§Ã£o filter()  Ã© possivel filtrar linhas de um conjunto de dados com base valor de suas variÃ¡veis. primeiro exemplo, selecionaremos linhas onde o valor da variÃ¡vel MGRA Ã© maior que 280.segundo exemplo, selecionaremos apenas linhas onde MGRA Ã© maior que 220 OU APLA Ã© menor que 1.3 OU o NGRA Ã© maior que 820.Ãºltimo exemplo, selecionaremos apenas linhas onde MGRA Ã© maior que Ã© maior que 220 E APLA Ã© menor que 2.Isto Ã© aproximadamente equivalente ao seguinte cÃ³digo R base.","code":"\nmaize %>% \n  filter(MGRA > 280)\n# # A tibble: 4 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    I      2.92  1.64  18    56.0  289.  393.   734\n# 2 A1    H10   I      2.92  1.61  20.3  55.4  283.  441.   641\n# 3 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n# 4 A4    H10   I      2.65  1.47  14    50.3  287.  275.   493\nmaize %>% \n  filter(MGRA > 280 | APLA < 1.3 | NGRA > 820)\n# # A tibble: 10 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H6    I      2.92  1.64  18    56.0 289.   393.   734\n#  2 A1    H10   I      2.92  1.61  20.3  55.4 283.   441.   641\n#  3 A1    H13   II     2.47  1.28  15.3  53.0 291.   417.   698\n#  4 A2    H8    II     1.03  0.69  10.8  44.8  94.8  277.   342\n#  5 A2    H10   III    1.09  0.92  15    47.6 166.   299.   555\n#  6 A3    H10   I      1.04  0.71  14.8  45.5 112.   265.   423\n#  7 A3    H11   I      1     0.65  14.5  43.6 120.   210.   571\n#  8 A4    H8    I      2.65  1.67  18    50   277.   251.   903\n#  9 A4    H8    I      2.95  1.7   18.6  52.9 249.   302.   824\n# 10 A4    H10   I      2.65  1.47  14    50.3 287.   275.   493\nmaize %>% \n  filter(MGRA > 220 & APLA < 2)\n# # A tibble: 1 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    II     1.97  1.63  17.1  54.7  230.  375.   614\nmaize[maize$MGRA > 220 & maize$APLA < 2, ]"},{"path":"manipula.html","id":"trabalhando-com-nÃºmeros-e-seqÃ¼Ãªncias-de-caracteres","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.2 Trabalhando com nÃºmeros e seqÃ¼Ãªncias de caracteres","text":"","code":""},{"path":"manipula.html","id":"arredondando","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.2.1 Arredondando","text":"funÃ§Ã£o round_cols() arredonda uma coluna selecionada ou um quadro de dados inteiro para o nÃºmero especificado de casas decimais (padrÃ£o 0). Se nenhuma variÃ¡vel informada, todas variÃ¡veis numÃ©ricas serÃ£o arredondadas.Como alternativa, selecione variÃ¡veis para arredondar.","code":"\nround_cols(maize)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 214.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nround_cols(maize, MGRA, MMG, digits = 1)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375    609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 214.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406    551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"extraindo-e-substituindo-nÃºmeros","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.2.2 Extraindo e substituindo nÃºmeros","text":"funÃ§Ãµes extract_number() e replace_number() pode ser usado para extrair ou substituir nÃºmeros. Como exemplo, extrairemos o nÃºmero de cada genÃ³tipo em data_g. Por padrÃ£o, os nÃºmeros extraÃ­dos sÃ£o colocados como uma nova variÃ¡vel chamada new_var apÃ³s Ãºltima coluna dos dados.Se o argumento drop estiver definido como TRUE, apenas nova variÃ¡vel serÃ¡ mantida e todas outras serÃ£o descartadas.Para extrair os resultados em um vetor, use o argumento pull = TRUE. Isso Ã© particularmente Ãºtil quando extract_ * ou replace_ * sÃ£o usados em uma funÃ§Ã£o como add_cols().Para substituir nÃºmeros de uma determinada coluna por uma substituiÃ§Ã£o especificada, use replace_number(). Por padrÃ£o, os nÃºmeros sÃ£o substituÃ­dos por \"\". O argumento drop epull tambÃ©m podem ser usados, como mostrado acima.","code":"\nextract_number(maize, HIB, .after = \"HIB\")\n# # A tibble: 780 x 11\n#    AMB   HIB   new_var REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr>   <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1          1 I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1          1 I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1          1 I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1          1 I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1          1 I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1          1 II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1          1 II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1          1 II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1          1 II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1          1 II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nextract_number(maize, HIB, drop = TRUE)\n# # A tibble: 780 x 1\n#    new_var\n#      <dbl>\n#  1       1\n#  2       1\n#  3       1\n#  4       1\n#  5       1\n#  6       1\n#  7       1\n#  8       1\n#  9       1\n# 10       1\n# # ... with 770 more rows\nextract_number(maize, HIB, pull = TRUE)\n#   [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2\n#  [26]  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4\n#  [51]  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5\n#  [76]  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7\n# [101]  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9\n# [126]  9  9  9  9  9  9  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10\n# [151] 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12\n# [176] 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13  1  1  1  1  1\n# [201]  1  1  1  1  1  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n# [226]  3  3  3  3  3  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4\n# [251]  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6\n# [276]  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n# [301]  8  8  8  8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9\n# [326]  9  9  9  9  9 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11\n# [351] 11 11 11 11 11 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12\n# [376] 13 13 13 13 13 13 13 13 13 13 13 13 13 13 13  1  1  1  1  1  1  1  1  1  1\n# [401]  1  1  1  1  1  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3\n# [426]  3  3  3  3  3  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n# [451]  5  5  5  5  5  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6\n# [476]  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  8  8  8  8  8\n# [501]  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9\n# [526] 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11\n# [551] 11 11 11 11 11 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13\n# [576] 13 13 13 13 13 13 13 13 13 13  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n# [601]  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3\n# [626]  3  3  3  3  3  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5\n# [651]  5  5  5  5  5  5  5  5  5  5  6  6  6  6  6  6  6  6  6  6  6  6  6  6  6\n# [676]  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  8  8  8  8  8  8  8  8  8  8\n# [701]  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 10 10 10 10 10\n# [726] 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n# [751] 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13\n# [776] 13 13 13 13 13\nreplace_number(maize, HIB)\n# # A tibble: 780 x 11\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA new_var\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <chr>  \n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609 H      \n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427 H      \n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497 H      \n#  4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523 H      \n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551 H      \n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529 H      \n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294 H      \n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528 H      \n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538 H      \n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582 H      \n# # ... with 770 more rows\nreplace_number(maize,\n               var = REP,\n               pattern = \"^I$\",\n               replacement = \"REP_1\",\n               new_var = R_ONE,\n               .after = \"REP\")\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   R_ONE  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     REP_1  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     REP_1  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     REP_1  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     REP_1  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     REP_1  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"extraindo-substituindo-e-removendo-strings","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.2.3 Extraindo, substituindo e removendo strings","text":"funÃ§Ãµes extract_string() e replace_string() sÃ£o usados mesmo contexto de extract_number() e replace_number(), mas para lidar com seqÃ¼Ãªncias de caracteres.Para substituir strings, podemos usar funÃ§Ã£o replace_strings().Para remover todas seqÃ¼Ãªncias de caracteres de um quadro de dados, use remove_strings().","code":"\nextract_string(maize, HIB, .after = \"HIB\")\n# # A tibble: 780 x 11\n#    AMB   HIB   new_var REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr>   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    H       I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    H       I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    H       I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    H       I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    H       I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    H       II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    H       II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    H       II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    H       II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    H       II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nreplace_string(maize,\n               var = HIB,\n               new_var = HIBRIDO,\n               replacement = \"HIB_\",\n               .after = \"HIB\")\n# # A tibble: 780 x 11\n#    AMB   HIB   HIBRIDO REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr>   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    HIB_1   I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    HIB_1   I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    HIB_1   I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    HIB_1   I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    HIB_1   I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    HIB_1   II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    HIB_1   II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    HIB_1   II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    HIB_1   II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    HIB_1   II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nremove_strings(maize)\n# # A tibble: 780 x 10\n#      AMB   HIB   REP  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1     1     1    NA  2.45  2.39  16.9  52.1 228.   375.   609\n#  2     1     1    NA  2.5   1.43  14.4  50.7 187.   437.   427\n#  3     1     1    NA  2.69  1.52  16.5  54.7 230.   464.   497\n#  4     1     1    NA  2.8   1.64  16.8  52.0 213.   408.   523\n#  5     1     1    NA  2.62  1.55  15.9  51.6 224.   406.   551\n#  6     1     1    NA  2.12  1.8   15    51.4 203.   383.   529\n#  7     1     1    NA  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8     1     1    NA  2.97  1.84  15    53.4 204.   387.   528\n#  9     1     1    NA  3.1   1.78  13.6  50.8 187.   348.   538\n# 10     1     1    NA  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"formatando-strings","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.2.4 Formatando strings","text":"funÃ§Ã£o tidy_strings() organiza cadeias de caracteres, colunas nÃ£o numÃ©ricas ou quaisquer colunas selecionadas em um quadro de dados, colocando todas palavras em maiÃºsculas, substituindo qualquer espaÃ§o, tabulaÃ§Ã£o e caracteres de pontuaÃ§Ã£o por _ e colocando _ entre letras maiÃºsculas e minÃºsculas. Considere seguintes cadeias de caracteres: messy_env por definiÃ§Ã£o deve representar um nÃ­vel Ãºnico ambiente de fator (ambiente 1). messy_gen mostra seis genÃ³tipos, emessy_int representa interaÃ§Ã£o desses genÃ³tipos com o ambiente 1.Esses vetores de caracteres sÃ£o visualmente confusos. Vamos arrumÃ¡-los.O tidy_strings() tambÃ©m funciona para arrumar um quadro de dados inteiro ou colunas especÃ­ficas. Vamos criar um quadro de dados â€˜bagunÃ§adoâ€™ contexto de testes de melhoramento de plantas.","code":"\nmessy_env <- c(\"ENV 1\", \"Env 1\", \"Env1\", \"env1\", \"Env.1\", \"Env_1\")\nmessy_gen <- c(\"GEN1\", \"gen 2\", \"Gen.3\", \"gen-4\", \"Gen_5\", \"GEN_6\")\nmessy_int <- c(\"Env1Gen1\", \"Env1_Gen2\", \"env1 gen3\", \"Env1 Gen4\", \"ENV_1GEN5\", \"ENV1GEN6\")\ntidy_strings(messy_env)\n# [1] \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\"\ntidy_strings(messy_gen)\n# [1] \"GEN_1\" \"GEN_2\" \"GEN_3\" \"GEN_4\" \"GEN_5\" \"GEN_6\"\ntidy_strings(messy_int)\n# [1] \"ENV_1_GEN_1\" \"ENV_1_GEN_2\" \"ENV_1_GEN_3\" \"ENV_1_GEN_4\" \"ENV_1_GEN_5\"\n# [6] \"ENV_1_GEN_6\"\nlibrary(tibble)\ndf <- tibble(Env = messy_env,\n             gen = messy_gen,\n             Env_GEN = interaction(Env, gen),\n             y = rnorm (6, 300, 10))\ndf\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <fct>       <dbl>\n# 1 ENV 1 GEN1  ENV 1.GEN1   294.\n# 2 Env 1 gen 2 Env 1.gen 2  300.\n# 3 Env1  Gen.3 Env1.Gen.3   291.\n# 4 env1  gen-4 env1.gen-4   302.\n# 5 Env.1 Gen_5 Env.1.Gen_5  293.\n# 6 Env_1 GEN_6 Env_1.GEN_6  318.\ntidy_strings(df)\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <chr>       <dbl>\n# 1 ENV_1 GEN_1 ENV_1_GEN_1  294.\n# 2 ENV_1 GEN_2 ENV_1_GEN_2  300.\n# 3 ENV_1 GEN_3 ENV_1_GEN_3  291.\n# 4 ENV_1 GEN_4 ENV_1_GEN_4  302.\n# 5 ENV_1 GEN_5 ENV_1_GEN_5  293.\n# 6 ENV_1 GEN_6 ENV_1_GEN_6  318.\ntidy_strings(df, gen)\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <fct>       <dbl>\n# 1 ENV 1 GEN_1 ENV 1.GEN1   294.\n# 2 Env 1 GEN_2 Env 1.gen 2  300.\n# 3 Env1  GEN_3 Env1.Gen.3   291.\n# 4 env1  GEN_4 env1.gen-4   302.\n# 5 Env.1 GEN_5 Env.1.Gen_5  293.\n# 6 Env_1 GEN_6 Env_1.GEN_6  318."},{"path":"manipula.html","id":"selecionar-linhas-por-sua-posiÃ§Ã£o","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.3 Selecionar linhas por sua posiÃ§Ã£o","text":"funÃ§Ã£o slice() Ã© usada para selecionar linhas por sua posiÃ§Ã£o ordinal tibble. Os tibbles agrupados usam posiÃ§Ã£o ordinal dentro grupo.","code":"\n# seleciona as trÃªs primeiras linhas\nslice(maize, 1:3)\n# # A tibble: 3 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# Seleciona as 3 Ãºltimas linhas\nslice(maize, 778:n())\n# # A tibble: 3 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A4    H13   III    2.2   0.9   12.1  40.9  92.8  322.   288\n# 2 A4    H13   III    2.15  1.07  10.6  46.0  91.4  300.   305\n# 3 A4    H13   III    2.19  1.12  14.5  51.9 144.   352.   408\n# seleciona as duas primeiras linhas de cada ambiente\nmaize %>%\n  group_by(AMB) %>%\n  slice(1:2)\n# # A tibble: 8 x 10\n# # Groups:   AMB [4]\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A2    H1    I      3.06  1.89  18.2  54.4  244.  421.   581\n# 4 A2    H1    I      3.04  1.89  15.4  53.4  193.  369.   523\n# 5 A3    H1    I      2.12  1.03  18.5  52.0  214.  382.   560\n# 6 A3    H1    I      2     1.05  19.9  53.3  253.  444.   570\n# 7 A4    H1    I      2.13  1.1   12.8  47.6  144.  280.   516\n# 8 A4    H1    I      2.3   1.25  13.1  50.0  140.  230.   609"},{"path":"manipula.html","id":"combinando-os-verbos-para-manipulaÃ§Ã£o","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.4 Combinando os verbos para manipulaÃ§Ã£o","text":"Esta sessÃ£o tem o objetivo de demonstrar como os verbos dplyr em conjunto com funÃ§Ãµes pivot_longer() pacote tidyr21 e column_to_rownames()  pacote tibble22 podem ser combinados para construir uma matriz dupla entrada onde linhas correspondem aos genÃ³tipos e colunas correspondem aos ambientes. Esta matriz serÃ¡ preenchida com o valor mÃ©dio da MGRA considerando apenas duas primeiras repetiÃ§Ãµes de cada hÃ­brido em cada ambiente.\r\n\r\n\r\n\r\n\r\nNote que mesma tabela dupla entrada pode ser obtida com funÃ§Ã£o make_mat() pacote metan.","code":"\nmaize %>%\n  filter(REP %in% c(\"I\", \"II\")) %>%\n  group_by(AMB, HIB) %>%\n  summarise(MGRA_me = mean(MGRA)) %>%\n  pivot_wider(names_from = HIB, values_from = MGRA_me) %>%\n  round_cols(digits = 1)\n# # A tibble: 4 x 14\n# # Groups:   AMB [4]\n#   AMB      H1   H10   H11   H12   H13    H2    H3    H4    H5    H6    H7    H8\n#   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1     200.  185.  199.  174.  222.  205.  201.  204.  190.  238.  185.  198.\n# 2 A2     194.  151.  169.  136.  158.  219.  201.  193.  180.  204.  148.  116 \n# 3 A3     147.  117   127.  153.  186.  161.  146.  150   147.  127.  146.  149.\n# 4 A4     195.  180.  170.  191.  167   156.  148.  182.  209.  164.  195.  182.\n# # ... with 1 more variable: H9 <dbl>\nmaize %>%\n  filter(REP %in% c(\"I\", \"II\")) %>%\n  make_mat(AMB, HIB, MGRA) %>% \n  round_cols(digits = 1)\n#       H1   H10   H11   H12   H13    H2    H3    H4    H5    H6    H7    H8\n# A1 200.2 185.1 199.3 174.5 221.9 204.9 201.3 204.4 189.5 238.2 184.8 198.2\n# A2 193.8 151.3 168.7 136.2 158.3 218.8 200.6 193.3 180.1 204.5 148.2 116.0\n# A3 147.2 117.0 127.1 153.1 186.5 160.6 146.4 150.0 147.1 127.4 146.4 148.7\n# A4 195.2 179.5 169.5 190.9 167.0 155.5 147.7 182.1 208.7 164.3 195.4 181.9\n#       H9\n# A1 203.4\n# A2 107.2\n# A3 117.3\n# A4 154.8"},{"path":"manipula.html","id":"trabalhando-com-duas-tabelas-ao-mesmo-tempo","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5 Trabalhando com duas tabelas ao mesmo tempo","text":"","code":""},{"path":"manipula.html","id":"junÃ§Ãµes-com-mutaÃ§Ã£o-de-dados","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.1 JunÃ§Ãµes com mutaÃ§Ã£o de dados","text":"Ã‰ raro que uma anÃ¡lise de dados envolva apenas uma Ãºnica tabela de dados. Na prÃ¡tica, diversas tabela podem existir e ferramentas flexÃ­veis para combinÃ¡-las sÃ£o necessÃ¡rias. dplyr, existem trÃªs famÃ­lias de verbos que permitem trabalhar com duas tabelas ao mesmo tempo, permitindo: () juntar tabelas, (ii) Filtrar registros e (iii) realizar operaÃ§Ãµes.Os seguintes cÃ³digos criam trÃªs novos conjuntos de dados. maize2 contÃ©m dados de duas repetiÃ§Ãµes para os hÃ­bridos H1:H5 nos ambientes H1 e H2. mean_h e mean_a contÃ©m mÃ©dias para os hÃ­bridos e ambientes, respectivamente. Juntando coluna MGRA e NGRA da tabela mean_h na tabela maize2 considerando variÃ¡veis com mesmo nome nas duas tabelas (neste caso, HIB)Juntando colunas da tabela mean_a na tabela maize2","code":"\nmaize_small <- \n  maize %>%\n  filter(HIB %in% c(\"H1\", \"H2\", \"H3\")) %>%\n  filter(AMB %in% c(\"A1\", \"A2\"))\n\nmaize2 <- \n  maize_small %>% \n  means_by(AMB, HIB) %>% \n  select(AMB:APLA) %>% \n  round_cols(digits = 1)\nmaize2\n# # A tibble: 6 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H1      2.7\n# 2 A1    H2      2.8\n# 3 A1    H3      2.9\n# 4 A2    H1      2.9\n# 5 A2    H2      2.9\n# 6 A2    H3      2.9\n\nmean_h <- \n  maize_small %>%\n  means_by(HIB) %>% \n  select(HIB, contains(\"A\")) %>% \n  round_cols(digits = 1)\n  \nmean_a <-\n  maize_small %>%\n  means_by(AMB) %>% \n  select(AMB, contains(\"ES\")) %>% \n  round_cols(digits = 1)\nleft_join(maize2, mean_h %>% select(HIB, MGRA, NGRA), by = \"HIB\")\n# # A tibble: 6 x 5\n#   AMB   HIB    APLA  MGRA  NGRA\n#   <chr> <chr> <dbl> <dbl> <dbl>\n# 1 A1    H1      2.7  195.  506.\n# 2 A1    H2      2.8  211.  577.\n# 3 A1    H3      2.9  195.  542.\n# 4 A2    H1      2.9  195.  506.\n# 5 A2    H2      2.9  211.  577.\n# 6 A2    H3      2.9  195.  542.\nfull_join(maize2, mean_a, by = \"AMB\")\n# # A tibble: 6 x 6\n#   AMB   HIB    APLA  AIES  CESP  DIES\n#   <chr> <chr> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1      2.7   1.5  15.3  51.6\n# 2 A1    H2      2.8   1.5  15.3  51.6\n# 3 A1    H3      2.9   1.5  15.3  51.6\n# 4 A2    H1      2.9   1.8  15.4  51.8\n# 5 A2    H2      2.9   1.8  15.4  51.8\n# 6 A2    H3      2.9   1.8  15.4  51.8"},{"path":"manipula.html","id":"junÃ§Ãµes-com-filtragem-de-dados","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.2 JunÃ§Ãµes com filtragem de dados","text":"Filtrando linhas da tabela maize2 com base nas variÃ¡veis que combinam na tabela mean_h (neste caso, coluna HIB) Filtrando linhas da tabela maize2 com base nas variÃ¡veis que NÃƒO combinam na tabela mean_h (neste caso, coluna HIB) ","code":"\nsemi_join(maize2, mean_h, by = c(\"HIB\", \"APLA\"))\n# # A tibble: 3 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H3      2.9\n# 2 A2    H2      2.9\n# 3 A2    H3      2.9\nanti_join(maize2, mean_h, by = c(\"HIB\", \"APLA\"))\n# # A tibble: 3 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H1      2.7\n# 2 A1    H2      2.8\n# 3 A2    H1      2.9"},{"path":"manipula.html","id":"operaÃ§Ãµes-com-conjuntos","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.3 OperaÃ§Ãµes com conjuntos","text":"Nesta seÃ§Ã£o serÃ¡ demonstrado como Ã© possivel utilizar operaÃ§Ãµes de cojuntos como interseÃ§Ã£o e uniÃ£o. Ã‰ esperado que entradas x e y tenham mesmas variÃ¡veis. Para isto, vamos criar dois novos conjuntos de dados chamados data_1_to_5 e data_3_to_10, quais contÃ©m, respectivamente cinco primeiras linhas\r\ne linhas 3 10 de maize. Note que funÃ§Ã£o slice()  Ã© utilizada para selecionar linhas com base em sua posiÃ§Ã£o.","code":"\ndata_1_to_5 <- \n  maize %>%\n  slice(1:5) %>% \n  add_cols(id = 1:5, .before = 1)\ndata_3_to_10 <- \n  maize %>%\n  slice(3:10) %>% \n  add_cols(id = 3:10, .before = 1)"},{"path":"manipula.html","id":"interseÃ§Ã£o-de-conjuntos","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.3.1 InterseÃ§Ã£o de conjuntos","text":"funÃ§Ã£o intersect() (interseÃ§Ã£o de conjunto) retorna somente linhas presentes nos dois conjuntos, neste caso, linhas 3, 4 5 conjunto maize","code":"\nintersect(data_1_to_5, data_3_to_10)\n# # A tibble: 3 x 11\n#      id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1     3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# 2     4 A1    H1    I      2.8   1.64  16.8  52.0  213.  408.   523\n# 3     5 A1    H1    I      2.62  1.55  15.9  51.6  224.  406.   551"},{"path":"manipula.html","id":"uniÃ£o-de-conjuntos","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.3.2 UniÃ£o de conjuntos","text":"funÃ§Ã£o union() (uniÃ£o de conjunto) junta os dois conjuntos sem que haja duplicaÃ§Ã£o de registros.","code":"\nunion(data_1_to_5, data_3_to_10)\n# # A tibble: 10 x 11\n#       id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1     1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2     2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3     3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4     4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5     5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6     6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7     7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8     8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9     9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10    10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582"},{"path":"manipula.html","id":"diferenÃ§a-de-conjuntos","chapter":"CapÃ­tulo 7 ManipulaÃ§Ã£o de dados","heading":"7.5.3.3 DiferenÃ§a de conjuntos","text":"funÃ§Ã£o setdiff()  (diferenÃ§de conjunto, ou complementar) cria uma tabela somente com os registros em data_1_to_5 que nÃ£o estÃ£o em data_3_to_10.","code":"\nsetdiff(data_1_to_5, data_3_to_10)\n# # A tibble: 2 x 11\n#      id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1     1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2     2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427"},{"path":"graph.html","id":"graph","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","text":"â€œO grÃ¡fico simples trouxe mais informaÃ§Ãµes Ã  mente analista de dados\r\nque qualquer outro dispositivo.â€ â€” John Tukey","code":""},{"path":"graph.html","id":"o-pacote-ggplot2","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.1 O pacote ggplot2","text":"O ggplot2 (https://ggplot2.tidyverse.org/)\\indt{ggplot2} Ã© um pacote R para produÃ§Ã£o de grÃ¡ficos que diferentemente da maioria dos outros pacotes, apresenta uma profunda gramÃ¡tica baseada livro grammar graphics (Wilkinson 2005). Os grÃ¡ficos originados em ggplot2 sÃ£o baseados em camadas, e cada grÃ¡fico tem trÃªs componentes chave: data, os dados de onde o grÃ¡fico serÃ¡ criado; aes() (aesthetic mappings), que controla o mapeamento estÃ©tico e propriedades visuais grÃ¡fico; e ao menos uma camada que irÃ¡ descrever como cada observaÃ§Ã£o serÃ¡ renderizada. Camadas sÃ£o usualmente criadas utilizando uma funÃ§Ã£o geom_(). referÃªncia principal ao pacote Ã© o livro Ggplot2 : elegant graphics data analysis (Wickham 2009).","code":""},{"path":"graph.html","id":"meu-primeiro-grÃ¡fico-em-ggplot2","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.2 Meu primeiro grÃ¡fico em ggplot2","text":"seguir, vamos discutir os aspcetos bÃ¡sicos para construÃ§Ã£o de grÃ¡ficos utilizando o pacote ggplot2. funÃ§Ã£o plot_grid() pacote cowplot23 foi utilizado aqui para organizar os grÃ¡ficos em forma de painÃ©. O pacote qqplotr24 tambÃ©m Ã© utilizado como uma extensÃ£o pacote ggplot225 para confecÃ§Ã£o de grÃ¡ficos tipo Q-Q plots. Os dados contidos na aba gg arquivo data_R.xlsx serÃ£o utilizados. Estes dados podem ser carregados pelo seguinte comando.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\ndados_gg <- import(url, sheet = \"gg\")\nstr(dados_gg)\n# 'data.frame': 120 obs. of  6 variables:\n#  $ AMB  : chr  \"E1\" \"E1\" \"E1\" \"E1\" ...\n#  $ GEN  : chr  \"G1\" \"G1\" \"G1\" \"G2\" ...\n#  $ BLOCO: num  1 2 3 1 2 3 1 2 3 1 ...\n#  $ RG   : num  2167 2503 2427 3208 2933 ...\n#  $ PH   : num  44.9 46.9 47.8 45.2 45.3 ...\n#  $ MMG  : num  31.3 32.9 32.3 29 30.5 ..."},{"path":"graph.html","id":"as-camadas-de-um-grÃ¡fico-ggplot2","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.3 As camadas de um grÃ¡fico ggplot2","text":"ggplot2, os grÃ¡ficos sÃ£o construÃ­dos camada por camada (ou, layers, em inglÃªs). Neste exemplo, vamos confecionar um grÃ¡fico onde o eixo x serÃ¡ representado pela variÃ¡vel RG e o eixo y pela variÃ¡vel PH.Este comando criou um grÃ¡fico e armazenou objeto p1, que serÃ¡ plotado posteriormente. Observe que o primeiro argumento da funÃ§Ã£o Ã© o data frame onde nossos dados foram armazenados. funÃ§Ã£o aes()  descreve como variÃ¡veis sÃ£o mapeadas (neste caso RG eixo x e PH eixo y). funÃ§Ã£o geom_point() definiu que forma geomÃ©trica ser utilizada Ã© baseada em pontos, gerando, assim, um grÃ¡fico de dispersÃ£o. Isto Ã© tudo que precisa ser feito para confecÃ§Ã£o de um grÃ¡fico simples.","code":"\np1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n      geom_point()"},{"path":"graph.html","id":"aesthetics-estÃ©tica","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.4 Aesthetics (estÃ©tica)","text":"â€œO maior valor de uma imagem Ã© quando ela nos obriga perceber\r\no que nunca esperamos ver.â€ â€” John TukeyAlterar estÃ©tica dos grÃ¡ficos ggplot2 Ã© uma tarefa relativamente simples. grÃ¡fico anterior, os valores PH e RG foram plotados sem nenhum tipo de mapeamento estÃ©tico. Digamos que marcadores com diferentes cores para cada ambiente poderia nos ajudar compreender melhor o padrÃ£o presente em nossos dados. Vamos confecionar este grÃ¡fico.Ao incluirmos colour = AMB dentro da funÃ§Ã£o aes, dizemos ao ggplot que os pontos devem ser mapeados esteticamente (neste caso utilziando cores) para cada nÃ­vel fator AMB presente em nossos dados. Digamos que em vez de utilizar diferentes cores, os ambientes deveriam ser representados por diferentes tipos de marcadores (quadrados, triÃ¢ngulo, etc.) Neste caso, o argumento colour = AMB deveria ser substituÃ­por shape = AMB.\r\nFigure 8.1: GrÃ¡fico de dispersÃ£o padrÃ£o (p1) e com pontos mapeados por cores (p2) e marcadores (p3) para cada nÃ­vel fator â€˜AMBâ€™.\r\nExercÃ­cio 4Constua um grÃ¡fico semelhante ao anterior, onde o tamanho dos pontos deve ser baseado em uma terceira variÃ¡vel nosso conjunto de dados, neste exemplo, MMG.Resposta","code":"\np2 <- ggplot(dados_gg, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()\np3 <- ggplot(dados_gg, aes(x = RG, y = PH, shape = AMB)) +\n      geom_point()\nplot_grid(p1, p2, p3,\n          ncol = 3,\n          labels = c(\"p1\", \"p2\", \"p3\"),\n          rel_widths = c(1, 1.2, 1.2))"},{"path":"graph.html","id":"facet-facetas","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.5 Facet (facetas)","text":"Mapeando os diferentes nÃ­veis de AMB para diferentes cores, incluÃ­mos em um Ãºnico grÃ¡fico os dados de todos os ambientes. Mas, e se nosso objetivo fosse realizar um grÃ¡fico para cada ambiente? O ggplot2 tem uma poderosa ferramenta para isto: funÃ§Ãµes facet_. Ao utilziar estas funÃ§Ãµes, o conjunto de dados Ã© subdividido e um grÃ¡fico Ã© construÃ­para cada um destes subconjuntos. Vamos ver como elas podem nos ajudar em nossso problema. Neste exemplo, um grÃ¡fico completamente diferente anterior Ã© gerado com apenas uma simples modificaÃ§Ã£o: excluÃ­mos mapeamento estÃ©tico o argumento colour = AMB e incluÃ­mos uma nova funÃ§Ã£o, facet_wrap(~AMB). Neste caso, informamos que um grÃ¡fico deveria ser realizado para cada ambiente. Simples, nÃ£o? exemplo anterior, utilizamos funÃ§Ã£o facet_wrap() para confeccionar um grÃ¡fico foi criado para cada nÃ­vel fator AMB.Substitua funÃ§Ã£o facet_wrap(~ AMB) por facet_grid(~ AMB) e compare os dois grÃ¡ficos.","code":"\nfac1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n        geom_point()+\n        facet_wrap(~AMB)"},{"path":"graph.html","id":"theme-temas","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.6 Theme (temas)","text":"Cada grÃ¡fico criado com funÃ§Ã£o ggplot() tem um tema padrÃ£o. Tema, aqui, Ã© toda propriedade relacionada ao aspecto visual grÃ¡fico, que nÃ£o foi definida na funÃ§Ã£o aes() e que pode ser modificada utilizando funÃ§Ã£o theme() (veja ?theme). O ggplot2 jÃ¡ conta com alguns temas personalizados para facilitar nosso trabalho. Considerando o exemplo anterior, vamos utilziar funÃ§Ã£o theme_bw() (preto e branco) e funÃ§Ã£o theme() para modificar propriedades visuais grÃ¡fico.\r\nFigure 8.2: GrÃ¡fico de dispersÃ£o considerando confecÃ§Ã£o de um grÃ¡fico para cada nÃ­vel de um fator(f1) e modificaÃ§Ãµes na propriedades tema de um grÃ¡fico ggplot2 (f2)\r\nOs argumentos inseridos dentro das funÃ§Ã£o theme() modificaram aparÃªncia nosso grÃ¡fico. InÃºmeros outros argumentos sÃ£o disponÃ­veis, fazendo com que os grÃ¡ficos originados sejam completamente personalizÃ¡veis. Digamos que precisamos confecionar diversos grÃ¡ficos e gostarÃ­amos de manter o mesmo tema grÃ¡fico acima. Seria exaustivo e desinteressante informar cada vez estes argumentos para cada grÃ¡fico, nÃ£o? Felizmente, outra poderosa ferramenta proporcionada pelo ggplot2 Ã© possibilidade de confecionarmos nossos prÃ³prios temas. Para isto, vamos executar o seguinte comando para criar um tema personalizado (my_theme()). Este tema pode entÃ£o ser aplicado como uma camada adicional cada grÃ¡fico que confecionarmos. Para evitar necessidade da inclusÃ£o deste tema em cada grÃ¡fico gerado, iremos definir este tema como padrÃ£o utilizando funÃ§Ã£o theme_set() .ExercÃ­cio 5Constua um grÃ¡fico semelhante ao observado acima, onde diferentes cores devem ser atribuÃ­das para cada genÃ³tipo. Em adiÃ§Ã£o, aplique o tema personalizado que acabamos de criar ao grÃ¡fico.Resposta","code":"\nfac2 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n        geom_point() +\n        facet_wrap(~AMB) +\n        theme_bw() +\n        theme(panel.grid = element_blank(), # remove as linhas do corpo do grÃ¡fico\n             # sem bordas entre os painÃ©is\n              panel.spacing = unit(0, \"cm\"),\n             # modifica o texto dos eixos\n              axis.text = element_text(size = 12, colour = \"black\"),\n             # cor dos marcadores\n              axis.ticks = element_line(colour = \"black\"),\n             # tamanho dos marcadores\n              axis.ticks.length = unit(.2, \"cm\"), \n             #cor da borda\n              panel.border = element_rect(colour = \"black\", fill = NA, size = 0.5))+\n       # tÃ­tulo dos eixos\n       labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\") \n\nplot_grid(fac1, fac2, labels = c(\"f1\", \"f2\"))\nmy_theme <- function () {\n  theme_bw() %+replace% # permite que os valores informados possam ser sobescritos\n    theme(axis.ticks.length = unit(.2, \"cm\"),\n          axis.text = element_text(size = 12, colour = \"black\"),\n          axis.title = element_text(size = 12, colour = \"black\"),\n          axis.ticks = element_line(colour = \"black\"),\n          panel.border = element_rect(colour = \"black\", fill = NA, size = 0.5),\n          panel.grid =  element_blank())\n}\ntheme_set(my_theme())"},{"path":"graph.html","id":"geoms-geometria","chapter":"CapÃ­tulo 8 GrÃ¡ficos com o pacote ggplot2","heading":"8.7 Geoms (geometria)","text":"funÃ§Ãµes geom_ definem qual forma geomÃ©trica serÃ¡ utilizada para visualizaÃ§Ã£o dos dados grÃ¡fico. AtÃ© agora, utilizamos funÃ§Ã£o geom_point()  para construir grÃ¡ficos de dispersÃ£o. Basicamente, qualquer outro tipo de grÃ¡fico pode ser criado dependendo da funÃ§Ã£o geom_ utilizada. Dentre diversas disponÃ­veis pacote ggplot2 funÃ§Ãµes geom_ mais utilizadas sÃ£o:geom_abline(): para retas definidas por um intercepto e uma inclinaÃ§Ã£o;geom_hline(): para retas horizontais definidas por um intercept y;geom_vline(): para retas verticais definidas por um intercept x;geom_boxplot(): para boxplots;geom_histogram(): para histogramas de frequÃªncia;geom_smooth(): ajusta uma funÃ§Ã£o para o conjunto de dados e mostra uma banda de confianÃ§;geom_density(): para densidades;geom_area(): para Ã¡reas;geom_bar(): para barras;geom_errorbar() para barras de erro;Deste ponto em diante, vamos confeccionar alguns exemplos utilziando algumas destas funÃ§Ãµes (ou combinaÃ§Ãµes destas funÃ§Ãµes) incluindo argumentos de mapeamento de estÃ©tica e temas vistos atÃ© agora. \r\nFigure 8.3: GrÃ¡fico de dispersÃ£o, combinando pontos e linhas de regressÃ£o.\r\nExercÃ­cio 6\r\ngrÃ¡fico s1, uma regressÃ£o linear foi ajustada quando incluÃ­mos funÃ§Ã£o geom_smooth(method = \"lm\", se = F).Como este grÃ¡fico pode nos ajudar compreender relaÃ§Ã£o entre variÃ¡veis RG e PH? Ao incluir o argumento colour = AMB, uma regressÃ£o para cada ambiente foi ajustada (s2).Como este grÃ¡fico pode nos ajudar compreender relaÃ§Ã£o entre variÃ¡veis RG e PH? Ao incluir o argumento colour = AMB, uma regressÃ£o para cada ambiente foi ajustada (s2).Modifique o grÃ¡fico s2 para que os ambientes ainda continuem sendo mapeados por cores, mas uma Ãºnica linha de regressÃ£o seja ajustada.Modifique o grÃ¡fico s2 para que os ambientes ainda continuem sendo mapeados por cores, mas uma Ãºnica linha de regressÃ£o seja ajustada.RespostaGrÃ¡ficos tipo boxplot\r\nFigure 8.4: GrÃ¡fico tipo boxplot combinando mapeamentos estÃ©ticos e inclusÃ£o de linhas.\r\nlinha orizontal tracejada representa mÃ©dia geral GY. Seis estatÃ­sticas sÃ£o mostradas neste boxplot. mediana (linha horizontal), mÃ©dia (ponto) caixas inferior e superior correspondem ao primeiro e terceiro quartil (percentis 25 e 75, respectivamente). linha vertical superior se estende da caixa atÃ© o maior valor, nÃ£o maior que \\(1,5 \\times {IQR}\\) (onde IQR Ã© amplitude interquartÃ­lica). linha vertical inferior se estende da caixa atÃ© o menor valor, de mÃ¡ximo, \\(1,5 \\times {IQR}\\). Dados alÃ©m das linhas horizontais podem ser considerados outliers.\\(~\\)GrÃ¡ficos tipo histograma\r\nFigure 8.5: GrÃ¡fico tipo histograma com estimativas de funÃ§Ã£o de probabilidade kernel e normal.\r\nhistograma (h1), linha preta representa estimativa de densidade Kernel (Silverman 1998). linha vermelha representa estimativa da funÃ§Ã£o de probabilidade normal. Para isto, escala eixo y foi mudada de contagem para densidade.\\(~\\)GrÃ¡ficos tipo barra\r\nFigure 8.6: GrÃ¡fico tipo barras, com mapeamento estÃ©tico e barras de erro.\r\nafirmaÃ§Ã£o de que um grÃ¡fico ggplot2 Ã© feito em camadas fica mais evidente aqui. grÃ¡fico p1, barras representam mÃ©dias geral dos hÃ­bridos em nosso conjunto de dados. segundo grÃ¡fico, um novo argumento visto (fill = AMB). Isto informa que barras devem ser coloridas para cada nÃ­vel fator AMB. funÃ§Ã£o stat_summary(),  tambÃ©m vista pela primeira vez aqui, foi utilizada segundo grÃ¡fico para substituir funÃ§Ã£o geom_bar(). Com isto, foi possÃ­vel incluir mÃ©dias (fun = mean e geom = \"bar), bem como barras de erro (fun.data = mean_se e geom = \"errorbar\"). GrÃ¡ficos de dispersÃ£o com linhas de valores preditos\r\nFigure 8.7: GrÃ¡fico de dispersÃ£o combinado com inclusÃ£o de curvas ajustadas.\r\nGrÃ¡ficos tipo quantil-quantil (Q-Q plots)Esta funÃ§Ã£o Ã© muito util para verificar normalidade dos resÃ­duos da ANOVA e regressÃµes lineares ou nÃ£o lineares. programaÃ§Ã£o abaixo foi utilizada artigo de LÃºcio, Santos, Olivoto (2017) para demonstrar intepretaÃ§Ã£o dos grÃ¡ficos. funÃ§Ãµes stat_qq_band(), stat_qq_line() e stat_qq_point() pacote qqplotr26 serÃ£o utilizadas. Este Ã© uma das inÃºmeras extensÃµes pacote ggplot2 que podem ser encontradas aqui.27.\r\nFigure 8.8: GrÃ¡fico quantil-quantil de conjuntos de dados com assimetria Ã  esquerda, direita e com distribuiÃ§Ã£o normal.\r\n   \r\nNestes exemplos vimoss alguns grÃ¡ficos simples que podem ser originados pelo ggplot2. potencialidades deste pacote, entanto vÃ£o muito alÃ©m. Uma galeria com diversos exemplos de grÃ¡ficos ggplot2 com cÃ³digos disponÃ­veis pode ser vista aqui.28Note que grÃ¡fico acima, funÃ§Ãµes pacote qqplotr foram carregadas utilizando qqplotr::. Neste caso, indicamos que funÃ§Ã£o desejada Ã© uma funÃ§Ã£o deste pacote. Isto Ã© Ãºtil, principalmente quando dois pacotes tem funÃ§Ãµes com o mesmo nome. Utilizando :: especificamos de qual pacote funÃ§Ã£o deve ser carregada.","code":"\n\ns1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+ # estima uma regressÃ£o linear\n      labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\")\n\ns2 <- ggplot(dados_gg, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+\n      labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\")\nplot_grid(s1, s2, labels = c(\"s1\", \"s2\"), rel_widths  = c(1, 1.2))\n\nmean_rg <- mean(dados_gg$RG) # calcula a mÃ©dia geral do RG\nbox1 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_boxplot()\n\nbox2 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_boxplot(width = 0.5, col = \"black\", fill = \"gray\")+ # boxplot\n        # mostra a mÃ©dia por um ponto\n        stat_summary(geom = \"point\", fun.y = mean) + \n        # adiciona uma linha na mÃ©dia geral\n        geom_hline(yintercept = mean_rg, linetype = \"dashed\")+ \n        labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\")\n\nplot_grid(box1, box2, labels = c(\"b1\", \"b2\"))\n\nh1 <- ggplot(dados_gg, aes(x = RG)) +\n      geom_histogram()\n\nh2 <- ggplot(dados_gg, aes(x = RG)) +\n      geom_histogram(binwidth = 200, colour = \"black\", \n                     aes(y = ..density.., fill = ..count..)) +\n      geom_density() +\n      stat_function(fun = dnorm,\n                    color = \"red\",\n                    size = 1,\n                    args = list(mean = mean(dados_gg$RG),\n                                sd = sd(dados_gg$RG))) +\n      labs(x = \"rendimento de grÃ£os\", y = \"Densidade\")\n\nplot_grid(h1, h2, rel_widths = c(1, 1.4), labels = c(\"h1\", \"h2\"))\n\nbar1 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_bar(stat = \"summary\",\n                fun = mean,\n                position = \"dodge\")\n\nbar2 <- ggplot(dados_gg, aes(x = GEN, y = RG, fill = AMB)) +\n        stat_summary(fun = mean,\n                     geom = \"bar\",\n                     col = \"black\",\n                     width = 0.8,\n                     position = position_dodge()) + \n        stat_summary(fun.data = mean_se,\n                     geom = \"errorbar\",\n                     width = 0.2,\n                     position = position_dodge(0.8))\n \nplot_grid(bar1, bar2, rel_widths = c(0.8, 1.2), labels = c(\"bar1\", \"bar2\"))\n#### PolinÃ´mio de segundo grau\ndado_reg = tibble(dose = c(15,20,25,30,35,40),\n                  prod = c(65,70,73,75,69,62))\nl1 <- ggplot(dado_reg, aes(dose, prod))+\n      geom_point()+\n      stat_smooth(method = \"lm\",\n                  formula = \"y ~ poly(x, 1)\",\n                  se = FALSE)\nl2 <- ggplot(dado_reg, aes(dose, prod))+\n      geom_point()+\n      stat_smooth(method = \"lm\",\n                  formula = \"y ~ poly(x, 2)\",\n                  linetype = \"dashed\",\n                  col = \"black\",\n                  level = 0.95)\n\nplot_grid(l1, l2, labels = c(\"l1\", \"l2\"))\n# simulando dados com diferentes assimetrias\nassimetria <- tibble(esquerda = rbeta(5000,1,7),\n                     direita = rbeta(5000,7,1),\n                     normal = rnorm(5000,5,3))\n\nassimetria_graf <- pivot_longer(assimetria, everything()) # organiza os dados para usar facete_wrap\nggplot(assimetria_graf, aes(sample = value))+\n       facet_wrap(~name, scales = \"free\")+\n       qqplotr::stat_qq_band(fill = \"gray\")+\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\nlabs(x = \"Theoretical quantiles\", y = \"Sample quantiles\")"},{"path":"exporta.html","id":"exporta","chapter":"CapÃ­tulo 9 Exportando dados","heading":"CapÃ­tulo 9 Exportando dados","text":"SerÃ¡ demonstrado nessa seÃ§Ã£o como exportar dados e tabelas gerados dentro software R. Quanto saÃ­da de resultados, serÃ¡ dado enfase saÃ­das com extensÃµes .csv, .txt e .xlsx. Quanto aos grÃ¡ficos, serÃ¡ mostrado como salvar figuras em alta resoluÃ§Ã£o.","code":""},{"path":"exporta.html","id":"exportando-com-diferentes-extensÃµes","chapter":"CapÃ­tulo 9 Exportando dados","heading":"9.1 Exportando com diferentes extensÃµes","text":"funÃ§Ã£o export() d pacote rio (https://cran.r-project.org/web/packages/rio/index.html) pode ser utilizada para exportar objetos R em arquivos dos mais diversos formatos.Em arquivos .csv, os valores sÃ£o separados por vÃ­rgula. exemplo abaixo, Ã© mostrado como o objeto quantitativo pode ser salvo em um arquivo .csv com nome quanti_exemplo. Para salvar saÃ­das em extensÃ£o .txt ou .xlsx, basta substituir extensÃ£o arquivo.formato .xlsx Ã© possÃ­vel informar em qual planilha o objeto serÃ¡ salvo. Neste caso, planilhas existentes nÃ£o serÃ£o modificadas.TambÃ©m Ã© possÃ­vel salvar diferentes objetos em diferentes planilhas mesmo arquivo. Vamos considerar que cada nÃ­vel fator TIPO objeto quanti deve ser salvo em uma planilha diferente.Neste caso, um arquivo chamado quanti_exemplo_plan.xlsx contendo planilhas linear, quadratico e cubico foi criado diretÃ³rio padrÃ£o.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nquanti <- import(url, sheet = \"QUANTI\")\nhead(quanti)\n# exportar para o diretÃ³ri padrÃ£o (csv)\nexport(quanti, file = \"quanti_exemplo.csv\")\n# exportar para o diretÃ³ri padrÃ£o (txt)\nexport(quanti, file = \"quanti_exemplo.txt\")\n# exportar para o diretÃ³ri padrÃ£o (xslx)\nexport(quanti, file = \"quanti_exemplo.xlsx\")\nexport(quanti, file = \"quanti_exemplo.xlsx\", which = \"quanti2\")\nlinear <- subset(quanti, TIPO == \"LINEAR\")\nquadratico <- subset(quanti, TIPO == \"QUADRÃTICA\")\ncubico <- subset(quanti, TIPO == \"CÃšBICA\")\nexport(list(linear = linear,\n            quadratico = quadratico, \n            cubico = cubico),\n       file = \"quanti_exemplo_plan.xlsx\")"},{"path":"exporta.html","id":"exportanto-grÃ¡ficos","chapter":"CapÃ­tulo 9 Exportando dados","heading":"9.2 Exportanto grÃ¡ficos","text":"Os grÃ¡ficos podem ser exportados clicando em Export  output dos grÃ¡ficos. VocÃª pode escolher entre salvar como imagem ou como PDF. Os formatos de imagem disponÃ­veis sÃ£o: .PNG, .TIFF, .JPEG, .BMP , .SVG  e .ESP . outra opÃ§Ã£o Ã© salvar em um arquivo PDF . principal diferenÃ§entre estes formatos Ã© o mÃ©todo de renderizaÃ§Ã£o utilizado na formaÃ§Ã£o grÃ¡fico. Existem basicamente dois mÃ©todos de renderizaÃ§Ã£o de imagens: raster images e vector-based images. Imagens rasterizadas usam muitos pixels coloridos ou blocos de construÃ§Ã£o individuais para formar uma imagem completa. JPEGs, GIFs e PNGs e TIFFs sÃ£o tipos de imagem raster mais comuns. Como imagens raster sÃ£o criadas usando um nÃºmero fixo de pixels coloridos, elas nÃ£o podem ser redimensionadas drasticamente sem comprometer sua resoluÃ§Ã£o. Quando esticados para caber em um espaÃ§o que eles nÃ£o foram projetados para preencher, seus pixels ficam visivelmente granulados e imagem Ã© distorcida. Ã‰ importante que vocÃª salve os arquivos raster precisamente nas dimensÃµes necessÃ¡rias e com devida resoluÃ§Ã£o para uma boa apresentaÃ§Ã£o. Para salvar estas imagens Ã© necessÃ¡rio informar Density Pixels per Inch (DPI) , ou seja, quantos pontos por polegada quadrada deverÃ¡ conter imagem. Quanto maior este valor, maior serÃ¡ qualidade da imagem e tambÃ©m maior serÃ¡ seu tamanho (em Mb).Imagens vetoriais (vector-based graphics), por outro lado, permitem mais flexibilidade. ConstruÃ­dos usando fÃ³rmulas matemÃ¡ticas em vez de blocos coloridos individuais, os tipos de arquivos vetoriais, como .PDF e .EPS *, sÃ£o excelentes para criar grÃ¡ficos de alta resoluÃ§Ã£o sem que seu redimensionamento prejudique qualidade grÃ¡fico. maioria das revistas cintÃ­ficas aceitam todos estes tipos de formatos. Na dÃºvida, escolha sempre o formato .PDF (ou .EPS)!Salvando figuras como imagensSalvando figuras em PDFUma alternativa prÃ¡tica para salvar em pdf imagens Ã© atravÃ©s da funÃ§Ã£o pdf(). Os argumentos width e height correspondem dimensÃ£o da figura (em polegadas) e pointsize corresponde ao tamanho da fonte. funÃ§Ã£o ggsave()  tambÃ©m pode ser utilizada para salvar os grÃ¡ficos para o diretÃ³rio de trabalho. Por padrÃ£o, esta funÃ§Ã£o salva o Ãºltimo grÃ¡fico mostrado. Para salvar-mos o grÃ¡fico p1, gerado anteriormente, basta executar o seguinte comando.Exportando figuras como imagens em alta qualidadeCom funÃ§Ãµes tiff() , png() , jpeg()  e bmp()  Ã© possÃ­vel salvar grÃ¡ficos como imagem em alta resoluÃ§Ã£o. Como na funÃ§Ã£o pdf() , width e height correspondem dimensÃ£o. PorÃ©m, nestas funÃ§Ãµes Ã© possÃ­vel escolher unidade atravÃ©s argumento units e resoluÃ§Ã£o atravÃ©s argumento res.","code":"\npdf(\"Figura1.pdf\", width = 5, height = 4)\n\np1 <- ggplot(dados_oat, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+\n      my_theme()+\n      labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\")\np1\n\ndev.off()\nggsave(\"Figure_ggsave.pdf\", p1)\ntiff(filename = \"Figura3.tiff\", width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\npng(filename = \"Figura3.png\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\njpeg(filename = \"Figura3.jpeg\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\nbmp(filename = \"Figura3.bmp\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()"},{"path":"analdata.html","id":"analdata","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","text":"â€œMuito melhor uma resposta aproximada Ã  pergunta certa, que muitas vezes Ã© vaga, que uma resposta exata Ã  pergunta errada, que sempre pode ser feita com precisÃ£o.â€ â€” John TukeyNesta seÃ§Ã£o serÃ¡ abordado aspectos relacionados anÃ¡lise de experimentos agrÃ­colas, com Ãªnfase na utilizaÃ§Ã£o de testes paramÃ©tricos. Esta seÃ§Ã£o serÃ¡ dividida em trÃªs partes principais:Parte 1: estatistica bÃ¡sica: Medidas de tendÃªncia central e de variabilidade. Intervalos de confianÃ§para mÃ©dia. Testes de hipÃ³teses para verificar igualdade entre mÃ©dias de uma ou duas amostras.Parte 1: estatistica bÃ¡sica: Medidas de tendÃªncia central e de variabilidade. Intervalos de confianÃ§para mÃ©dia. Testes de hipÃ³teses para verificar igualdade entre mÃ©dias de uma ou duas amostras.Parte 2: delineamentos bÃ¡sicos: delineamentos experimentais inteiramente casualisado (DIC) e blocos ao acaso (DBC). Pressupostos  dos modelos estatisticos. Testes complementares (mÃ©dia e regressÃ£o).Parte 2: delineamentos bÃ¡sicos: delineamentos experimentais inteiramente casualisado (DIC) e blocos ao acaso (DBC). Pressupostos  dos modelos estatisticos. Testes complementares (mÃ©dia e regressÃ£o).Parte 3: anÃ¡lise de covariÃ¢ncia: AnÃ¡lise de covariÃ¢ncia como uma ferramenta estatÃ­stica para reduÃ§Ã£o erro experimental.Parte 3: anÃ¡lise de covariÃ¢ncia: AnÃ¡lise de covariÃ¢ncia como uma ferramenta estatÃ­stica para reduÃ§Ã£o erro experimental.Parte 4: modelos lineares generalizados: Modelos Lineares Generalizados aplicados anÃ¡lise de dados nÃ£o gaussianos.Parte 4: modelos lineares generalizados: Modelos Lineares Generalizados aplicados anÃ¡lise de dados nÃ£o gaussianos.Parte 5: experimentos fatoriais: experimentos fatorias e experimentos com parcelas subdivididas.Parte 5: experimentos fatoriais: experimentos fatorias e experimentos com parcelas subdivididas.","code":""},{"path":"analdata.html","id":"ebasic","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1 Estatistica bÃ¡sica","text":"","code":""},{"path":"analdata.html","id":"medidas-de-tendÃªncia-central","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.1 Medidas de tendÃªncia central","text":"Nesta seÃ§Ã£o mostraremos como calular medidas de tendÃªncia central e medidas de variabilidade. medidas de tendencia central sÃ£o valores que representam um conjunto de dados. Entre mais comuns podemos citar mÃ©dia, mediana e moda.O R calcula mÃ©dia  e mediana  atravÃ©s das funÃ§Ãµes mean()  e median() , porÃ©m nÃ£o calcula moda. blog RidÃ­culas, mantido pelo LEG da UFPR, dois mÃ©todos sÃ£o dicutidos. Incentivamos leitura material.","code":"\nset.seed(1)\nAmostra1 <- rnorm(100, 12, 3) # Gera uma amostra com distribuiÃ§Ã£o normal\nAmostra2 <- rpois(100, 12) # Gera uma amostra com distribuiÃ§Ã£o Poisson\nmean(Amostra1) # mÃ©dia\n# [1] 12.32666\nmedian(Amostra1) # mediana\n# [1] 12.34173"},{"path":"analdata.html","id":"medidas-de-variabilidade","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.2 Medidas de variabilidade","text":"seguintes medidas de variabilidade podem ser computadas, com suas respectivas funÃ§Ãµes:Desvio padrÃ£o sd()VariÃ¢ncia var()Amplitude total range()Amplitude interquartÃ­lica IQR()O desvio padrÃ£o e variÃ¢ncia podem ser obtidas com funÃ§Ãµes sd()  e var(), respectivamente. NÃ£o existe R base uma funÃ§Ã£o para computar o coeficiente de variaÃ§Ã£o, entÃ£o vamos criÃ¡-la utilizando abordagem function():  distribuiÃ§Ã£o dos dados pode ser determinada utilizando histograma de frequÃªncias, QQ-Plos e Box-Plot (conforme visto anteriormente). EstatÃ­sticas como amplitude, erro padrÃ£o da mÃ©dia, intervalo de confianÃ§, entre outros, podem ser obtidas com funÃ§Ã£o desc_stat()  pacote metan29. Esta funÃ§Ã£o permite computar estatisticas para uma ou mais variÃ¡veis de um data frame ou um vetor de dados numÃ©ricos. Para um exemplo numÃ©rico, consulte seÃ§Ã£o 7.3.1.","code":"\nsd(Amostra1)\n# [1] 2.694598\nvar(Amostra1)\n# [1] 7.260859\nrange(Amostra1)\n# [1]  5.35590 19.20485\nIQR(Amostra1)\n# [1] 3.557364\nCV <- function(dados){\n  if(!class(dados) == \"numeric\"){\n    stop(\"Os dados precisam ser numÃ©ricos\")\n    } #Indica que os dados devem ser numÃ©ricos\n  media <- mean(dados)\n  sd <- sd(dados)\n  CV <- (sd/media) * 100\n  return(CV) # Valor que serÃ¡ retornado pela funÃ§Ã£o\n}\n\nCV(Amostra1)\n# [1] 21.85992"},{"path":"analdata.html","id":"resumindo-dados-com-o-pacote-dplyr","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.3 Resumindo dados com o pacote dplyr","text":"Diversos verbos pacote dplyr podem ser utilizados para resumir conjuntos de dados. Iniciaremos\r\ncom funÃ§Ã£o count() para contar valores que se repetem em uma determinada variÃ¡vel. Por exemplo, Ã© possÃ­vel identificar qual Ã© o valor de APLA que mais se repete utilizandoPara identificar quais os valores distintos de APLA foram observados funÃ§Ã£o distinct() Ã© usada.Utilizando funÃ§Ã£o summarise() Ã© possÃ­vel criar uma ou mais variÃ¡veis escalares resumindo variÃ¡veis de um tibble existente. Como resultado, uma linha Ã© retornada. O seguinte cÃ³digo calcula mÃ©dia global da MGRA e retorna o n utilizado na estimativa.Muitas vezes Ã© necessÃ¡rio computar uma determinada funÃ§Ã£o (como mÃ©dia) para cada nÃ­vel de uma variÃ¡vel categÃ³rica. Felizmente, o pacote dplyr possibilita que isto seja realizado facilmente. Continuamos mesmo exemplo anterior. Neste caso, entanto, o objetivo Ã© calcular mÃ©dia da MGRA para cada hÃ­brido. Utilizando funÃ§Ã£o group_by()  antes da funÃ§Ã£o summarise()  uma linha de resultado para cada nÃ­vel fator hÃ­brido Ã© retornado.AtÃ© aqui vimos como mÃ©dia (global ou para cada hÃ­brido) da MGRA pode ser calculada. Quase sempre, entanto, quando calculamos mÃ©dia (ou qualquer outra medida) em um conjunto de dados, queremos fazÃª-la para todas variÃ¡veis numÃ©ricas. Implementar isto com dplyr Ã© relativamente fÃ¡cil. Existem basicamente trÃªs opÃ§Ãµes para isto, utilizando variantes summarise_all(), summarise_if(), ou summarise_at(). Todos os verbos principais pacote dplyr apresentam estas variantes, o que torna fÃ¡cil aplicar mesma funÃ§Ã£o para mÃºltiplas variÃ¡veis. Estas trÃªs variantes proporcionam: _all() aplicar funÃ§Ã£o todas variÃ¡veis;_at() aplicar funÃ§Ã£o variÃ¡veis selecionadas com vetores de caracteres ou utilizando vars()_if() aplicar funÃ§Ã£o variÃ¡veis selecionadas com uma funÃ§Ã£o, por exemplo .numeric().Veremos como estas variantes funcionam, calculando mÃ©dia para variÃ¡veis conjunto de dados.Note que utilizando funÃ§Ã£o summarise_all() mÃ©dia para todas variÃ¡veis numÃ©ricas foi calculada e um valor NA foi retornado para variÃ¡veis categÃ³ricas. Se o objetivo Ã© computar mÃ©dia somente para variÃ¡veis numÃ©ricas (o que Ã© o mais lÃ³gico), funÃ§Ã£o summarise_if() Ã© melhor escolha.FunÃ§Ãµes prÃ³prias podem ser aplicadas dentro da funÃ§Ã£o summarise para computar uma estatÃ­stica personalizada. Como exemplo, vamos criar uma funÃ§Ã£o chamada mse que retornarÃ¡ o valor da mÃ©dia e o erro padrÃ£o da mÃ©dia e aplicÃ¡-la para cada nÃ­vel fator AMB.Se desejamos computar mais de uma funÃ§Ã£o para variÃ¡veis especÃ­ficas, entÃ£o o prÃ³ximo cÃ³digo nos ajudarÃ¡. Note que para aplicar mais de uma funÃ§Ã£o Ã© necessÃ¡rio declarar o argumento .funs e criar um vetor com o nome das funÃ§Ãµes. Neste caso, os sufixos _m e _sd representam mÃ©dia e o desvio padrÃ£o, respectivamente.","code":"\nlibrary(metan)\ncount(maize, APLA, sort = TRUE)\n# # A tibble: 143 x 2\n#     APLA     n\n#    <dbl> <int>\n#  1  2.6     20\n#  2  2.8     20\n#  3  2.5     16\n#  4  2       14\n#  5  2.92    14\n#  6  2.1     13\n#  7  2.3     12\n#  8  2.7     12\n#  9  1.92    11\n# 10  2.04    11\n# # ... with 133 more rows\ndistinct(maize, APLA)\n# # A tibble: 143 x 1\n#     APLA\n#    <dbl>\n#  1  2.45\n#  2  2.5 \n#  3  2.69\n#  4  2.8 \n#  5  2.62\n#  6  2.12\n#  7  3.15\n#  8  2.97\n#  9  3.1 \n# 10  3.02\n# # ... with 133 more rows\nmaize %>% \n  summarise(MGRA_mean = mean(MGRA),\n            n = n())\n# # A tibble: 1 x 2\n#   MGRA_mean     n\n#       <dbl> <int>\n# 1      173.   780\nmaize %>% \n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA),\n            n = n())\n# # A tibble: 13 x 3\n#    HIB   MGRA_mean     n\n#    <chr>     <dbl> <int>\n#  1 H1         184.    60\n#  2 H10        164.    60\n#  3 H11        167.    60\n#  4 H12        157.    60\n#  5 H13        180.    60\n#  6 H2         187.    60\n#  7 H3         169.    60\n#  8 H4         184.    60\n#  9 H5         184.    60\n# 10 H6         188.    60\n# 11 H7         171.    60\n# 12 H8         160.    60\n# 13 H9         153.    60\nmaize %>% \n  summarise_all(mean)\n# # A tibble: 1 x 10\n#     AMB   HIB   REP  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1    NA    NA    NA  2.48  1.34  15.2  49.5  173.  339.  512.\nmaize %>% \n  summarise_if(is.numeric, mean)\n# # A tibble: 1 x 7\n#    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1  2.48  1.34  15.2  49.5  173.  339.  512.\nmse <- function(x){\n  me = round(mean(x), 3)\n  se = round(sd(x)/sqrt(n()), 3)\n  return(paste(me, \"+-\", se))\n}\nmaize %>% \n  group_by(AMB) %>%\n  summarise(MGRA_mean_se = mse(MGRA))\n# # A tibble: 4 x 2\n#   AMB   MGRA_mean_se    \n#   <chr> <chr>           \n# 1 A1    199.437 +- 3.232\n# 2 A2    168.436 +- 3.381\n# 3 A3    146.811 +- 2.787\n# 4 A4    177.072 +- 3.118\nmaize %>%\n  group_by(AMB) %>%\n  summarise_at(vars(starts_with(\"M\"),\n                    ends_with(\"S\"),\n                    contains(\"GR\")),\n               .funs = c(m = mean, sd = sd))\n# # A tibble: 4 x 11\n#   AMB   MGRA_m MMG_m AIES_m DIES_m NGRA_m MGRA_sd MMG_sd AIES_sd DIES_sd NGRA_sd\n#   <chr>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n# 1 A1      199.  360.   1.58   51.6   558.    45.1   58.1   0.187    3.23    117.\n# 2 A2      168.  334.   1.31   48.7   505.    47.2   67.9   0.369    3.53    103.\n# 3 A3      147.  318.   1.08   47.9   468.    38.9   61.7   0.225    3.57    110.\n# 4 A4      177.  343.   1.41   49.9   516.    43.5   58.6   0.212    3.35    106."},{"path":"analdata.html","id":"dstat","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.4 EstatÃ­stica descritiva com pacote metan","text":"O pacote metan fornece uma estrutura simples e intuitiva para o cÃ¡lculo de estatÃ­sticas descritivas. Um conjunto de funÃ§Ãµes pode ser usado para calcular rapidamente estatÃ­sticas descritivas mais usadas.Para calcular os valores mÃ©dios para cada nÃ­vel de um fator, por exemplo para cada nÃ­vel fator HIB conjunto de dados maize, usamos funÃ§Ã£omeans_by().seguintes funÃ§Ãµes _by() estÃ£o disponÃ­veis para calcular principais estatÃ­sticas descritivas por nÃ­veis de um fator.cv_by () Para cÃ¡lculo coeficiente de variaÃ§Ã£o.max_by () Para calcular valores mÃ¡ximos.means_by () Para calcular meios aritmÃ©ticos.min_by () Para compilar valores mÃ­nimos.n_by () Para obter o comprimento.sd_by () Para calcular o desvio padrÃ£o amostral.sem_by () Para calcular o erro padrÃ£o da mÃ©dia.","code":"\nmeans_by(maize, HIB)\n# # A tibble: 13 x 8\n#    HIB    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     2.62  1.50  15.1  51.2  184.  365.  507.\n#  2 H10    2.31  1.26  15.1  48.4  164.  320.  504.\n#  3 H11    2.39  1.27  15.2  48.8  167.  333.  501.\n#  4 H12    2.44  1.28  14.3  48.6  157.  316.  502.\n#  5 H13    2.54  1.35  15.0  50.6  180.  340.  538.\n#  6 H2     2.60  1.38  15.3  50.9  187.  356.  527.\n#  7 H3     2.59  1.41  14.5  49.4  169.  346.  491.\n#  8 H4     2.58  1.43  15.7  49.2  184.  346.  535.\n#  9 H5     2.57  1.37  15.6  49.9  184.  341.  542.\n# 10 H6     2.56  1.41  15.8  51.5  188.  363.  516.\n# 11 H7     2.40  1.32  15.4  49.5  171.  345.  498.\n# 12 H8     2.33  1.21  15.0  48.4  160.  322.  496.\n# 13 H9     2.36  1.27  15.0  47.6  153.  311.  496."},{"path":"analdata.html","id":"funÃ§Ãµes-Ãºteis","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.4.1 FunÃ§Ãµes Ãºteis","text":"Outras funÃ§Ãµes Ãºteis tambÃ©m sÃ£o implementadas. Todos eles funcionam naturalmente com %>%, lidam com dados agrupados com group_by() e vÃ¡rias variÃ¡veis (todas variÃ¡veis numÃ©ricas de .data por padrÃ£o).av_dev () calcula o desvio mÃ©dio absoluto.ci_mean () calcula o intervalo de confianÃ§para mÃ©dia.cv () calcula o coeficiente de variaÃ§Ã£o.freq_table () Calcula fÃ¡bula de frequÃªncia.hm_mean (), gm_mean () calcula mÃ©dias harmÃ´nica e geomÃ©trica, respectivamente. mÃ©dia harmÃ´nica Ã© o recÃ­proco da mÃ©dia aritmÃ©tica dos recÃ­procos. mÃ©dia geomÃ©trica Ã© enÃ©sima raiz de n produtos.kurt () calcula curtose como usada SAS e SPSS.range_data () Calcula o intervalo dos valores.sd_amo (), sd_pop () Calcula amostra e desvio padrÃ£o populacional, respectivamente.sem () calcula o erro padrÃ£o da mÃ©dia.skew () calcula assimetria usada SAS e SPSS.sum_dev () calcula soma dos desvios absolutos.sum_sq_dev () calcula soma dos desvios ao quadrado.var_amo (), var_pop () calcula amostra e variaÃ§Ã£o populacional.valid_n () Retorna o comprimento vÃ¡lido (nÃ£o NA) de um dado.","code":""},{"path":"analdata.html","id":"a-funÃ§Ã£o-desc_stat","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.4.2 A funÃ§Ã£o desc_stat()","text":"Para calcular todas estatÃ­sticas de uma sÃ³ vez, podemos usar desc_stat(). Esta funÃ§Ã£o pode ser usada para calcular medidas de tendÃªncia central, posiÃ§Ã£o e dispersÃ£o. Por padrÃ£o (stats = \"main\"), sete estatÃ­sticas (coeficiente de variaÃ§Ã£o, mÃ¡ximo, mÃ©dia, mediana, mÃ­nimo, desvio padrÃ£o da amostra, erro padrÃ£o e intervalo de confianÃ§da mÃ©dia) sÃ£o calculadas. Outros valores permitidos sÃ£o \"\" para mostrar todas estatÃ­sticas, \"robust\" para mostrar estatÃ­sticas robustas, \"quantile\" para mostrar estatÃ­sticas quantÃ­licas ou escolher uma (ou mais) estatÃ­sticas usando um vetor separado por vÃ­rgula com os nomes das estatÃ­sticas, por exemplo, stats = c(\"mean, cv\"). TambÃ©m podemos usar hist = TRUE para criar um histograma para cada variÃ¡vel. Aqui, auxiliares selecionados tambÃ©m podem ser usados argumento ....Todas estatÃ­sticas para todas variÃ¡veis numÃ©ricasEstatÃ­sticas robustas usando select helpersFunÃ§Ãµes quantÃ­licas escolhendo nomes de variÃ¡veisCrie um histograma para cada variÃ¡vel","code":"\ndesc_stat(maize, stats = \"all\")\n# # A tibble: 7 x 29\n#   variable av.dev     ci    cv  gmean  hmean     iqr    kurt     mad    max\n#   <chr>     <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n# 1 AIES      0.268 0.0221 23.4    1.30   1.26   0.502 -0.729    0.363   2.39\n# 2 APLA      0.322 0.0264 15.1    2.45   2.42   0.64  -0.292    0.474   3.3 \n# 3 CESP      1.65  0.152  14.3   15.0   14.5    2.6    2.83     1.93   20.4 \n# 4 DIES      3.02  0.260   7.46  49.4   49.3    5.29  -0.285    3.82   59.7 \n# 5 MGRA     39.0   3.35   27.5  166.   157.    69.0   -0.56    51.0   291.  \n# 6 MMG      51.5   4.46   18.7  332.   325.    87.8   -0.0484  66.2   546.  \n# 7 NGRA     88.4   7.98   22.2  498.   482.   148.     0.321  110.    903   \n# # ... with 19 more variables: mean <dbl>, median <dbl>, min <dbl>, n <dbl>,\n# #   q2.5 <dbl>, q25 <dbl>, q75 <dbl>, q97.5 <dbl>, range <dbl>, sd.amo <dbl>,\n# #   sd.pop <dbl>, se <dbl>, skew <dbl>, sum <dbl>, sum.dev <dbl>,\n# #   sum.sq.dev <dbl>, valid.n <dbl>, var.amo <dbl>, var.pop <dbl>maize %>%\r\nÂ Â desc_stat(contains(\"N\"),\r\nÂ Â Â Â Â Â Â Â Â Â Â Â stats = \"robust\")\r\n# # A tibble: 1 x 4\r\n#   variable     n median   iqr\r\n#   <chr>    <dbl>  <dbl> <dbl>\r\n# 1 NGRA       780    517  148.maize %>%\r\nÂ Â desc_stat(APLA, AIES, CESP,\r\nÂ Â Â Â Â Â Â Â Â Â Â Â stats = \"quantile\")\r\n# # A tibble: 3 x 7\r\n#   variable     n   min   q25 median   q75   max\r\n#   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>\r\n# 1 AIES       780   0.5  1.09   1.38  1.59  2.39\r\n# 2 APLA       780   1    2.16   2.52  2.8   3.3 \r\n# 3 CESP       780   0.8 14     15.4  16.6  20.4maize %>%\r\nÂ Â desc_stat(APLA, AIES, CESP,\r\nÂ Â Â Â Â Â Â Â Â Â Â Â hist = TRUE)# # A tibble: 3 x 9\r\n#   variable    cv   max  mean median   min sd.amo     se     ci\r\n#   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>\r\n# 1 AIES      23.4  2.39  1.34   1.38   0.5  0.314 0.0113 0.0221\r\n# 2 APLA      15.1  3.3   2.48   2.52   1    0.375 0.0134 0.0264\r\n# 3 CESP      14.3 20.4  15.2   15.4    0.8  2.16  0.0774 0.152"},{"path":"analdata.html","id":"estatÃ­sticas-por-nÃ­veis-de-fatores","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.4.3 EstatÃ­sticas por nÃ­veis de fatores","text":"Para calcular estatÃ­sticas para cada nÃ­vel de um fator, use o argumento . AlÃ©m disso, Ã© possÃ­vel selecionar estatÃ­sticas serem computadas usando o argumento stats, que Ã© um Ãºnico nome estatÃ­stico, por exemplo,\"mean\"ou um vetor de nomes separados por vÃ­rgula com \" inÃ­cio e apenas o final vetor. Tenha em atenÃ§Ã£o que os nomes das estatÃ­sticas NÃƒO diferenciam maiÃºsculas de minÃºsculas, por exemplo, sÃ£o reconhecidos \"mean\", \"Mean\" ou \"MEAN\". VÃ­rgula ou espaÃ§os podem ser usados para separar os nomes das estatÃ­sticas.Todas opÃ§Ãµes abaixo funcionarÃ£o:\r\nstats = c (\"mean, se, cv, max, min\")\r\nstats = c (\"mean se cv max min\")\r\nstats = c (\"MEAN, Se, CV max MIN\")\r\nTodas opÃ§Ãµes abaixo funcionarÃ£o:stats = c (\"mean, se, cv, max, min\")stats = c (\"mean se cv max min\")stats = c (\"MEAN, Se, CV max MIN\")Para calcular estatÃ­sticas descritivas por mais de uma variÃ¡vel de agrupamento, precisamos passar dados agrupados para o argumento .data com funÃ§Ã£ogroup_by(). Vamos calcular mÃ©dia, o erro padrÃ£o da mÃ©dia e o tamanho da amostra para variÃ¡veis EP eEL para todas combinaÃ§Ãµes dos fatores AMB eHIB.Quando estatÃ­sticas sÃ£o calculadas para nÃ­veis de um fator, funÃ§Ã£o desc_wider() pode ser utilizada para converter uma estatÃ­stica calculada em um conjunto de dados em formato wide, ou seja, variÃ¡veis nas colunas, fatores nas linhas com o valor da estatÃ­stica escolhida preenchendo tabela.","code":"desc_stat (maize,\r\nÂ Â Â Â Â Â Â Â Â Â contains(\"C\"),\r\nÂ Â Â Â Â Â Â Â Â Â stats = (\"mean, se, cv, max, min\"),\r\nÂ Â Â Â Â Â Â Â Â Â by = AMB)\r\n# # A tibble: 4 x 7\r\n#   AMB   variable  mean    se    cv   max   min\r\n#   <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\r\n# 1 A1    CESP      15.6 0.171  15.3  20.3   0.8\r\n# 2 A2    CESP      15.2 0.148  13.6  19.2   8.7\r\n# 3 A3    CESP      14.7 0.145  13.8  19.9   7.5\r\n# 4 A4    CESP      15.1 0.148  13.7  20.4   8.2\nstats <- \nmaize %>% \n  group_by(AMB, HIB) %>% \n  desc_stat(APLA, AIES, CESP,\n            stats = c(\"mean, se, n\"))\nstats\n# # A tibble: 156 x 6\n#    AMB   HIB   variable  mean     se     n\n#    <chr> <chr> <chr>    <dbl>  <dbl> <dbl>\n#  1 A1    H1    AIES      1.68 0.0599    15\n#  2 A1    H1    APLA      2.72 0.0695    15\n#  3 A1    H1    CESP     15.4  0.451     15\n#  4 A1    H10   AIES      1.62 0.0506    15\n#  5 A1    H10   APLA      2.78 0.0716    15\n#  6 A1    H10   CESP     16.1  0.662     15\n#  7 A1    H11   AIES      1.58 0.0302    15\n#  8 A1    H11   APLA      2.75 0.0236    15\n#  9 A1    H11   CESP     16.6  0.332     15\n# 10 A1    H12   AIES      1.54 0.0456    15\n# # ... with 146 more rows\ndesc_wider(stats, se)\n# # A tibble: 52 x 5\n#    AMB   HIB     AIES   APLA  CESP\n#    <chr> <chr>  <dbl>  <dbl> <dbl>\n#  1 A1    H1    0.0599 0.0695 0.451\n#  2 A1    H10   0.0506 0.0716 0.662\n#  3 A1    H11   0.0302 0.0236 0.332\n#  4 A1    H12   0.0456 0.0582 0.498\n#  5 A1    H13   0.0562 0.05   0.295\n#  6 A1    H2    0.0327 0.0507 0.637\n#  7 A1    H3    0.0383 0.0323 0.543\n#  8 A1    H4    0.0448 0.043  0.437\n#  9 A1    H5    0.0315 0.0442 0.618\n# 10 A1    H6    0.0447 0.0704 0.347\n# # ... with 42 more rows"},{"path":"analdata.html","id":"testes-de-aderÃªncia","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.5 Testes de aderÃªncia","text":"Testes de aderÃªncia distribuiÃ§Ãµes teÃ³ricas tambÃ©m sÃ£o de grande utilizada para ciÃªncias agrÃ¡rias. O teste de Shapiro-Wilk, realizado pela funÃ§Ã£o shapiro.test() , Ã© amplamente utilizada para realizar o teste de normalidade dos dados. Para testar aderÃªncia outras distribuiÃ§Ãµes teÃ³ricas, o teste de Kolmolgorov-Smirnov (funÃ§Ã£o ks.test())  Ã© uma alternativa.","code":"\n# Teste de Shapiro-Wilk\nshapiro.test(Amostra1)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra1\n# W = 0.9956, p-value = 0.9876\nshapiro.test(Amostra2)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra2\n# W = 0.95815, p-value = 0.002976\n\n# Kolmogorovâ€“Smirnov\n# Amostra1 e Amostra2 provÃ©m da mesma distribuiÃ§Ã£o?\nks.test(Amostra1, Amostra2)\n# \n#   Two-sample Kolmogorov-Smirnov test\n# \n# data:  Amostra1 and Amostra2\n# D = 0.29, p-value = 0.0004453\n# alternative hypothesis: two-sided\n\n# Amostra1 ~ N(12, 3)?\nks.test(Amostra1, \"pnorm\", 12, 3)\n# \n#   One-sample Kolmogorov-Smirnov test\n# \n# data:  Amostra1\n# D = 0.094659, p-value = 0.3317\n# alternative hypothesis: two-sided"},{"path":"analdata.html","id":"intervalos-de-confianÃ§a","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.6 Intervalos de confianÃ§a","text":"estimaÃ§Ã£o por intervalo nÃ£o fornece idÃ©ia da margem de erro cometida ao estimar um determinado parÃ¢metro (Ferreira 2009). Por isso, para verificar se uma dada hipÃ³tese \\(H_0\\) (de igualdade) Ã© ou nÃ£o verdadeira, deve-se utilizar intervalos de confianÃ§ou testes de hipÃ³teses. construÃ§Ã£o destes intervalos, e particularidades dos testes de hipÃ³teses, serÃ£o discutidos seguir. Recomendamos como literatura o livro EstatÃ­stica BÃ¡sica30 escrito pelo Prof.Â Daniel Furtado Ferreira da UFV.\r\nPara verificar normalidade dos dados, funÃ§Ãµes shapiro.test() e ks.test() e os grÃ¡ficos QQ-Plot sÃ£o de grande utilidade.  SerÃ¡ demostrado como testar hiÃ³teses para uma e duas mÃ©dias pelo teste t de Student, o que exiege que os dados tenham distribuiÃ§Ã£o normal univariada (jÃ¡ discutido anteriormente) ou bivariada (dados emparelhados). Para testar normalidade bivariada, basta testar normalidade da diferenÃ§entre variÃ¡veis: partir de um intervalo  que tenha alta probabilidade de conter o valor paramÃ©trico, Ã© possÃ­vel diferenciar duas estimativas (Ferreira 2009). O intervalo de confianÃ§de uma mÃ©dia amostral de 95% Ã© dado por:\\[\r\nP\\left[ {\\bar X - {t_{\\alpha /2}}\\frac{S}{{\\sqrt n }} \\le \\mu  \\le \\bar X + {t_{\\alpha /2}}\\frac{S}{{\\sqrt n }}} \\right] = 1 - \\alpha \r\n\\]Na expressÃ£o acima, \\(\\bar X\\) Ã© mÃ©dia, \\(S\\) Ã© o desvio padrÃ£o e \\(-t_{\\alpha /2}\\) e \\(+t_{\\alpha /2}\\) sÃ£o os quantis inferior e superior, respectivamente, da distribuiÃ§Ã£o t de Student. O intervalo acima indica que o valor parÃ¢metro (\\(\\mu\\)) tem 95% de chance de estar contido intervalo. Ressalta-se que expressÃ£o acima estÃ¡ relacionada com precisÃ£o e nÃ£o com acurÃ¡cia da estimativa. Para calcular esse intervalo, podemos utilizar funÃ§Ã£o t.teste() .O intervalo de confianÃ§Ã©, por default, de 95%. PoÃ©m, pode-se modificar atravÃ©s argumento conf.level. Para gerar os grÃ¡ficos de intervalo de confianÃ§serÃ¡ utilizada funÃ§Ã£o ggplot() pacote ggplot2. \r\nFigure 10.1: GrÃ¡ficos de intervalo de confianÃ§(mais de uma variÃ¡vel)\r\n","code":"\nAmostra3 <- Amostra1 - Amostra2\nshapiro.test(Amostra3)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra3\n# W = 0.97923, p-value = 0.1158\nresult <- t.test(Amostra1)\nresult$conf.int # Intervalo de confianÃ§a\n# [1] 11.79200 12.86133\n# attr(,\"conf.level\")\n# [1] 0.95\nresult$estimate # mÃ©dia\n# mean of x \n#  12.32666\nresult <- t.test(Amostra1, conf.level = 0.99)\nresult1 <- t.test(Amostra1, conf.level = 0.90)\nresult$conf.int # Intervalo de confianÃ§a\n# [1] 11.61895 13.03437\n# attr(,\"conf.level\")\n# [1] 0.99\nresult1$conf.int # Intervalo de confianÃ§a\n# [1] 11.87925 12.77407\n# attr(,\"conf.level\")\n# [1] 0.9\nset.seed(100) #Ajusta a semente para reproduÃ§Ã£o dos nÃºmeros\nAmostra1 <- rnorm(100,10,10)\nAmostra2 <- rnorm(100,10,24)\nAmostra3 <- rnorm(100,25,15)\nAmostra4 <- rnorm(100,20,30)\n\ndados_IC <- tibble(\n    Factor = c(\"Amostra 1\",\"Amostra 2\", \"Amostra 3\", \"Amostra 4\"),\n    LL = c(t.test(Amostra1)$conf.int[1],\n           t.test(Amostra2)$conf.int[1],\n           t.test(Amostra3)$conf.int[1],\n           t.test(Amostra4)$conf.int[1]),\n    Mean = c(t.test(Amostra1)$estimate,\n             t.test(Amostra2)$estimate,\n             t.test(Amostra3)$estimate,\n             t.test(Amostra4)$estimate),\n    UL = c(t.test(Amostra1)$conf.int[2],\n           t.test(Amostra2)$conf.int[2],\n           t.test(Amostra3)$conf.int[2],\n           t.test(Amostra4)$conf.int[2]))\n\n# GrÃ¡fico com barras de erros\ncbPalette = c(\"red\", \"blue\", \"green\", \"pink\") # armazenando as cores\nerr1 = ggplot(data = dados_IC, aes(y = Mean, x = Factor, colour = Factor)) + \n              geom_point(size = 2.5)+ # adiciona um ponto ao grÃ¡fico\n              geom_errorbar(aes(ymax = UL, ymin = LL), width = 0.1)+\n              scale_color_manual(values = cbPalette)+ \n              coord_flip()+ # \"inverte\" o \"x\" e o \"y\" \n              expand_limits(y = c(0.4,0.6))+\n              theme(legend.position = \"none\")\n\n# GrÃ¡fico de barras com barras de erros \nerr2 = ggplot(data = dados_IC, aes(y = Mean, x = Factor)) + \n              geom_bar(aes(fill = Factor), stat = \"identity\", position = \"dodge\")+\n              geom_errorbar(aes( ymax = UL, ymin = LL), width = 0.2)+\n              expand_limits(y = c(0,30)) +\n              scale_fill_manual(values = cbPalette)+\n              theme(legend.position = \"none\")\n\nplot_grid(err1, err2)"},{"path":"analdata.html","id":"teste-de-hipÃ³teses-para-amostras-independentes","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.7 Teste de hipÃ³teses para amostras independentes","text":"Os testes de hipÃ³teses  aqui demonstrados tem como objetivo () verificar se determianda amostra difrere ou nÃ£o de zero (\\({H_0}:\\mu = 0\\)) e (ii) se duas amostras sÃ£o ou nÃ£o iguais (\\({H_0}:{\\mu _1} = {\\mu _2}\\)). Para testar hipÃ³teses pode-se utilizar funÃ§Ã£o t.teste(). Utilizaremos como amostras os dados da variÃ¡vel MGRA conjunto maize. primeira amostra corresponde contÃ©m os valores de A1 e segunda os valores de A2Alternativamente, o pacote ggstatplot31 pode ser utilizado para confecionar grÃ¡ficos que incluem teste de hipÃ³teses.O teste t pode ser utilizado para testar tanto hipÃ³teses tipo \\({H_A}:\\mu > 0\\) ou \\({H_A}:\\mu < 0\\). PorÃ©m, para isso, devemos utilizar o argumento alternative para indicar que o teste utilizado Ã© unilateral.Outro pressuposto para realizar o teste t Ã© homogeneidade das variÃ¢ncias. Quando variÃ¢ncias sÃ£o heterogÃªneas, o grau de liberdade utilizado Ã© calculado pela aproximaÃ§Ã£o de Welch-Satterthwaite: \\[\r\n\\nu  \\cong \\frac{{{{\\left( {\\frac{{S_1^2}}{{{n_1}}} + \\frac{{S_2^2}}{{{n_2}}}} \\right)}^2}}}{{\\frac{{{{\\left( {\\frac{{S_1^2}}{{{n_1}}}} \\right)}^2}}}{{{n_1} - 1}} + \\frac{{{{\\left( {\\frac{{S_2^2}}{{{n_2}}}} \\right)}^2}}}{{{n_2} - 1}}}}\r\n\\]","code":"\nAmostra1 <- maize %>% filter(AMB == \"A1\") %>% select(MGRA) %>% pull()\nAmostra2 <- maize %>% filter(AMB == \"A2\") %>% select(MGRA) %>% pull()\n\nt.test(Amostra1) # testa se a amostra difere de zero\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value < 2.2e-16\n# alternative hypothesis: true mean is not equal to 0\n# 95 percent confidence interval:\n#  193.0630 205.8111\n# sample estimates:\n# mean of x \n#   199.437\nt.test(Amostra1, Amostra2) # testa se as amostras difrem entre si\n# \n#   Welch Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 387.21, p-value = 1.144e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80515 40.19763\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356\nt.test(Amostra1, alternative = \"greater\") # unilateral a direita\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value < 2.2e-16\n# alternative hypothesis: true mean is greater than 0\n# 95 percent confidence interval:\n#  194.0956      Inf\n# sample estimates:\n# mean of x \n#   199.437\nt.test(Amostra1, alternative = \"less\") # unilateral a esquerda\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value = 1\n# alternative hypothesis: true mean is less than 0\n# 95 percent confidence interval:\n#      -Inf 204.7784\n# sample estimates:\n# mean of x \n#   199.437\nvar.test(Amostra1, Amostra2) # Teste F para variÃ¢ncias\n# \n#   F test to compare two variances\n# \n# data:  Amostra1 and Amostra2\n# F = 0.91355, num df = 194, denom df = 194, p-value = 0.5295\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.688888 1.211488\n# sample estimates:\n# ratio of variances \n#          0.9135534\nt.test(Amostra1, Amostra2, var.equal = FALSE) # Por default, usa Welch-Satterthwaite\n# \n#   Welch Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 387.21, p-value = 1.144e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80515 40.19763\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356\n\nAmostra3 <- rnorm(30, 10, 5)\nAmostra4 <- rnorm(30, 18, 5)\nvar.test(Amostra3, Amostra4)\n# \n#   F test to compare two variances\n# \n# data:  Amostra3 and Amostra4\n# F = 0.65152, num df = 29, denom df = 29, p-value = 0.2545\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.3101021 1.3688474\n# sample estimates:\n# ratio of variances \n#          0.6515231\nt.test(Amostra1, Amostra2, var.equal = TRUE) # Quando variÃ¢ncias sÃ£o iguais\n# \n#   Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 388, p-value = 1.142e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80520 40.19757\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356"},{"path":"analdata.html","id":"teste-de-hipÃ³teses-para-amostras-dependentes","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.1.8 Teste de hipÃ³teses para amostras dependentes","text":"formas de comparaÃ§Ã£o discutidas acima consideram amostras como sendo independentes entre si. Para dados emparelhados, deve-se utiliza o argumento paired =  TRUE.Observa-se que existe uma diferenÃ§valor da estatistica teste, nos graus de liberdade, valor tabelado e, consequentemente, p-valor. Para duas amostras independentes, o teste para diferenÃ§Ã© dado por\\[\r\n{t_c} = \\frac{{{{\\bar X}_1} - {{\\bar X}_2}}}{{S\\sqrt {\\frac{1}{{{n_1}}} + \\frac{1}{{{n_2}}}} }} \\sim {t_{\\left( {\\alpha ,\\nu } \\right)}}\r\n\\]Onde \\(\\alpha\\) Ã© probabilidade de erro, \\(\\nu\\) Ã© o grau de liberdade (nÂº total de obervaÃ§Ãµes-2), \\(S\\) Ã© mÃ©dia ponderada desvio padrÃ£o, \\({\\bar X}_1\\) e \\({\\bar X}_2\\) sÃ£o mÃ©dia das amostras 1 e 2, respectivamente, e \\(n_1\\) e \\(n_1\\) e \\(n_2\\) sÃ£o os tamanhos de amostra da amostra 1 e 2, respectivamente. resultado acima, obervamos que \\(\\nu = 58\\).caso de amostras pareadas (dependentes), estatÃ­stica teste Ã© dada por \\[\r\n{t_c} = \\frac{{\\bar d - {\\mu _0}}}{{\\frac{{{S_d}}}{{\\sqrt n }}}} \\sim {t_{\\left( {\\alpha ,\\nu } \\right)}} \r\n\\]Onde \\(\\alpha\\) Ã© probabilidade de erro, \\(\\nu\\) Ã© o grau de liberdade (nÂº de diferenÃ§-1), \\(\\bar d\\) Ã© mÃ©dia das diferenÃ§, \\(S_d\\) Ã© o desvio padrÃ£o das diferenÃ§e \\(n\\) Ã© o nÃºmero de diferenÃ§. resultado acima,obervamos que \\(\\nu = 29\\).","code":"\nAmostra1 <- rnorm(30,10,5)\nAmostra2 <- rnorm(30,15,5)\n\nvar.test(Amostra1, Amostra2) # Teste F para variÃ¢ncias\n# \n#   F test to compare two variances\n# \n# data:  Amostra1 and Amostra2\n# F = 0.74981, num df = 29, denom df = 29, p-value = 0.4429\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.3568811 1.5753388\n# sample estimates:\n# ratio of variances \n#          0.7498058\n\nt.test(Amostra1, Amostra2, var.equal = TRUE) # Independentes\n# \n#   Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = -3.2434, df = 58, p-value = 0.001961\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  -7.208743 -1.706572\n# sample estimates:\n# mean of x mean of y \n#  9.873579 14.331237\nt.test(Amostra1, Amostra2, var.equal = TRUE, paired = TRUE) # Emparelhadas\n# \n#   Paired t-test\n# \n# data:  Amostra1 and Amostra2\n# t = -3.6981, df = 29, p-value = 0.000902\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  -6.922951 -1.992364\n# sample estimates:\n# mean of the differences \n#               -4.457658"},{"path":"analdata.html","id":"dbasic","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2 Delineamentos bÃ¡sicos","text":"anÃ¡lises realizadas atÃ© agora tinham como objetivo verificar existÃªncia de diferenÃ§entre mÃ©dias de duas amostras. PorÃ©m, quando deseja-se estudar o efeito de â€œgrupos de fatoresâ€ sobre determinado fenÃ´meno, anÃ¡lise da variÃ¢ncia (ANOVA) Ã© indicada. ANOVA atribui diversos fatores partes da variabilidade dos dados (Casella 2008). Os delineamentos experimentais tambÃ©m sÃ£o parte importante da ANOVA. SerÃ¡ dado mais destaque aos mais comuns: o delineamento inteiramente casualisado (DIC)  e o blocos ao acaso (DBC) . O bloqueamento tem como objetivo remover parte da variabilidade. Como nÃ£o se deseja encotrar diferenÃ§entre os blocos, anÃ¡lises complementares nÃ£o sÃ£o realizadas para este fator.Nessa seÃ§Ã£o serÃ¡ demostrado como analisar dados experimentais utilizando estes dois delineamentos. Em um primeiro momento, serÃ£o demonstrados experimentos unifatorias e, posteriormente, experimentos bifatoriais com e sem parcelas subdivididas. Formas de como verificar se os pressupostos  modelo estatistico estÃ£o sendo cumpridos, e formas de contornar este problema caso estejam sendo violados, tambÃ©m serÃ£o demonstrados. Por fim, apÃ³s anÃ¡lise da variÃ¢ncia, serÃ£o mostrados os testes complementares utilizados para tratamentos quali e quantitativos.","code":""},{"path":"analdata.html","id":"princÃ­pios-bÃ¡sicos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.1 PrincÃ­pios bÃ¡sicos","text":"Os principios bÃ¡sicos da experimentaÃ§Ã£o sÃ£o casualisaÃ§Ã£o e repetiÃ§Ã£o. repetiÃ§Ã£o possibilita que o erro seja estimado, e casualisaÃ§Ã£o que eles sejam independentes.","code":""},{"path":"analdata.html","id":"pressupostos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.2 Pressupostos","text":"Independente delineamento, os pressupostos  modelo estatÃ­stico sÃ£o que os erros sÃ£o independentes, homocedÃ¡sticos e normais:\\[\r\n{\\boldsymbol{\\varepsilon }} \\sim {\\textrm N}\\left( {0,{\\boldsymbol{}}{\\sigma ^2}} \\right)\r\n\\]formas de realizar esse diagnÃ³stico Ã© atravÃ©s de testes estatÃ­sticos e grÃ¡ficos de diagnÃ³sticos. Uma ferramente para contornar o problema de violaÃ§Ã£o dos pressupostos Ã© transformaÃ§Ã£o dos dados. Existem vÃ¡rias formas de transformar os dados (cada uma adequada um caso especÃ­fico), mas serÃ¡ dado enfase Ã  transformaÃ§Ã£o Box-Cox.","code":""},{"path":"analdata.html","id":"estimaÃ§Ã£o","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.3 EstimaÃ§Ã£o","text":"estimativa dos parÃ¢metros Ã© realizado pelo mÃ©todo dos minimos quadrados. Devido matriz delineamento ser de posto incompleto (modelo superparametrizado), uma restriÃ§Ã£o deve ser imposta ao fator tratamentos para que estimativa efeito dos fatores seja Ãºnica (\\(\\sum {{t_i}} = 0\\)). ReparametrizaÃ§Ãµes ou combinaÃ§Ãµes lineares tambÃ©m podem ser utilizados para garantir unicidade dos parÃ¢metros (Rencher Schaalje 2008).","code":""},{"path":"analdata.html","id":"reparametrizaÃ§Ã£o-condiÃ§Ãµes-marginais-ou-combinaÃ§Ãµes-lineares","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.4 ReparametrizaÃ§Ã£o, condiÃ§Ãµes marginais ou combinaÃ§Ãµes lineares?","text":"funÃ§Ãµes lm()  e aov()  sÃ£o usualmente utilizadas para realizar anÃ¡lise da variÃ¢ncia. Ambas utilizam reparametrizaÃ§Ã£o o modelo para estimar os parÃ¢metros. Vamos ver isso exemplo abaixo utilizando o conjunto de dados QUALIAbaixo Ã© apresentado o calculo da ANOVA de mod1 atravÃ©s de uma aboragem matricial. Portanto verifica-se que reparametrizaÃ§Ã£o Ã© o mÃ©todo utilizado pelas funÃ§Ãµes aov() e lm(). Demonstrar como o procedimento de estimaÃ§Ã£o e cÃ¡lculo da ANOVA serviu apenas para verificar de maneira didÃ¡tica como funÃ§Ãµes R contornam o problema da singularidade da matriz delineamento. Conforme jÃ¡ relatado acima, funÃ§Ãµes aov() e lm() podem ser utilizadas para realizar ANOVA. PorÃ©m, nesta seÃ§Ã£o serÃ¡ dado enfase funÃ§Ãµes pacote ExpDes.pt,  cujas funÃ§Ãµes possibilitam analisar dados uni e bifatoriais; este Ãºltimo com e sem parcelas subdivididas.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\ndados <- import(url, sheet = \"QUALI\")\n\n## Usando a funÃ§Ã£o aov()\nmod1 <- aov(RG ~ HIBRIDO, data = dados)\ncoef(mod1)\n#  (Intercept) HIBRIDONP_10  HIBRIDONP_2  HIBRIDONP_3  HIBRIDONP_4  HIBRIDONP_5 \n#     10.27750     -3.99450     -1.44550     -0.79750     -0.79675     -1.52675 \n#  HIBRIDONP_6  HIBRIDONP_7  HIBRIDONP_8  HIBRIDONP_9 \n#     -2.94125     -3.23250     -3.03250     -3.28525\n\nmod2 <- lm(RG ~ HIBRIDO, data = dados)\ncoef(mod2)\n#  (Intercept) HIBRIDONP_10  HIBRIDONP_2  HIBRIDONP_3  HIBRIDONP_4  HIBRIDONP_5 \n#     10.27750     -3.99450     -1.44550     -0.79750     -0.79675     -1.52675 \n#  HIBRIDONP_6  HIBRIDONP_7  HIBRIDONP_8  HIBRIDONP_9 \n#     -2.94125     -3.23250     -3.03250     -3.28525\n\n## Abordagem matricial\ny <- as.matrix(dados$RG)\nx <- model.matrix(~HIBRIDO, data = dados)\nbeta <- solve(t(x) %*% x) %*% t(x) %*% y\nbeta\n#                  [,1]\n# (Intercept)  10.27750\n# HIBRIDONP_10 -3.99450\n# HIBRIDONP_2  -1.44550\n# HIBRIDONP_3  -0.79750\n# HIBRIDONP_4  -0.79675\n# HIBRIDONP_5  -1.52675\n# HIBRIDONP_6  -2.94125\n# HIBRIDONP_7  -3.23250\n# HIBRIDONP_8  -3.03250\n# HIBRIDONP_9  -3.28525\n# Anova com uma abrodagem matricial\nSQtrat <- t(beta) %*% t(x) %*% y - ((sum(y))^2)/40\nSQtotal <- t(y) %*% y - ((sum(y))^2)/40\nSQres <- SQtotal - SQtrat\nFc <- (SQtrat / 9) / (SQres / 30)\np_val <- pf(Fc, 9, 30, lower.tail = FALSE)\ncat(\"\\nSQtrat = \", SQtrat)\n# \n# SQtrat =  65.66176\ncat(\"\\nSQtotal = \", SQtotal)\n# \n# SQtotal =  171.8282\ncat(\"\\nSQres = \", SQres)\n# \n# SQres =  106.1664\ncat(\"\\nFc = \", Fc)\n# \n# Fc =  2.061599\ncat(\"\\np_val = \", p_val, \"\\n\")\n# \n# p_val =  0.06654502\n\nanova(mod1)\n# Analysis of Variance Table\n# \n# Response: RG\n#           Df  Sum Sq Mean Sq F value  Pr(>F)  \n# HIBRIDO    9  65.662  7.2958  2.0616 0.06655 .\n# Residuals 30 106.166  3.5389                  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"analdata.html","id":"delineamento-inteiramente-casualizado-dic","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.5 Delineamento inteiramente casualizado (DIC)","text":"","code":""},{"path":"analdata.html","id":"modelo-estatÃ­stico","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.5.1 Modelo estatÃ­stico","text":"O delineamento inteiramente casualizado (DIC) Ã© um delineamento adequado para Ã¡reas uniformes (parcelas sÃ£o uniformes), onde nÃ£o hÃ¡ necessidade de controle local (bloqueamento). Neste delineamento, os tratamentos devem ser distribuidos aleatoriamente nas parcelas.O modelo DIC Ã© dado por \\[\r\n{Y_{ij}} = m + {t_i} + {\\varepsilon _{ij}}\r\n\\]Onde \\(m\\) Ã© mÃ©dia geral experimento, \\(t_i\\) Ã© o efeito de tratamentos, sendo estimado por \\(\\hat t_i = \\bar Y_{.} - \\bar Y_{..}\\) com seguinte restriÃ§Ã£o: \\(\\sum_i \\hat t_i = 0 ~~~~\\forall_i\\) (leia-se, o somatÃ³rio dos efeitos de tratamento Ã© zero para todo tratamento ). \\(\\epsilon_{ij}\\) Ã© o erro experimental estimado por (\\(\\hat e_{ij} = Y_{ij} - m - \\hat t_i\\)) onde \\({e_{ij}}\\sim NID(0,{\\sigma ^2})\\).Experimentos em DIC serÃ£o analisados utilizando da funÃ§Ã£o dic()  pacote ExpDes.pt. Os argumentos desta funÃ§Ã£o sÃ£o:Para maiores detalhes, ver o pdf pacote32 ou ir em ajuda (digitar ?ExpDes.pt console).","code":""},{"path":"analdata.html","id":"dic-com-fatores-qualitativos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.5.2 DIC com fatores qualitativos","text":"Os dados utilizados neste exemplo estÃ£o na planilha QUALI conjunto de dados data_R.xlsx. Os prÃ³ximos cÃ³digos carregam o conjunto de dados e criam um grÃ¡fico tipo boxplot para explorar o padrÃ£o dos dados.Analizando o boxplot acima Ã© razoÃ¡vel dizer que mÃ©dias dos tratamentos sÃ£o diferentes, principalmente comparando o NUPEC_10 com NUPEC_1. Esta suspeita de diferenÃ§, entanto, deve ser suportada com realizaÃ§Ã£o da anÃ¡lise de variÃ¢ncia. pacote ExpDes.pt, quando os fatores sÃ£o qualitativos, anÃ¡lise complementar aplicada Ã© comparÃ§Ã£o de mÃ©dias. funÃ§Ã£o dic() pacote retorna tabela da ANOVA, anÃ¡lise de pressupostos  (normalidade e homogeneidade) e o teste de comparaÃ§Ã£o de mÃ©dias.funÃ§Ãµes pacote ExpDes.pt utilizam os dados â€œanexadosâ€ ao ambiente de trabalho, ou seja, um argumento data = . nÃ£o existe para suas funÃ§Ãµes. Note que exemplo abaixo foi utilizado funÃ§Ã£o (qualitativo, dic(...)). Isto permite acessar variÃ¡veis presentes data frame. Uma outra maneira de realizar esta mesma anÃ¡lise Ã© utilizando funÃ§Ã£o attach(qualitativo), qual carregarÃ¡ o data frame ambiente R, assim Ã© possÃ­vel utilizar funÃ§Ã£o dic(...). ApÃ³s realizada anÃ¡lise, Ã© recomendado executar o comando detach(qualitativo) para â€œlimparâ€ os dados ambiente de trabalho.interpretaÃ§Ã£o da significÃ¢ncia, ou seja, se mÃ©dias de produtividade dos hÃ­bridos foram significativamente diferentes uma determinada probabilidade de erro Ã© feita verificando-se o valor de â€œPr>fcâ€ na ANOVA. figura abaixo mostra distribuiÃ§Ã£o F considerando os graus de liberdade de tratamento e erro \\(F_{9, 30}\\) e nos ajuda compreender um pouco melhor isto. O valor de F calculado em nosso exemplo foi de 2.0616, o que resulta em uma probabilidade de erro acumulada de 0.066545 (6,654%). Na figura abaixo, esta probabilidade de erro acumulada estÃ¡ representada pela cor vermelha. Para que uma diferenÃ§significativa 5% de probabilidade de erro tivesse sido observada, o valor de F calculado deveria ter sido 2.2107 [qf(0.05, 9, 30, lower.tail = FALSE)], representado neste caso pela cor verde grÃ¡fico.Em sequÃªncia ANOVA, funÃ§Ã£o retorna o resultado da anÃ¡lise complementar solicitada. Neste exemplo, o teste de Tukey (5% de erro) Ã© utilizado. Este teste realiza todas combinaÃ§Ãµes possÃ­veis entre mÃ©dias (por isso o nome comparaÃ§Ã£o mÃºltipla de medias), comparando se diferenÃ§entre duas mÃ©dias Ã© maior ou menor que uma diferenÃ§mÃ­nima significativa (DMS). Esta DMS Ã© calculada pela seguinte fÃ³rmula \\(DMS = q \\times \\sqrt{QME/r}\\), onde q Ã© um valor tabelado, considerando o nÃºmero de tratamentos e o GL erro; QME Ã© o quadrado mÃ©dio erro; e r Ã© o nÃºmero de repetiÃ§Ãµes (ou blocos). O valor de q pode ser encontrado apÃªndice 1. Para este caso, considerando 10 e 30 como o nÃºmero de tratamentos e o GL erro, respectivamente, o valor de q Ã© 5,76, que aplicado na fÃ³rmula resulta em \\(DMS = 5,76 \\times \\sqrt{3.5389/4}=5.417\\). Logo, diferenÃ§mÃ­nima entre duas mÃ©dias para que estas sejam significativamente diferentes (5% de erro), deve ser de 5,417 toneladas. Como diferenÃ§entre maior mÃ©dia (NUPEC_01) e menor mÃ©dia (NUPEC_10), foi de 3,994 toneladas, mÃ©dia de todos os tratamentos foram consideradas iguais.Considerando nosso exemplo, parece razoÃ¡vel dizer que 10,27 t Ã© uma produÃ§Ã£o maior que 6,28 t. EntÃ£o, Ã© justo perguntar: O que pode ter acontecido para que mÃ©dias nÃ£o tenham sido consideradas diferentes considerando probabilidade de erro, mesmo tendo fortes indÃ­cios de que elas seriam? primeira opÃ§Ã£o que nos vem mente â€“e que na maioria das vezes Ã© encontrada em artigos cientÃ­ficosâ€“ Ã© que alteraÃ§Ãµes rendimento de grÃ£o observadas fora resultado acaso; ou seja, neste caso, hÃ¡ probabilidade de 6,65% de que uma diferenÃ§pelo menos tÃ£o grande quanto observada estudo possa ser gerada partir de amostras aleatÃ³rias se os tratamentos nÃ£o aferatem variÃ¡vel resposta. Logo, recomendaÃ§Ã£o estatÃ­stica neste caso, seria por optar por qualquer um dos tratamentos. ponto de vista prÃ¡tico, sabemos que esta recomendaÃ§Ã£o estÃ¡ totalmente equivocada. Neste ponto surge uma importante (e polÃªmica) questÃ£o: interpretaÃ§Ã£o p-valor. Um p-valor de 0,05 nÃ£o significa que hÃ¡ uma chance de 95% de que determinada hipÃ³tese esteja correta. Em vez disso, significa que se hipÃ³tese nula verdadeira e todas outras suposiÃ§Ãµes feitas forem vÃ¡lidas, haverÃ¡ 5% de chance que diferenÃ§ao menos tÃ£o grandes quanto observadas podem ser obtidas de amostras aleatÃ³rias. Ã‰ preciso ter em mente que o p-valor relatado pelos testes Ã© um significado probabilÃ­stico, nÃ£o biolÃ³gico. Assim, em experimentos biolÃ³gicos interpretaÃ§Ã£o desta estatÃ­stica deve ser cautelosa, pois um p-valor nÃ£o pode indicar importÃ¢ncia de uma descoberta. Por exemplo, um medicamento pode ter um efeito estatisticamente significativo nos nÃ­veis de glicose sangue dos pacientes sem ter um efeito terapÃªutico. Sugerimos leitura de cinco interessantes artigos relacionados este assunto (Altman Krzywinski 2017; Baker 2016; Chawla 2017; Krzywinski Altman 2013; Nuzzo 2014).fÃ³rmula da DMS descrita acima Ã© utilizada apenas se (e somente se) o nÃºmero de repetiÃ§Ãµes de todos os tratamentos Ã© igual. Caso algum tratamento apresente um nÃºmero inferir de repetiÃ§Ãµes, fato comumente observado em experimentos de campo devido presenÃ§de parcelas perdidas, DMS deste par de mÃ©dias em especÃ­fico deve ser corrigida. Geralmente, anÃ¡lises complementares sÃ£o realizadas quando ANOVA indica significÃ¢ncia para um determinado fator de variaÃ§Ã£o, entanto, o teste Tukey pode revelar diferenÃ§entre mÃ©dias, mesmo quando o teste F nÃ£o indicar essa diferenÃ§. Isto pode ser observado, principalmente quando probabilidade de erro muito prÃ³xima de 5%, por exemplo, Pr>Fc = 0.0502. recÃ­proca tambÃ©m Ã© verdadeira. O teste Tukey pode indicar que mÃ©dias nÃ£o diferem, se Pr>Fc = 0.0492, por exemplo.Em adiÃ§Ã£o Ã  justificativa anterior (alteraÃ§Ãµes rendimento de grÃ£o observadas fora resultado acaso), existem pelo menos mais trÃªs razÃµes potenciais para nÃ£o regeiÃ§Ã£o da hipÃ³tese \\(H_0\\) em nosso exemplo: () um experimento mal projetado com poder insuficiente para detectar uma diferenÃ§(Ã  5% de erro) entre mÃ©dias; (ii) os tratamentos foram mal escolhidos e nÃ£o refletiram adequadamente hipÃ³tese inicial estudo; ou (iii) o experimento foi indevidamente instalado e conduzido sem supervisÃ£o adequada, com baixo controle de qualidade sobre os protocolos de tratamento, coleta e anÃ¡lise de dados. Esta Ãºltima opÃ§Ã£o parece ser mais razoÃ¡vel aqui. Ã‰ possivel observar boxplot para o fator bloco que o bloco 4 parece ter uma mÃ©dia superior aos outros blocos. Sabe-ser que DIC, toda diferenÃ§entre repetiÃ§Ãµes de um mesmo tratamento comporÃ£o o erro experimental. Logo, neste exemplo, Ã¡rea experimental nÃ£o era homogÃªnea como se pressupunha na instalaÃ§Ã£o experimento. Isto ficarÃ¡ claro, posteriormente, ao analisarmos o mesmo conjunto de dados, entanto considerando um .Ã‰ possÃ­vel extrair os erros atravÃ©s de mod3$residuos. TambÃ©m Ã© possÃ­vel fazer diagnÃ³stico dos pressupostos modelo estatÃ­stico atravÃ©s de grÃ¡ficos utilizando funÃ§Ã£o plotres(): \r\nFigure 10.2: GrÃ¡fico de resÃ­duos gerado pela funÃ§Ã£o plotres()\r\nO p-valor teste de Shapiro-Wilk indicou que os resÃ­duos  nÃ£o seguem uma seguem uma distribuiÃ§Ã£o normal, o que pode ser confirmado pelo QQ-Plot. O argumento hvar = \"levene\" na funÃ§Ã£o Ã© utilizado para testar homogeneidade dos resÃ­duos. De acordo com o resultado teste, os resÃ­duos podem ser considerados homogÃªneos.ExercÃ­cio 7Utilize os pacotes dplyr e ggplot2 para confeccionar um grÃ¡fico de barras onde o eixo y Ã© representado pelos hÃ­bridos e o eixo x pelo rendimento de grÃ£os.Adicione uma linha vertical tracejada mostrando mÃ©dia global experimento.Adicione letra â€œaâ€ em todas barras para identificar nÃ£o diferenÃ§entre mÃ©dias.Resposta","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nqualitativo <- import(url, sheet = \"QUALI\")\np1 <- ggplot(qualitativo, aes(HIBRIDO, RG))+\n      geom_hline(yintercept = mean(qualitativo$RG), linetype = \"dashed\")+\n      geom_boxplot()+\n      stat_summary(geom = \"point\", fun = mean, shape = 23)+\n      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\np2 <- ggplot(qualitativo, aes(factor(BLOCO), RG))+\n      geom_hline(yintercept = mean(qualitativo$RG), linetype = \"dashed\")+\n      geom_boxplot()+\n      stat_summary(geom = \"point\", fun = mean, shape = 23)\nplot_grid(p1, p2)\n\nmod3 <- with(qualitativo, dic(HIBRIDO, RG))\n# ------------------------------------------------------------------------\n# Quadro da analise de variancia\n# ------------------------------------------------------------------------\n#            GL      SQ QM     Fc    Pr>Fc\n# Tratamento  9  65.662  3 2.0616 0.066545\n# Residuo    30 106.166  2                \n# Total      39 171.828  1                \n# ------------------------------------------------------------------------\n# CV = 23.02 %\n# \n# ------------------------------------------------------------------------\n# Teste de normalidade dos residuos \n# Valor-p:  3.544449e-05 \n# ATENCAO: a 5% de significancia, os residuos nao podem ser considerados normais!\n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# Teste de homogeneidade de variancia \n# valor-p:  0.9990636 \n# De acordo com o teste de bartlett a 5% de significancia, as variancias podem ser consideradas homogeneas.\n# ------------------------------------------------------------------------\n# \n# De acordo com o teste F, as medias nao podem ser consideradas diferentes.\n# ------------------------------------------------------------------------\n#    Niveis   Medias\n# 1    NP_1 10.27750\n# 2   NP_10  6.28300\n# 3    NP_2  8.83200\n# 4    NP_3  9.48000\n# 5    NP_4  9.48075\n# 6    NP_5  8.75075\n# 7    NP_6  7.33625\n# 8    NP_7  7.04500\n# 9    NP_8  7.24500\n# 10   NP_9  6.99225\n# ------------------------------------------------------------------------# Warning: `expand_scale()` is deprecated; use `expansion()` instead.\nplotres(mod3)"},{"path":"analdata.html","id":"dic-com-fatores-quantitativos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.5.3 DIC com fatores quantitativos","text":"Vimos anteriormente que os fatores qualitativos sÃ£o comparados atravÃ©s de comparaÃ§Ãµes de mÃ©dias. caso de fatores quantitativos, o comum Ã© utilizar regressÃµes como anÃ¡lise complementar. Neste tipo de anÃ¡lise \\(SQ_{Trat}\\) Ã© decomposta, e cada polinÃ´mio explicarÃ¡ parte desta soma de quadrados. O maior grau significativo polinÃ´mio determinarÃ¡ qual regressÃ£o escolhida. Para implementar essa anÃ¡lise, utilizando funÃ§Ã£o dic(), basta indicar como FALSE argumentos quali. O arquivo de dados â€œQUALITATIVO.xlsxâ€ contÃ©m trÃªs exemplos, com comportamento linear, quadrÃ¡tico e cÃºbico. Utilizando o que aprendemos ggplot2 atÃ© agora, vamos criar um grÃ¡fico para visualisar estes dados.   \r\nFigure 10.3: Exemplo de regressÃ£o com comportamento linear, quadrÃ¡tico e cÃºbico\r\nComo exemplo didÃ¡tico, vamos analisar os dados que parecem ter comportamento quadrÃ¡tico para ver se, estatisticamente, isto Ã© confirmado. Para isto, utilizamos funÃ§Ã£o subset()  para criar um novo conjunto de dados que contenha somente o nÃ­vel â€œQUADRÃTICAâ€ dos nosso dados originais. Para evitar uma longa saÃ­da, os resultados desta seÃ§Ã£o em especÃ­fico nÃ£o foram mostrados.Abaixo vamos reproduzir o exemplo para os usuÃ¡rios das funÃ§Ãµes aov() ou lm().\r\n O polinÃ´mio  de segundo grau deve ser o escolhido, pois ele foi o maior grau significativo (p-valor menor que 0,05). Os desvios da regressÃ£o nada mais sÃ£o que FALTA DE AJUSTE  dos modelos. PorÃ©m, como Ã© comum em ciÃªncias agrÃ¡rias, ajusta-se apenas polinÃ´mios atÃ© terceira ordem, devido dificuldade de interpretÃ§Ã£o de polinÃ´mios com ordens superiores. Ã‰ importante ressaltar que aqui estamos falando de regressÃµes polinomiais. Como serÃ¡ visto posteriormente, falta de ajuste Ã© inaceitÃ¡vel para modelos de regressÃ£o mÃºltipla. O valor de \\(R^2\\) Ã© dado pela razÃ£o entre \\(SQ_{RegressÃ£o}\\) e \\(SQ_{Tratamento}\\). exemplo acima, o \\(R^2\\) da regressÃ£o quadrÃ¡tica (selecionada) Ã©:\\[\r\n{R^2} = \\frac{{S{Q_{{\\rm{RegressÃ£o}}}}}}{{S{Q_{{\\rm{Tratamento}}}}}} = \\frac{{2,4900 + 1,5465}}{{4,0505}} = 0.9965\r\n\\]Percebe-se claramente pelos resultados acima que significÃ¢ncia grau polinÃ´mio estÃ¡ diretamente relacionado com quanto ele â€œcontribuiâ€ para explicar \\(SQ_{Trat}\\).\r\nforma mais prÃ¡tica de analisar uma regressÃ£o Ã© atravÃ©s de grÃ¡ficos. funÃ§Ã£o dic() fornece essa alternativa, atravÃ©s da funÃ§Ã£o graphics(). Nesta funÃ§Ã£o existe dois argumentos obrigatÃ³rios: , onde indicamos o objeto que contÃ©m saÃ­da da anÃ¡lise experimento e grau, onde indicamos o grau polinÃ´mio.GrÃ¡fico de resultados: uma proposta\r\nFigure 10.4: GrÃ¡fico de linhas gerado pela funÃ§Ã£o ggplot()\r\n  \r\nEste Ã© um exemplo simples de uso da funÃ§Ã£o ggplot(). PorÃ©m, como visto atÃ© aqui, esta funÃ§Ã£o (mesmo em aplicaÃ§Ãµes simples) tem suas complexidades. Pensando nisso, o pacote metan33 foi desenvolvido e contÃ©m funÃ§Ãµes Ãºteis que facilitam confeÃ§Ã£o de grÃ¡ficos. partir dos experimentos fatoriais, apenas funÃ§Ãµes deste pacote serÃ£o utilizados para confecÃ§Ã£o dos grÃ¡ficos.ExercÃ­cio 8Utilize funÃ§Ã£o plot_lines() pacote metan** para confeccionar um grÃ¡fico semelhante ao anterior. Para maiores detalhes veja ?metan::plot_lines.Resposta","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nquantitativo_todos <- import(url, sheet = \"QUANTI\")\nggplot(quantitativo_todos, aes(DOSEN, RG, col = TIPO)) +\n  geom_point() +\n  geom_smooth()\nquantitativo <- filter(quantitativo_todos, TIPO  ==  \"QUADRÃTICA\")\ncrd_Reg <- with(quantitativo, dic(DOSEN, RG, quali = FALSE, hvar = \"levene\"))\nReg <- quantitativo\nTotal <- aov(RG ~ 1, data = Reg)\nSaturado <- aov(RG ~ factor(DOSEN), data = Reg)\nLinear <- lm(RG ~ DOSEN, data = Reg)\nQuadratica <- lm(RG ~ poly(DOSEN, 2, raw = TRUE), data = Reg)\nCubica <- lm(RG ~ poly(DOSEN, 3, raw = TRUE), data = Reg)\nanova(Total, Linear, Quadratica, Cubica, Saturado) \n# Analysis of Variance Table\n# \n# Model 1: RG ~ 1\n# Model 2: RG ~ DOSEN\n# Model 3: RG ~ poly(DOSEN, 2, raw = TRUE)\n# Model 4: RG ~ poly(DOSEN, 3, raw = TRUE)\n# Model 5: RG ~ factor(DOSEN)\n#   Res.Df    RSS Df Sum of Sq       F    Pr(>F)    \n# 1     19 4.5308                                   \n# 2     18 4.5135  1    0.0172  0.4273    0.5232    \n# 3     17 0.7071  1    3.8064 94.4317 7.275e-08 ***\n# 4     16 0.6735  1    0.0336  0.8346    0.3754    \n# 5     15 0.6046  1    0.0688  1.7075    0.2110    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nggplot(quantitativo, aes(x = DOSEN, y = RG))+ # Indicar dados e variÃ¡veis\ngeom_point(color = \"blue\", size = 2) + # Adiociona e edita os pontos\ngeom_smooth(method = \"lm\",\n            formula = y ~ poly(x, 2, raw = TRUE),\n            color = \"red\",\n            fill = \"grey\")"},{"path":"analdata.html","id":"delineamento-blocos-ao-acaso-dbc","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.6 Delineamento blocos ao acaso (DBC)","text":"delineamento de blocos ao acaso uma restriÃ§Ã£o  na casualisaÃ§Ã£o Ã© imposta visando agrupar unidades experimentais uniformes dentro de um bloco, de maneira que heterogeneidade da Ã¡rea experimental fique entre os blocos. O bloquemento tem como objetivo reduzir o erro experimental, â€œtransferindoâ€ parte erro experimental para efeito de bloco. O modelo DBC Ã© dado por\\[\r\n{Y_{ij}} = m + {b_j} + {t_i} + {\\varepsilon _{ij}}\r\n\\]Onde \\(m\\) Ã© mÃ©dia geral experimento, \\(b_j\\) Ã© o efeito de bloco, \\(t_i\\) Ã© o efeito de tratamentos e \\(\\epsilon_{ij}\\) Ã© o erro experimental. pacote ExpDes.pt, este delineamento Ã© executado pela funÃ§Ã£o dbc(). Os argumentos desta funÃ§Ã£o sÃ£o:Percebe-se que apenas um argumento foi adicionado funÃ§Ã£o: bloco.","code":""},{"path":"analdata.html","id":"dbc-com-fatores-qualitativos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.6.1 DBC com fatores qualitativos","text":"Neste exemplo, vamos realizar ANOVA com os mesmos dados exemplo , agrupando mÃ©dias pelo teste Scott-Knott utilizando o argumento mcomp = \"sk\":Considerando o bloco como um fator de variaÃ§Ã£o experimento, pode-se afirmar que mÃ©dia de produtividade difere entre os hÃ­bridos testados. Como o efeito de bloco foi significativo Pr>Fc < 0.05, conclui-se que escolha pelo delineamento em blocos casualizados foi correta, e, principalmente, que disposiÃ§Ã£o dos blocos na Ã¡rea experimental possibilitou lograr heterogeneidade entre os blocos, deixando homogeneidade dentro de cada bloco. O Fc para o fator de tratamento â€œHÃBRIDOSâ€ foi de 18.716, qual acumula uma probabilidade de erro de somente 0.0000002029.Incluindo o bloco como fonte de variaÃ§Ã£o nota-se que 3 graus de liberdade (GL) que antes compunham o erro â€œsaÃ­ramâ€ e passaram compor o GL bloco. Assim, neste exemplo, o GL erro foi de 27. entanto, uma grande parte da soma de quadrados erro (aproximadamente 90%) observada delineamento DIC era oriunda efeito de bloco. Com isto, soma de quadrado erro considerando o delineamento DBC foi de apenas 10.525 e, mesmo com â€œperdaâ€ de 3 GL para compor o bloco, o quadrado mÃ©dio erro foi de somente 0.390 (89% menor quando comparado com o delineamento DIC). Consequentemente o valor F calculado para o fator HÃBRIDO foi significativo. Cabe ressaltar que soma de quadrados para o fator de variaÃ§Ã£o HÃBRIDO nÃ£o muda se o delineamento DIC ou DBC.Uma sugestÃ£o para apresentaÃ§Ã£o dos dados Ã© confecÃ§Ã£o de um grÃ¡fico de barras. Para isto, utilizaremos funÃ§Ã£o plot_bars() pacote metan. Utilizando o argumento lab.bars Ã© possÃ­vel adicionar letras teste SK. Por padrÃ£o, barras de erro mostrando o erro padrÃ£o da mÃ©dia sÃ£o adicionadas. Para suprimir estas barras use errorbar = FALSE. Ã‰ possÃ­vel tambÃ©m adicionar os valores Ã s barras usando values = TRUE, ordenar em ordem crescente ou decrescente, bem como modificar cor e preenchimento das barras facilmente","code":"\nmod4 <- with(qualitativo, dbc(HIBRIDO, BLOCO, RG, mcomp = \"sk\"))\n# ------------------------------------------------------------------------\n# Quadro da analise de variancia\n# ------------------------------------------------------------------------\n#            GL      SQ QM     Fc      Pr>Fc\n# Tratamento  9  65.662  4 18.716 2.0298e-09\n# Bloco       3  95.642  3 81.785 1.1000e-13\n# Residuo    27  10.525  2                  \n# Total      39 171.828  1                  \n# ------------------------------------------------------------------------\n# CV = 7.64 %\n# \n# ------------------------------------------------------------------------\n# Teste de normalidade dos residuos \n# valor-p:  0.6834664 \n# De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# Teste de homogeneidade de variancia \n# valor-p:  0.2599294 \n# De acordo com o teste de oneillmathews a 5% de significancia, as variancias podem ser consideradas homogeneas.\n# ------------------------------------------------------------------------\n# \n# Teste de Scott-Knott\n# ------------------------------------------------------------------------\n#    Grupos Tratamentos   Medias\n# 1       a        NP_1 10.27750\n# 2       a        NP_4  9.48075\n# 3       a        NP_3  9.48000\n# 4       b        NP_2  8.83200\n# 5       b        NP_5  8.75075\n# 6       c        NP_6  7.33625\n# 7       c        NP_8  7.24500\n# 8       c        NP_7  7.04500\n# 9       c        NP_9  6.99225\n# 10      c       NP_10  6.28300\n# ------------------------------------------------------------------------\np1 <- plot_bars(qualitativo, x = HIBRIDO, y = RG)\np2 <- plot_bars(qualitativo,\n                x = HIBRIDO,\n                y = RG,\n                lab.bar = c(\"a\", \"c\", \"b\", \"a\", \"a\", \"b\", \"c\", \"c\", \"c\", \"c\"),\n                order = \"desc\",\n                errorbar = FALSE,\n                values = TRUE,\n                n.dodge = 2)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"analdata.html","id":"dbc-com-fatores-quantitativos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.6.2 DBC com fatores quantitativos","text":"Para realizar anÃ¡lise de regressÃ£o considerando um delineamento DBC basta utilizar funÃ§Ã£o dbc()  incluindo o seguinte argumento quali = FALSE.ExercÃ­cio 9Rode programaÃ§Ã£o para os dados quantitativos considerando o delineamento DBC e compare os resultados com aquela obtida pela funÃ§Ã£o dic() para os mesmos dados.Rode programaÃ§Ã£o para os dados quantitativos considerando o delineamento DBC e compare os resultados com aquela obtida pela funÃ§Ã£o dic() para os mesmos dados.estimativas dos parÃ¢metros da regressÃ£o sÃ£o mesmas?estimativas dos parÃ¢metros da regressÃ£o sÃ£o mesmas?O p-valor para o testes de hipÃ³tese para efeito de tratamento Ã© o mesmo?O p-valor para o testes de hipÃ³tese para efeito de tratamento Ã© o mesmo?Resposta","code":""},{"path":"analdata.html","id":"anÃ¡lise-de-experimentos-utilizando-modelos-mistos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.2.6.3 AnÃ¡lise de experimentos utilizando modelos mistos","text":"funÃ§Ã£o gamem() pacote metan pode ser utilizada para analizar dados de experimentos unifatoriais utilizando um modelo misto de acordo com seguinte equaÃ§Ã£o:\\[\r\ny_{ij}= \\mu  + \\alpha_i + \\tau_j + \\varepsilon_{ij}\r\n\\]onde \\(y_ {ij}\\) Ã© o valor observado para o -Ã©simo genÃ³tipo na j-Ã©sima repetiÃ§Ã£o (= 1, 2, â€¦ g; j = 1, 2,. ., r); sendo g e r o nÃºmero de genÃ³tipos e repetiÃ§Ãµes, respectivamente; \\(\\alpha_i\\) Ã© o efeito aleatÃ³rio -Ã©simo genÃ³tipo; \\(\\tau_j\\) Ã© o efeito fixo da j-Ã©sima repetiÃ§Ã£o; e \\(\\varepsilon_ {ij}\\) Ã© o erro aleatÃ³rio associado \\(y_{ij}\\). Neste exemplo, usaremos os dados de exemplo data_g pacote metan.maneira mais fÃ¡cil de obter os resultados modelo acima Ã© usando funÃ§Ã£o get_model_data(). Vamos fazer isso.Teste de razÃ£o de mÃ¡xima verossimilhanÃ§aComponentes de variÃ¢nciaMÃ©dias preditasNo exemplo acima, o design experimental foi o de blocos completos casualizados. TambÃ©m Ã© possÃ­vel analisar um experimento conduzido em alfa-lattice com funÃ§Ã£o gamem(), baseado na seguinte equaÃ§Ã£o:\\[\\begin{gather}\r\ny_{ijk}= \\mu  + \\alpha_i + \\gamma_j + (\\gamma \\tau)_{jk} + \\varepsilon_{ijk}\r\n\\end{gather}\\]onde \\(y_ {ijk}\\) Ã© o valor observado -Ã©simo genÃ³tipo k-Ã©simo bloco da j- Ã©sima repetiÃ§Ã£o (= 1, 2, â€¦ g; j = 1, 2, .., r; k = 1, 2, .., b); respectivamente; \\(\\alpha_i\\) Ã© o efeito aleatÃ³rio -Ã©simo genÃ³tipo; \\(\\gamma_j\\) Ã© o efeito fixo da j-Ã©sima repetiÃ§Ã£o; \\((\\gamma \\tau)_{jk}\\) Ã© o efeito aleatÃ³rio k-Ã©simo bloco incompleto aninhado na repetiÃ§Ã£o j; e \\(\\varepsilon_{ijk}\\) Ã© o erro aleatÃ³rio associado \\(y_{ijk}\\). Neste exemplo, usaremos os dados de exemplo data_alpha pacote metan.","code":"\ngen_mod <- gamem(data_g, GEN, REP,\n                 resp = c(ED, CL, CD, KW, TKW, NKR))\n# Method: REML/BLUP\n# Random effects: GEN\n# Fixed effects: REP\n# Denominador DF: Satterthwaite's method\n# ---------------------------------------------------------------------------\n# P-values for Likelihood Ratio Test of the analyzed traits\n# ---------------------------------------------------------------------------\n#     model       ED       CL    CD     KW     TKW   NKR\n#  Complete       NA       NA    NA     NA      NA    NA\n#  Genotype 2.73e-05 2.25e-06 0.118 0.0253 0.00955 0.216\n# ---------------------------------------------------------------------------\n# Variables with nonsignificant Genotype effect\n# CD NKR \n# ---------------------------------------------------------------------------\nget_model_data(gen_mod, \"lrt\")\n# Class of the model: gamem\n# Variable extracted: lrt\n# # A tibble: 6 x 8\n#   VAR   model     npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 ED    Genotype     4  -91.9  192. 17.6      1   0.0000273 \n# 2 CL    Genotype     4  -86.2  180. 22.4      1   0.00000225\n# 3 CD    Genotype     4  -52.5  113.  2.45     1   0.118     \n# 4 KW    Genotype     4 -165.   339.  5.00     1   0.0253    \n# 5 TKW   Genotype     4 -190.   389.  6.72     1   0.00955   \n# 6 NKR   Genotype     4  -96.3  201.  1.53     1   0.216\nget_model_data(gen_mod, \"genpar\")\n# Class of the model: gamem\n# Variable extracted: genpar\n# # A tibble: 11 x 7\n#    Parameters     ED     CL     CD      KW      TKW    NKR\n#    <chr>       <dbl>  <dbl>  <dbl>   <dbl>    <dbl>  <dbl>\n#  1 Gen_var     5.37   4.27   0.240 181.     841.     2.15 \n#  2 Gen (%)    68.8   75.1   27.4    39.2     45.2   21.6  \n#  3 Res_var     2.43   1.41   0.634 280.    1018.     7.80 \n#  4 Res (%)    31.2   24.9   72.6    60.8     54.8   78.4  \n#  5 Phen_var    7.80   5.68   0.873 461.    1859.     9.94 \n#  6 H2          0.688  0.751  0.274   0.392    0.452  0.216\n#  7 H2mg        0.869  0.901  0.532   0.659    0.712  0.452\n#  8 Accuracy    0.932  0.949  0.729   0.812    0.844  0.673\n#  9 CVg         4.84   7.26   3.10    9.16     9.13   4.82 \n# 10 CVr         3.26   4.18   5.05   11.4     10.0    9.19 \n# 11 CV ratio    1.49   1.74   0.615   0.803    0.909  0.525\nget_model_data(gen_mod, \"blupg\")\n# Class of the model: gamem\n# Variable extracted: blupg\n# # A tibble: 13 x 7\n#    GEN      ED    CL    CD    KW   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     50.2  30.7  15.8  153.  354.  29.5\n#  2 H10    44.4  25.1  15.5  129.  268.  31.7\n#  3 H11    47.2  26.6  15.6  143.  297.  31.3\n#  4 H12    47.8  26.1  15.2  148.  293.  30.0\n#  5 H13    50.3  27.4  15.9  170.  319.  31.2\n#  6 H2     50.3  30.0  16.3  156.  338.  29.6\n#  7 H3     47.2  28.6  16.1  142.  331.  30.2\n#  8 H4     46.1  27.8  16.2  145.  310.  31.8\n#  9 H5     49.8  30.1  16.2  156.  309.  31.3\n# 10 H6     49.7  31.6  15.2  140.  325.  28.6\n# 11 H7     48.7  30.0  15.5  153.  346.  30.0\n# 12 H8     46.3  29.0  15.8  143.  339.  29.5\n# 13 H9     44.4  27.0  15.8  131.  301.  30.4\ngen_alpha <- gamem(data_alpha, GEN, REP, YIELD, block = BLOCK)\n# Method: REML/BLUP\n# Random effects: GEN, BLOCK(REP)\n# Fixed effects: REP\n# Denominador DF: Satterthwaite's method\n# ---------------------------------------------------------------------------\n# P-values for Likelihood Ratio Test of the analyzed traits\n# ---------------------------------------------------------------------------\n#      model    YIELD\n#   Complete       NA\n#   Genotype 1.18e-06\n#  rep:block 3.35e-03\n# ---------------------------------------------------------------------------\n# All variables with significant (p < 0.05) genotype effect\nget_model_data(gen_alpha, \"lrt\")\n# Class of the model: gamem\n# Variable extracted: lrt\n# # A tibble: 2 x 8\n#   VAR   model      npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>     <int>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 YIELD Genotype      5  -58.4  127. 23.6      1   0.00000118\n# 2 YIELD rep:block     5  -50.9  112.  8.61     1   0.00335\nget_model_data(gen_alpha, \"details\")\n# Class of the model: gamem\n# Variable extracted: details\n# # A tibble: 6 x 2\n#   Parameters YIELD                   \n#   <chr>      <chr>                   \n# 1 Ngen       24                      \n# 2 OVmean     4.4795                  \n# 3 Min        2.8873 (G03 in B6 of R3)\n# 4 Max        5.8757 (G05 in B1 of R1)\n# 5 MinGEN     3.3431 (G03)            \n# 6 MaxGEN     5.1625 (G01)\nget_model_data(gen_alpha, \"genpar\")\n# Class of the model: gamem\n# Variable extracted: genpar\n# # A tibble: 13 x 2\n#    Parameters      YIELD\n#    <chr>           <dbl>\n#  1 Gen_var        0.143 \n#  2 Gen (%)       48.5   \n#  3 rep:block_var  0.0702\n#  4 rep:block (%) 23.8   \n#  5 Res_var        0.0816\n#  6 Res (%)       27.7   \n#  7 Phen_var       0.295 \n#  8 H2             0.485 \n#  9 H2mg           0.798 \n# 10 Accuracy       0.893 \n# 11 CVg            8.44  \n# 12 CVr            6.38  \n# 13 CV ratio       1.32"},{"path":"analdata.html","id":"transformaÃ§Ã£o-de-dados","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.3 TransformaÃ§Ã£o de dados","text":"Em todos os exemplos apresentados atÃ© aqui, os resÃ­duos  devem cumprir os seguintes pressupsotos: normalidade, homocedasticidade e independÃªncia:\\[\r\n{\\boldsymbol{\\varepsilon }} \\sim {\\rm N}\\left( {0,{\\boldsymbol{}}{\\sigma ^2}} \\right)\r\n\\]Esses pressupostos  sÃ£o necessÃ¡rios para que o teste F seja utilizado na anÃ¡lise de variÃ¢ncia. Sob normalidade  dos resÃ­duos e hipÃ³tese nula \\(H_0\\), razÃ£o entre somas de quadrado de tratamento e resÃ­duo tem distribuiÃ§Ã£o F (Rencher Schaalje 2008). JÃ¡ em condiÃ§Ãµes de nÃ£o normalidade dos resÃ­duos, o poder teste  (probabilidade de rejeitar \\(H_0\\)) Ã© reduzido. Apesar disso, nÃ£o hÃ¡ grandes mudanÃ§erro tipo quando pressuposiÃ§Ã£o de normalidade Ã© violada (Senoglu Tiku 2001), e por isso ele Ã© considerado robusto.Apesar teste F de ser robusto desvios da normalidade, Ã© comum que ela seja cumprida para que o teste seja aplicado. Quando pressuposiÃ§Ãµes  nÃ£o sÃ£o cumpridas, um dos procedimentos mais comum Ã© transformar os dados. transformaÃ§Ã£o Box-Cox (Box Cox 1964)  Ã© uma das mais comuns. Ela consiste em transformar os valores de \\(Y_i\\) por \\(Y_i(\\lambda)\\), sendo o valor de \\(\\lambda\\) estimado por mÃ¡xima verossimilhanÃ§. ApÃ³s transformaÃ§Ã£o de \\(Y_i\\) por \\(Y_i(\\lambda)\\) os dados seguem distribuiÃ§Ã£o normal com variÃ¢ncia constante.funÃ§Ã£o boxcox() , pacote MASS, pode ser utilizada para estimar o valor de \\(\\lambda\\). Uma sequÃªncia de valores de \\(\\lambda\\) sÃ£o estimados, e o escolhido Ã© aquele que maximiza funÃ§Ã£o de log-verossimilhanÃ§. modelo considerando o delineamento inteiramente casualizado (qualitativo), o pressuposto  de normalidade foi violado. O prÃ³ximo passo Ã© encontrar o valor de \\(\\lambda\\) para transformar variÃ¡veis, utilizando para isso funÃ§Ã£o boxcox().\r\nFigure 10.5: GrÃ¡fico gerado pela funÃ§Ã£o boxcox() para identificar o valor de lambda\r\nPercebe-se que o intervalo de \\(\\lambda\\) cruza pelo valor zero, indicando que transformaÃ§Ã£o log Ã© mais adequada. Uma forma mais prÃ¡tica de encontrar o valor de \\(\\lambda\\) que maximiza funÃ§Ã£o de log-verossimilhanÃ§ Ã© utilizar funÃ§Ã£o locator() . ApÃ³s executar funÃ§Ã£o abaixo, basta clicar com o cursor sobre o ponto que maximiza funÃ§Ã£o para que coordenadas sejam mostradas console.Utilizando transformaÃ§Ã£o log(), os resÃ­duos ainda continuaram sendo nÃ£o normais, apesar de se aproximaram mais da distribuiÃ§Ã£o normal neste caso. Quando transformaÃ§Ã£o nÃ£o eficiente, recomenda-se utilizaÃ§Ã£o de testes nÃ£o paramÃ©tricos, por exemplo, caso de um delineamento inteiramente casualizado, o teste de Kruskal-Wallis ou de um delineamento de blocos completos casualisados, o teste de Friedman.","code":"\nMASS::boxcox(RG ~ HIBRIDO, data = qualitativo,\n             lambda = seq(-2, 2, length = 20)) # Indica os valores de lambda \nlocator(n = 1)\n\nmod5.1  = with(qualitativo, dic(HIBRIDO, log(RG)))\nplotres(mod5.1)"},{"path":"analdata.html","id":"ancova","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4 AnÃ¡lise de covariÃ¢ncia (ANCOVA)","text":"","code":""},{"path":"analdata.html","id":"introduÃ§Ã£o","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4.1 IntroduÃ§Ã£o","text":"Neta seÃ§Ã£o, veremos alguns conceitos estatÃ­sticos e uma aplicaÃ§Ã£o numÃ©rica de uma tÃ©cnica interessante que pode ser Ãºtil, se corretamente utilizada, para reduzir o erro experimental em experimentos agronÃ´micos: anÃ¡lise de covariÃ¢ncia (ANCOVA). ANCOVA Ã© um modelo linear geral que combina princÃ­pios de ANOVA e regressÃ£o para avaliar se mÃ©dias de uma variÃ¡vel dependente sÃ£o iguais entre nÃ­veis de uma variÃ¡vel independente categÃ³rica (tratamento), controlando estatisticamente os efeitos de outras variÃ¡veis contÃ­nuas que nÃ£o sÃ£o de interesse primÃ¡rio. Tais variÃ¡veis contÃ­nuas sÃ£o medidas em cada unidade experimental e sÃ£o chamadas covariÃ¡veis. Idealmente, estas variÃ¡veis devem ser determinadas antes que os tratamentos tenham sido atribuÃ­dos Ã s unidades experimentais ou, mÃ­nimo, que os valores das covariÃ¡veis nÃ£o sejam afetados pelos tratamentos aplicados (Snedecor Cochran 1967). ANCOVA tem vÃ¡rias aplicaÃ§Ãµes. contexto da experimentaÃ§Ã£o agronÃ´mica, ela Ã© frequentemente utilizada reduzir variabilidade experimento pela contabilizaÃ§Ã£o da variabilidade nas unidades experimentais que nÃ£o puderam ser controladas pelo design experimental.Considere um experimento conduzido em uma Ã¡rea em que unidades experimentais exibem considerÃ¡vel variabilidade que nÃ£o pode ser controlada utilizando estratÃ©gias de bloqueio. O pesquisador acredita, digamos, baseado em experiÃªncias passadas, que uma ou mais caracterÃ­sticas das unidades experimentais podem ajudar descrever parte da variabilidade entre unidades experimentais. Nesta condiÃ§Ã£o, o pesquisador pode utilizar resultados de experimentos passados observados nas mesmas unidades experimentais â€“excluido o efeito de tratamentoâ€“ como uma covariÃ¡vel. Outra estratÃ©gia â€“embora um pouco distante em tempos de recursos financeiros limitadosâ€“ poderia ser realizaÃ§Ã£o de uma anÃ¡lise de solo em cada unidade experimental e utilizar os resultados obtidos como covariÃ¡veis. Neste sentido, ao utilizar informaÃ§Ãµes relacionadas caracterÃ­sticas fisico-quÃ­micas solo como uma covariÃ¡vel, o pesquisador tenta explicar variabilidade nas unidades experimentais que nÃ£o podem ser convenientemente controladas por outra tÃ©cnica experimental.","code":""},{"path":"analdata.html","id":"modelo-estatÃ­stico-1","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4.2 Modelo estatÃ­stico","text":"Em uma estrura de tratamentos fixos, unifatorial, conduzidos em delineamento inteiramente casualizado, o modelo da ANOVA tradicional visto anteriormente pode ser escrito da seguinte forma quando uma covariÃ¡vel numÃ©rica (\\(X\\)) tambÃ©m foi mensurada em cada unidade experimental.\\[\r\nY_{ij} = \\mu + \\tau_i + \\beta(X_{ij} - \\bar X_{..}) + \\varepsilon_{ij}\r\n\\]onde \\(Y_{ij}\\) Ã© o valor observado -Ã©simo tratamento na j-Ã©sima repetiÃ§Ã£o; \\(\\mu\\) Ã© mÃ©dia geral; \\(\\tau_i\\) Ã© o efeito -Ã©simo tratamento; \\(\\beta\\) Ã© o coeficiente de regressÃ£o de Y em X e \\(\\varepsilon_{ij}\\) Ã© o erro aleatÃ³rio.pressuposiÃ§Ãµes modelo da ANCOVA sÃ£o mesmas que ANOVA, ou seja, aditividade dos efeitos de bloco e tratamento (em DBC), normalidade, homogeneidade e independÃªcia dos resÃ­duos, em adiÃ§Ã£o Ã  duas importantes consideraÃ§Ãµes adicionais: () independÃªncia entre covariÃ¡vel e o efeito tratamento; e (ii) homogeneidade dos coeficientes angulares da regressÃ£o, que serÃ£o tratados Ã  partir daqui como slopes. Considerando que os pressupostos da ANOVA jÃ¡ sÃ£o conhecidos, veremos com mais detalhes estes dois Ãºltimos Ã  seguir.","code":""},{"path":"analdata.html","id":"independÃªncia-entre-a-covariÃ¡vel-e-os-efeitos-de-tratamento","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4.3 IndependÃªncia entre a covariÃ¡vel e os efeitos de tratamento","text":"Vimos anteriormente que covariÃ¡vel deve ser independente efeito tratamento. figura 6 mostra trÃªs diferentes cenÃ¡rios. primeiro () represetaÃ§Ã£o esquemÃ¡tica de um delineamento unifatorial conduzido em DIC Ã© mostrada. Neste exemplo, variÃ¢ncia da variÃ¡vel resposta (RG) pode ser dividida em duas partes. Uma correspondente aos efeitos dos tratamentos e outra devido ao erro experimental, ou variÃ¢ncia nÃ£o explicada pelos tratamentos. Aqui, vimos novamente que toda variÃ¢ncia nÃ£o explicada pelos termos modelo irÃ¡ compor o erro experimental. segundo exemplo (b) um cenÃ¡rio ideal para ANCOVA Ã© representado. Nesta condiÃ§Ã£o, covariÃ¡vel compartilha sua variÃ¢ncia apenas com o pouco da variÃ¢ncia RG que Ã© atualmente inexplicado pelos efeitos dos tratamentos . Em outras palavras, Ã© completamente independente dos tratamentos. Este cenÃ¡rio Ã© o Ãºnico em que ANCOVA tradicional Ã© apropriada.Regra para anÃ¡lise de covariÃ¢nciaO terceiro exemplo, (c) representa uma situaÃ§Ã£o em que pessoas costumam usar ANCOVA quando nÃ£o deveriam. Nesta situaÃ§Ã£o, o efeito da covariÃ¡vel se sobrepÃµe ao efeito tratamento. Em outras palavras, o efeito tratamento Ã© confundido com o efeito da covariÃ¡vel. Em situÃ§Ãµes como esta, covariÃ¡vel reduzirÃ¡ (estatisticamente falando) o efeito tratamento, pois explica uma parte da variaÃ§Ã£o que seria atribuÃ­da ao efeito de tratamento. Assim, efeitos espÃºrios de tratamento podem surgir, comprometendo interpretaÃ§Ã£o da ANCOVA (Stevens 2009).ANCOVA nem sempre Ã© uma soluÃ§Ã£o magica. O problema compartilhamento da variÃ¢ncia da covariÃ¡vel com variÃ¢ncia de tratamento Ã© comum e muitas vezes Ã© ignorado ou incompreendido pelos pesquisadores (Miller Chapman 2001). Em uma ampla revisÃ£o, Miller e Chapman citam muitas situaÃ§Ãµes em que pessoas aplicam ANCOVA de maneira incorreta. Recomendamos leitura deste artigo. Felizmente, modelos generalizados de ANCOVA que permitem tratar esta interaÃ§Ã£o tratamento-covariÃ¡vel tÃªm sido desenvolvidos (Mayer et al. 2014), mas isto estÃ¡ alÃ©m objetivo deste material.","code":""},{"path":"analdata.html","id":"homogeneidade-dos-slopes-da-regressÃ£o","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4.4 Homogeneidade dos slopes da regressÃ£o","text":"Baseado modelo estatÃ­stico apresentado, percebe-se que quando uma ANCOVA Ã© realizada, buscamos uma relaÃ§Ã£o geral entre variÃ¡vel dependente e covariÃ¡vel; ou seja, uma regressÃ£o global Ã© ajustada aos dados, ignorando Ã  que nÃ­vel tratamento uma determinada obsevaÃ§Ã£o pertence. Ao ajustar esse modelo geral, supomos, portanto, que essa relaÃ§Ã£o geral seja verdadeira para todos os nÃ­veis fator tratamento. Por exemplo, se houver uma relaÃ§Ã£o positiva (slope positivo) entre covariÃ¡vel e variÃ¡vel dependente primeiro nÃ­vel (\\(T_1\\)), presumimos que hÃ¡ uma relaÃ§Ã£o positiva (slope positivo) em todos os outros nÃ­veis tambÃ©m. Vamos tentar tornar esse conceito um pouco mais concreto. figura 7 mostra um grÃ¡fico de dispersÃ£o que exibe uma relaÃ§Ã£o hipotÃ©tica entre covariÃ¡vel e variÃ¡vel dependente em duas condiÃ§Ãµes: heterogeneidade de slope (esquerda) e homogeneidade de slope (direita). Na primeira condiÃ§Ã£o, relaÃ§Ã£o entre variÃ¡vel dependente e covariÃ¡vel Ã© positiva para os tratamentos \\(T_1\\) e \\(T_2\\), mas para o \\(T_3\\), esta relaÃ§Ã£o parece ser negativa. Esta Ã© uma condiÃ§Ã£o em que utilizaÃ§Ã£o da ANCOVA nÃ£o Ã© indicada. Na segunda condiÃ§Ã£o, relaÃ§Ã£o entre variÃ¡vel dependente e covariÃ¡vel Ã© muito semelhante entre os tratamentos.\r\nFigure 10.6: GrÃ¡fico de dispersÃ£o e linhas de regressÃ£o entre variÃ¡vel dependente e covariÃ¡vel. diferentes cores represetam trÃªs tratamentos hipotÃ©ticos.\r\n   ","code":""},{"path":"analdata.html","id":"um-exemplo-numÃ©rico","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.4.5 Um exemplo numÃ©rico","text":"AtÃ© aqui, passamos por uma breve introduÃ§Ã£o abordando o modelo mais simples de ANCOVA. Esta tÃ©cnica, entanto, pode ser utilizada em qualquer delineamento experimental. Para maiores informaÃ§Ãµes, recomendamos leitura de trÃªs bons materiais: Rutherford (2001), um livro especÃ­fico para ANCOVA; Field, Miles, Field (2012), pp.Â 462, com aplicaÃ§Ã£o em R; e Scheiner Gurevitch (2001), pp.Â 77, com aplicaÃ§Ã£o em SAS.Vamos agora, utilizando um exemplo numÃ©rico, demonstrar como esta anÃ¡lise pode ser realizada software R e identificar como ela pode ser Ãºtil na anÃ¡lise de experimentos agronÃ´nicos. Os dados sÃ£o apresentados em Snedecor Cochran (1967), pp.Â 428 e sÃ£o resultantes de um experimento com 6 tratamentos (cultivares), conduzido em DBC com 4 blocos. variÃ¡vel resposta mensurada em cada unidade experimental foi gramas de espigas (GE) em adiÃ§Ã£o uma covariÃ¡vel, nÃºmero de plantas por unidade experimental (NPLA). Para realizar uma ANCOVA, recomendamos que seguintes etapas sejam seguidas:Assumindo que o R serÃ¡ utilizado, insira os dados e instale os pacotes necessÃ¡rios.Assumindo que o R serÃ¡ utilizado, insira os dados e instale os pacotes necessÃ¡rios.Explore os seus dados: FaÃ§uso de grÃ¡ficos para explorar o padrÃ£o encontrado nos dados. sugestÃ£o aqui Ã© utilizar grÃ¡ficos tipo boxplot, visto riqueza de informaÃ§Ãµes proporcionada por este tipo de grÃ¡ficos.Explore os seus dados: FaÃ§uso de grÃ¡ficos para explorar o padrÃ£o encontrado nos dados. sugestÃ£o aqui Ã© utilizar grÃ¡ficos tipo boxplot, visto riqueza de informaÃ§Ãµes proporcionada por este tipo de grÃ¡ficos.Verifique se covariÃ¡vel e os tratamentos sÃ£o independentes: Execute uma ANOVA com covariÃ¡vel como o variÃ¡vel dependente para verificar se covariÃ¡vel nÃ£o difere significativamente entre os nÃ­veis da variÃ¡vel independente (tratamento). Se um resultado significativo observado, interrompa anÃ¡lise aqui.Verifique se covariÃ¡vel e os tratamentos sÃ£o independentes: Execute uma ANOVA com covariÃ¡vel como o variÃ¡vel dependente para verificar se covariÃ¡vel nÃ£o difere significativamente entre os nÃ­veis da variÃ¡vel independente (tratamento). Se um resultado significativo observado, interrompa anÃ¡lise aqui.ANCOVA: assumindo que tudo estava bem nas etapas 2 e 3, execute anÃ¡lise principal de covariÃ¢ncia.ANCOVA: assumindo que tudo estava bem nas etapas 2 e 3, execute anÃ¡lise principal de covariÃ¢ncia.Verifique homogeneidade dos slopes da regressÃ£o: execute novamente ANCOVA, incluindo, agora, interaÃ§Ã£o entre variÃ¡vel independente e covariÃ¡vel. Se esta interaÃ§Ã£o Ã© significativa, entÃ£o vocÃª nÃ£o pode assumir homogeneidade dos slopes da regressÃ£o.Verifique homogeneidade dos slopes da regressÃ£o: execute novamente ANCOVA, incluindo, agora, interaÃ§Ã£o entre variÃ¡vel independente e covariÃ¡vel. Se esta interaÃ§Ã£o Ã© significativa, entÃ£o vocÃª nÃ£o pode assumir homogeneidade dos slopes da regressÃ£o.Compute anÃ¡lises complementares: encontrada diferenÃ§significativa para tratamento na etapa 4 e assumindo que etapa 5 indicou homogeneidade dos slopes da regressÃ£o, anÃ¡lises complementares â€“como comparaÃ§Ãµes mÃºltiplas de mÃ©diasâ€“ podem entÃ£o ser realizadas.Compute anÃ¡lises complementares: encontrada diferenÃ§significativa para tratamento na etapa 4 e assumindo que etapa 5 indicou homogeneidade dos slopes da regressÃ£o, anÃ¡lises complementares â€“como comparaÃ§Ãµes mÃºltiplas de mÃ©diasâ€“ podem entÃ£o ser realizadas.Vamos agora ver cada uma destas etapas detalhadamente.1. Download dos dados e pacotes necessÃ¡riosO seguinte cÃ³digo Ã© utilizado para instalar/carregar os pacotes necessÃ¡rios bem como para fazer o upload dos dados e armazenar dataframe covar   \r\n2. Explorando os dadosA construÃ§Ã£o de grÃ¡ficos tipo boxplot para variÃ¡vel resposta e covariÃ¡vel sÃ£o importantes, pois permitem identificar presenÃ§de possÃ­veis outliers nos dados alÃ©m de facilitar visualizaÃ§Ã£o de padrÃµes de associaÃ§Ã£o entre variÃ¡veis.\r\nFigure 10.7: GrÃ¡fico boxplot da variÃ¡vel independente (GE) e da covariÃ¡vel (NPUE) em cada nÃ­vel tratamento (T).\r\n   \r\nlinha tracejada horizontal representa mÃ©dia geral de cada variÃ¡vel. Seis estatÃ­sticas sÃ£o mostradas neste boxplot. mediana (linha horizontal); mÃ©dia (losango vermelho); caixas inferior e superior correspondem ao primeiro e terceiro quartis (percentis 25 e 75, respectivamente); linha vertical superior se estende da caixa atÃ© o maior valor, nÃ£o maior que \\(1,5 \\times {IQR}\\) (onde IQR Ã© amplitude interquartÃ­lica). linha vertical inferior se estende da caixa atÃ© o menor valor, de mÃ¡ximo, \\(1,5 \\times {IQR}\\). Dados alÃ©m destas linhas podem ser considerados outliers.3. Identificando independÃªncia entre covariÃ¡vel e os tratamentosA independÃªncia entre covariavel e os efeitos de tratamento, demostrada graficamente na figura 6 Ã© um importante pressuposto da ANCOVA. Esta independÃªncia Ã© checada realizando uma ANOVA considerando covariÃ¡vel como variÃ¡vel dependente. funÃ§Ã£o aov() Ã© utilizada para o ajuste modelo. O primeiro argumento, NPUE, Ã© variÃ¡vel resposta (neste caso covariÃ¡vel) que queremos analisar. O operador ~ informa que os seguintes argumentos irÃ£o ser considerados como fontes de variaÃ§Ã£o modelo. Neste caso, TRAT e BLOCO sÃ£o incluÃ­dos.O resultado da ANOVA acima indica que covariÃ¡vel Ã© independente dos efeitos de tratamento. Neste caso, prosseguimos para o prÃ³ximo passo. Se um resultado significativo encontrado nesta etapa, realizaÃ§Ã£o da ANCOVA nÃ£o Ã© recomendada.4. O modelo da ANCOVAPara realizar ANCOVA, utilizaremos mesma funÃ§Ã£o aov(). Agora, entanto, variÃ¡vel dependente (lado esquerdo operador ~) serÃ¡ GE e covariÃ¡vel NPUE serÃ¡ incluÃ­da modelo. Aqui, funÃ§Ã£o anova() Ã© substituÃ­da pela funÃ§Ã£o Anova() pacote car para que soma de quadrado tipo III (em linguagem SAS) seja computada. Este tipo de soma de quadrados Ã© utilizada, pois cada efeito Ã© ajustado para todos os outros termos modelo, diferentemente tipo (padrÃ£o na funÃ§Ã£o anova()), onde adiÃ§Ã£o de cada efeito Ã© feita sequencialmente e depende de como os termos modelo sÃ£o ordenados (Langsrud 2003).Analisando primeiro os valores de significÃ¢ncia, fica claro que covariÃ¡vel prediz significativamente variÃ¡vel dependente (p-valor < 0,01). Portanto, variÃ¡vel gramas de espiga por parcela Ã© influenciada pelo nÃºmero de plantas por parcela. Da mesma forma, o efeito de tratamento tambÃ©m foi significativo.   \r\nFigure 10.8: GrÃ¡fico residual modelo ANCOVA obtido pela funÃ§Ã£o autoplot().\r\nFigura abaixo obtida com funÃ§Ã£o autoplot(), mostra 4 grÃ¡ficos. Os dois primeiros sÃ£o os mais importantes para nÃ³s aqui. O primeiro (Residual vs fitted) pode ser utilizado para identificar homogeneidade das variÃ¢ncias. Uma distribuiÃ§Ã£o aleatÃ³ria dos pontos grÃ¡fico deve ser observada. Quando um padrÃ£o de distibuiÃ§Ã£o Ã© observado â€“como, por exemplo, distribuiÃ§Ã£o dos pontos em forma de funilâ€“ uma investigaÃ§Ã£o deve ser realizada, pois este padrÃ£o indica possiblidade de heterogeneidade das variÃ¢ncias. O segundo grÃ¡fico (Normal Q-Q) nos informa quanto normalidade dos resÃ­duos, ou seja, Ã© desejado que os pontos sejam distribuÃ­dos ao redor da linha diagonal. Em nosso caso, os pontos foram uniformemente distribuÃ­dos, o que confirma o resultado teste estatÃ­stico observado anteriormente.utilizaÃ§Ã£o de grÃ¡ficos residuais para identificar os pressupostos de modelos ainda nÃ£o Ã© muito difundida. entanto, estes grÃ¡ficos apresentam muitas vantagens em relaÃ§Ã£o aos mÃ©todos estatÃ­sticos, principalmente quando os testes sÃ£o aplicados em conjuntos de dados com um alto tamanho de amostra. Uma ampla discussÃ£o comparando mÃ©todos estatÃ­sticos e grÃ¡ficos na verificaÃ§Ã£o de pressupostos dos modelos Ã© apresentada por Kozak Piepho (2017).5. Homogeneidade dos slopes da regressÃ£oVimos anteriormente que homogeneidade dos slopes Ã© um pressuposto importante na ANCOVA. Para identificarmos isto em nosso exemplo, vamos criar um grÃ¡fico semelhate ao apresentado na figura 7.\r\nFigure 10.9: RegressÃµes ajustadas para cada tratamento entre variÃ¡vel dependente e covariÃ¡vel. linha preta tracejada representa regressÃ£o geral. O slope desta regressÃ£o Ã© utilizado para obtenÃ§Ã£o das mÃ©dias ajustadas.\r\nO grÃ¡fico acima mostra uma regressÃ£o linear ajustada para cada tratamento (linhas coloridas) e uma regerssÃ£o linear geral (linha pontilhada) entre covariÃ¡vel e variÃ¡vel resposta. Observando cada reta ajustada, Ã© razoÃ¡vel dizer que os slopes podem ser considerados homogÃªneos, ou seja, todos eles apresentam valor positivo. Um teste de hipÃ³tese pode ser utilizado para testarmos se os slopes podem ou nÃ£o ser considerados homogÃªneos. Para isto, basta incluirmos o termo de interaÃ§Ã£o entre covariÃ¡vel e o tratamento modelo ancova ajustado anterioremente. funÃ§Ã£o update() pode ser utilizada para este fim, como segue:Como jÃ¡ suspeitÃ¡vamos, interaÃ§Ã£o covariÃ¡vel \\(\\times\\) tratamento nÃ£o foi significativa, indicando homogeneidade dos slopes.Na funÃ§Ã£o update() acima, o operador .~. significa â€œmanter mesma variÃ¡vel resposta e preditores objeto ancovaâ€ e + NPUE:TRAT significa â€œadicionar o termo de interaÃ§Ã£o ao modelo ancovaâ€.6. AnÃ¡lises complementaresNos Ãºltimos cinco tÃ³picos, vimos em detalhe os principais passos para o cÃ¡lculo da ANCOVA. Conseguimos identificar que covariÃ¡vel influencia variÃ¡vel resposta, que ela Ã© idependente dos nossos tratamentos e os slopes das regressÃµes de cada tratamento sÃ£o homogÃªneos. Duas etapas restam para nÃ³s agora: () identificar o quanto ganhamos â€“em termos de sucesso preditivoâ€“ considerando inclusÃ£o da covariÃ¡vel modelo; e (ii) quais sÃ£o mÃ©dias ajustadas apra cada tratamento. Uma maneira simples de identificar se inclusÃ£o da covariÃ¡vel melhorou prediÃ§Ã£o modelo, Ã© por meio da criaÃ§Ã£o de um grÃ¡fico de dispersÃ£o com uma linha de referÃªncia 1:1 (valores preditos vs observados). Testes estatÃ­sticos de seleÃ§Ã£o de modelos â€“como, por exemplo, o AICâ€“ tambÃ©m podem serem utilizados. Nesta etapa, vamos criar um novo dataframe chamado covar_pred, qual conterÃ¡, alÃ©m dos dados originais, os valores preditos pela ANOVA e ANCOVA.  Neste grÃ¡fico, linha pontilhada representa linha de referÃªncia 1:1. Fica evidente ao observar o grÃ¡fico acima que utilizaÃ§Ã£o da covariÃ¡vel melhorou capacidade preditiva modelo, pois os valores preditos estavam mais prÃ³ximos da linha de referÃªncia. Os valores de AIC tambÃ©m indicaram que o modelo ANCOVA foi mais preciso.GrÃ¡ficos tipo 1:1 sÃ£o Ãºteis para identificar capacidade preditiva de modelos. Dois cuidados, entanto, precisam ser tomados. Primeiro, por ser um grÃ¡fico grÃ¡fico de dispersÃ£o com uma linha de referÃªncia 1:1, os eixos x e y devem estar na mesma escala. Segundo, Ã© assumido que linha diagonal tem intercepto igual zero e slope igual um. Assim, um teste de hipotese para estes parÃ¢metros pode ser Ãºtil.O prÃ³ximo passo Ã© obtermos mÃ©dias de GE para cada tratamento ajustadas para um mesmo valor de NPUE. Considerando estas variÃ¡veis como Y e X, respectivamente, este valor Ã© dado pela expressÃ£o:\\[\r\n\\hat m_i = \\bar{Y}_{.}- \\hat \\beta(\\bar{X}_{.}- \\bar X_{..})\r\n\\]Onde \\(\\bar{Y}_{.}\\) Ã© mÃ©dia ajustada de Y -Ã©simo tratamento; \\(\\hat \\beta\\) Ã© o slope da regressÃ£o linear estimada entre variÃ¡vel dependente e covariÃ¡vel; \\(\\bar{X}_{.}\\) Ã© mÃ©dia da covariÃ¡vel para o -Ã©simo tratamento; e \\(\\bar{X}_{..}\\) Ã© mÃ©dia geral da covariÃ¡vel. software R, mÃ©dias ajustadas bem como os testes post hocs podem ser obtidos pelos seguintes cÃ³digos.figura acima mostra mÃ©dias ajustadas para os dois mÃ©todos. barras azuis representam o intervalo de confianÃ§95% da mÃ©dia predita enquanto que setas vermelhas indicam comparaÃ§Ã£o das mÃ©dias pelo teste de Tukey. Cultivares com setas que nÃ£o se sobrepÃµe apresentam mÃ©dias significativamente diferentes. Note que amplitude intervalo de confianÃ§Ã© menor para o modelo da ANCOVA. Assim, vimos como este mÃ©todo pode ser Ãºtil na reduÃ§Ã£o erro em anÃ¡lise de dados experimentais. Seguindo este exemplo, ANCOVA pode ser aplicada em experimentos onde uma potencial covariÃ¡vel estÃ¡ disponÃ­vel, como por exemplo, severidade de ferrugem na folha observada em cada unidade experimental antes da aplicaÃ§Ã£o de fungicida.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\n# dados\ncovar <- import(url, sheet = \"COVAR\")\n# duas primeiras colunas como fator\ncovar <- to_factor(covar, 1:2)\n\ncovar_ggplot <- \n  covar %>%\n  pivot_longer(names_to = \"variable\", values_to = \"val\", cols = c(NPUE, GE))\n# Estrutura dos dados\ncovar_ggplot\n# # A tibble: 48 x 4\n#    TRAT  BLOCO variable   val\n#    <fct> <fct> <chr>    <dbl>\n#  1 T1    1     NPUE        28\n#  2 T1    1     GE         202\n#  3 T1    2     NPUE        22\n#  4 T1    2     GE         165\n#  5 T1    3     NPUE        27\n#  6 T1    3     GE         191\n#  7 T1    4     NPUE        19\n#  8 T1    4     GE         134\n#  9 T2    1     NPUE        23\n# 10 T2    1     GE         145\n# # ... with 38 more rows\nmean_var <- \n  covar_ggplot %>%\n  group_by(variable) %>%\n  summarise(mean = mean(val))\n\nggplot(covar_ggplot, aes(x = TRAT, y = val)) +\n  geom_boxplot(fill = \"gray75\", width = 0.6) +\n  facet_wrap(~ variable, scales = \"free_y\") +\n  geom_hline(data = mean_var, aes(yintercept = mean), linetype = \"dashed\") +\n  stat_summary(fun = mean,\n               geom = \"point\",\n               shape = 23,\n               fill = \"red\")+\n  labs(x = \"Tratamentos\", y = \"valores observados\")\n# modelo ANOVA convencional para a covariÃ¡vel\nconvencional <- aov(NPUE ~  TRAT + BLOCO, data = covar)\n# AnÃ¡lise de variÃ¢ncia\nanova(convencional) \n# Analysis of Variance Table\n# \n# Response: NPUE\n#           Df  Sum Sq Mean Sq F value Pr(>F)\n# TRAT       5  45.833  9.1667  1.2079 0.3524\n# BLOCO      3  21.667  7.2222  0.9517 0.4407\n# Residuals 15 113.833  7.5889\n# modelo ANOVA convencional\nancova <- aov(GE ~  TRAT + NPUE + BLOCO, data = covar)\n# AnÃ¡lise de variÃ¢ncia\nAnova(ancova, type = 3)\n# Anova Table (Type III tests)\n# \n# Response: GE\n#             Sum Sq Df F value    Pr(>F)    \n# (Intercept)  169.9  1  1.7476  0.207376    \n# TRAT        3227.3  5  6.6381  0.002296 ** \n# NPUE        7391.0  1 76.0124 4.963e-07 ***\n# BLOCO       1502.4  3  5.1504  0.013158 *  \n# Residuals   1361.3 14                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nresiduals <- residuals(ancova)\n# normalidade dos resÃ­duos\nshapiro.test(residuals)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals\n# W = 0.97894, p-value = 0.8755\n\n# Homogeneidade das variÃ¢ncias\nleveneTest(residuals ~ TRAT, data = covar)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  5  1.1686 0.3624\n#       18\n\n# Teste de aditividade de Tukey\ntukey.add.test(covar$GE, covar$BLOCO, covar$TRAT)\n# \n# Tukey's one df test for additivity \n# F = 1.0000419   Denom df = 14    p-value = 0.3342722\n\n# interpretaÃ§Ã£o grÃ¡fica\nautoplot(ancova) +\n  geom_point(col = \"black\") +\n  theme(text = element_text(size = 8),\n        axis.title = element_text(size = 8),\n        axis.text = element_text(size = 8),\n        aspect.ratio = 1)\nggplot(covar, aes(NPUE, GE, col = TRAT)) +\n  geom_point(aes(col = TRAT)) +\n  geom_smooth(aes(col = TRAT), method = \"lm\", se = F) +\n  geom_smooth(col = \"black\", linetype = \"dashed\", method = \"lm\", se = F)+\n  theme(legend.position = \"bottom\",\n        aspect.ratio = 1)+\n  guides(col = guide_legend(nrow = 1, byrow = TRUE))+\n  labs(x =\"NÃºmero de plantas por parcela\",\n       y = \"Gramas de espigas por parcela\")\n# Incluindo o termo de interaÃ§Ã£o\nancova_int = update(ancova, .~. + NPUE:TRAT)\n# AnÃ¡lise de variÃ¢ncia\nAnova(ancova_int, type = 3) \n# Anova Table (Type III tests)\n# \n# Response: GE\n#             Sum Sq Df F value    Pr(>F)    \n# (Intercept)  300.0  1  2.8462 0.1258641    \n# TRAT         597.9  5  1.1344 0.4084810    \n# NPUE        3906.0  1 37.0530 0.0001821 ***\n# BLOCO       1311.4  3  4.1467 0.0421214 *  \n# TRAT:NPUE    412.5  5  0.7827 0.5867580    \n# Residuals    948.7  9                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanovaa = aov(GE ~  TRAT + BLOCO, data = covar)\n\nancova_pred = covar %>% mutate(pred = predict(ancova), metodo = \"ANCOVA\")\nanova_pred =  covar %>% mutate(pred = predict(anovaa), metodo = \"ANOVA\")\npreditos = rbind(ancova_pred, anova_pred)\nAIC(anovaa, ancova)\n#        df      AIC\n# anovaa 10 229.6856\n# ancova 11 187.0242\n# grÃ¡fico 1:1\nggplot(preditos, aes(x = pred, y = GE))+\n  geom_point(aes(col = TRAT)) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  xlim(130, 270) +\n  ylim(130, 270) +\n  theme(aspect.ratio = 1,\n        panel.spacing = unit(0,\"cm\")) +\n    guides(col = guide_legend(nrow = 1, byrow = TRUE))+\n  theme(legend.position = \"bottom\", legend.title = element_blank())+\n    facet_wrap(~metodo)+\n  labs(x = \"Gramas de espigas estimado\",\n       y = \"Gramas de espiga observada \")\nmed_anova = emmeans(anovaa, ~ TRAT) # MÃ©dia nÃ£o ajustada\nmed_ancoova = emmeans(ancova, ~ TRAT) # MÃ©dia ajustada para NPUE\nxlab = \"Gramas de espiga por parcela\"\nylab = \"Tratamento\"\nscale_x = scale_x_continuous(limits = c(140, 260))\np1 = plot(med_anova, comparisons = T, xlab = xlab, ylab = ylab) + scale_x\np2 = plot(med_ancoova, comparisons = T, xlab = xlab, ylab = ylab) + scale_x\nplot_grid(p1, p2, labels = c(\"ANOVA\", \"ANCOVA\"),\n          hjust = -1.5, vjust = 2.5, label_size = 8)"},{"path":"analdata.html","id":"general","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5 AnÃ¡lise de dados nÃ£o gaussianos","text":"Dados com distribuiÃ§Ãµes nÃ£o normal sÃ£o mais comuns que podemos imaginar. Na Ã¡rea agrÃ­cola, exemplos incluem percentagem de sementes que germinam (Binomial), contagem de ervas daninhas por parcela (Poisson), proporÃ§Ã£o de Ã¡rea foliar necrosada por uma determinada doenÃ§(Beta), escala de sintomas de uma determinada doenÃ§(Multinominal). Para todas distribuiÃ§Ãµes, exceto normal, variÃ¢ncia Ã© dependente da mÃ©dia. Logo, assumindo efeito significativo dos tratamentos, o pressuposto da homogeneidade das variÃ¢ncias serÃ¡ violado quando dados nÃ£o normais sÃ£o analizados (Stroup 2015). Neste momento, questÃ£o que surge Ã©: como estes dados deveriam ser analizados? Antes de responder esta questÃ£o, vamos passar por uma breve histÃ³ria.","code":""},{"path":"analdata.html","id":"da-anÃ¡lise-de-variaÃ§Ã£o-aos-modelos-lineares-mistos-generalizados","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5.1 Da anÃ¡lise de variaÃ§Ã£o aos modelos lineares mistos generalizados","text":"Fisher Mackenzie (1923), estudando variaÃ§Ã£o de diferentes cultivares de batata, publicaram o primeiro trabalho demonstrando o uso da ANOVA para avaliaÃ§Ã£o de experimentos agrÃ­colas. O mÃ©todo da ANOVA se tornaria popular e amplamente utilizado apÃ³s Fisher estabelecer bases teÃ³ricas e os pressupostos desta tÃ©cnica (Fisher 1925, 1935). ideias de Fisher foram extendidas para experimentos mais complexos, mais tarde, por Yates (1940). AtÃ© entÃ£o, questÃ£o da ANOVA de dados nÃ£o gaussianos parecia estar resolvida. O teorema central limite dava suporte tal anÃ¡lise. Mais tarde, Bartlett (1947) demonstrou como transformaÃ§Ãµes de dados nÃ£o normais poderiam ser Ãºteis para realizaÃ§Ã£o da ANOVA. Esta tÃ©cnica permanece sendo utilizada atÃ© hoje.Nesta mesma Ã©poca, terminologia â€œModelos Mistosâ€  foi introduzida por Eisenhart (1947). Mais tarde, Henderson (1949); Henderson (1950) propos equaÃ§Ãµes modelo misto, das quais os BLUPs sÃ£o soluÃ§Ãµes. Em 1953, este mesmo autor demonstrou os diferentes tipos de somas de quadrados (vistos na seÃ§Ã£o da anÃ¡lise de covariÃ¢ncia) e descreveu alguns mÃ©todos para estimaÃ§Ã£o dos componentes de variÃ¢ncia em dados nÃ£o ortogonais (Henderson 1953). Mais tarde (1971), estimaÃ§Ã£o da MÃ¡xima VerossimilhanÃ§Restrita (REML) como um mÃ©todo para estimativa dos componentes de variÃ¢ncia em um modelo com dados desbalanceados foi apresentada por Patterson Thompson (1971).Um resumo atÃ© aquiO uso da ANOVA para anÃ¡lise de experimentos jÃ¡ estava consolidada;utilizaÃ§Ã£o de transformaÃ§Ã£o de variÃ¡veis era uma tÃ©cnica aceita para realizaÃ§Ã£o da ANOVA com dados nÃ£o normais;teoria por trÃ¡z dos modelos mistos jÃ¡ era compreendida, entanto seu uso nÃ£o era disseminado devido, principalmente, devido necessidade de operaÃ§Ãµes matriciais complexas.Nelder Wedderburn (1972) extenderam base modelo linear da ANOVA e regressÃ£o para acomodar suposiÃ§Ãµes de probabilidade mais plausÃ­veis aos dados observados, resultando nos conhecidos modelos lineares generalizados (GLM, generalized linear models). essÃªncia dessa generalizaÃ§Ã£o proporciona duas importantes mudanÃ§nos pressupostos dos modelos: primeira Ã© que os dados nÃ£o necessariamente precisam ser normalmente distribuÃ­dos mas podem assumir qualquer distribuiÃ§Ã£o da famÃ­lia exponencial de distribuiÃ§Ãµes qual inclui, entre outras, distribuiÃ§Ã£o Normal, Poisson e Binomial (Koopman 1936). segunda Ã© que mÃ©dia nÃ£o Ã© necessariamente tomada como uma combinaÃ§Ã£o linear de parÃ¢metros, mas que alguma funÃ§Ã£o da mÃ©dia Ã©. Esta funÃ§Ã£o Ã© conhecida como funÃ§Ã£o de ligaÃ§Ã£o e nos GLMs relaciona mÃ©dia da variÃ¡vel resposta Ã  combinaÃ§Ã£o linear das variÃ¡veis explicativas (Nelder Wedderburn 1972). Semelhante evoluÃ§Ã£o dos LMs para GLMs, os LMMs foram extendidos para modelos lineares mistos generalizados (GLMM, generalized linear mixed-effect model) por Wolfinger Oâ€™connell (1993) e Breslow Clayton (1993).","code":""},{"path":"analdata.html","id":"estratÃ©gias-para-anÃ¡lise-de-dados-nÃ£o-gaussianos","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5.2 EstratÃ©gias para anÃ¡lise de dados nÃ£o gaussianos","text":"Quando confrontados com dados nÃ£o normais, maioria dos pesquisadores opta por uma das trÃªs opÃ§Ãµes seguir: () confiar na robustez da ANOVA clÃ¡ssica Ã  pequenos desvios de normalidade em ensaios balanceados (Blanca et al. 2017) e realizÃ¡-la sem nenhum peso na consciÃªncia; (ii) realizar alguma transformaÃ§Ã£o na variÃ¡vel e realizar ANOVA com dados transformados; (iii) utilizar algum teste nÃ£o paramÃ©trico. segunda opÃ§Ã£o parece ser mais utilizada. VariÃ¡veis sÃ£o transformadas para que os pressupostos de normalidade e homogeneidade das variÃ¢ncias seja cuprido â€“ou ao menos aproximadoâ€“ (Bartlett 1947).O uso de testes nÃ£o paramÃ©tricos tambÃ©m pode ser uma alternativa para anÃ¡lise de dados nÃ£o normais. Os testes de Friedman (Friedman 1937) e de Kruskal & Wallis (Kruskal Wallis 1952) sÃ£o alternativas nÃ£o paramÃ©tricas para anÃ¡lise de variÃ¢ncia nos delineamentos DBC e DIC (unifatoriais), respectivamente. aplicaÃ§Ã£o destes testes nÃ£o serÃ¡ o foco aqui, principalmente devido limitaÃ§Ã£o com relaÃ§Ã£o complexidade design experimental suportada por estes testes. Uma excelente aplicaÃ§Ã£o prÃ¡tica, entanto, pode ser vista em Field, Miles, Field (2012), pp.Â 653.Vale ressaltar que escolha por estes â€œatalhosâ€, entanto, nem sempre Ã© soluÃ§Ã£o definitiva. Dados de contagem com muitos valores zero nÃ£o podem ser normalizados por transformaÃ§Ã£o. Os testes nÃ£o paramÃ©tricos, ao contrÃ¡rio de como muitos pensam, requerem, sim, alguns pressupostos. Por exemplo, o teste de Freedman Ã© flexivel quando nÃ£o normalidade mas sÃ³ Ã© valido se distribuiÃ§Ã£o da variÃ¡vel resposta entre os grupos mesma em todos os outros aspectos (variÃ¢ncia, assimetria, curtose). Quando estes pressupostos nÃ£o sÃ£o cumpridos, o poder teste Ã© reduzido (Laurent Turk 2013). Assim, ao vez de encaixar os dados em estatÃ­sticas clÃ¡ssicas, os pesquisadores devem utilizar abordagens estatÃ­sticas que correspondam aos seus dados (Mora et al. 2008). Em casos de variÃ¡veis nÃ£o normais, mais indicadas sÃ£o os GLMs (Nelder Wedderburn 1972) ou GLMMs (Wolfinger Oâ€™connell 1993).HÃ¡bitos de aprendizado de que ANOVA pode ser aplicada diretamente dados transformados nÃ£o ajudam â€“e muitas vezes impedemâ€“ rÃ¡pida expansÃ£o uso de modelos mais sofisticados como os GLM(M)s. O uso de GLM(M)s, entanto, requer um aprendizado considerÃ¡vel. Dependendo da aplicaÃ§Ã£o, subida pode ser Ã­ngreme. Apresentar uma introduÃ§Ã£o ao uso dos GLM(M)s em R para anÃ¡lise de dados nÃ£o gaussianos e comparÃ¡-los com os procedimentos tradicionais Ã© nosso principal objetivo aqui.","code":""},{"path":"analdata.html","id":"introduÃ§Ã£o-aos-modelos-lineares-generalizados","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5.3 IntroduÃ§Ã£o aos modelos lineares generalizados","text":"Vamos considerar um simples experimento que comparou eficiÃªncia de um herbicida prÃ©-emergente controle de uma determinada erva daninha em comparaÃ§Ã£o com um tratamento controle. O ensaio foi realizado em um delineamento DIC com oito repetiÃ§Ãµes, sendo cada repetiÃ§Ã£o caracterizada por um vaso. Logo, o ensaio foi composto por 16 vasos. Em cada vaso foram semeadas 50 sementes da determinada erva daninha e apÃ³s um perÃ­odo previamente especificado, o nÃºmero de sementes germinadas foi observado. Assumindo que germinaÃ§Ã£o de cada semente Ã© um processo independente, variÃ¡vel resposta em cada unidade experimental (\\(Y_{ij}\\)) tem uma distribuiÃ§Ã£o binomial com N = 50 e probabilidade de germinaÃ§Ã£o de \\(\\pi_{ij}\\) para o -Ã©simo tratamento na j-Ã©sima repetiÃ§Ã£o.Intuitivamente, primeira opÃ§Ã£o de muitos pesquisadores para analisar estes dados seria iniciar com uma aproximaÃ§Ã£o normal. Com N = 50 â€“e considerando p = 0,5â€“ mesmo dados binomiais apresentam boa aproximaÃ§Ã£o normal. O modelo com aproximaÃ§Ã£o normal seria:Variavel resposta: \\(p_{ij} = y_{ij}/50\\), onde \\(y_{ij}\\) Ã© o nÃºmero de sementes germinadas -Ã©simo tratamento na j-Ã©sima repetiÃ§Ã£oVariavel resposta: \\(p_{ij} = y_{ij}/50\\), onde \\(y_{ij}\\) Ã© o nÃºmero de sementes germinadas -Ã©simo tratamento na j-Ã©sima repetiÃ§Ã£oO modelo tradicional Ã© entÃ£o \\(p_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}\\) onde \\(\\mu\\) Ã© mÃ©dia geral; \\(\\tau_i\\) Ã© o efeito -Ã©simo tratamento e \\(\\varepsilon_{ij}\\) Ã© o erro aleatÃ³rio assumido \\(..d \\sim N(0, \\sigma^2 )\\).O modelo tradicional Ã© entÃ£o \\(p_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}\\) onde \\(\\mu\\) Ã© mÃ©dia geral; \\(\\tau_i\\) Ã© o efeito -Ã©simo tratamento e \\(\\varepsilon_{ij}\\) Ã© o erro aleatÃ³rio assumido \\(..d \\sim N(0, \\sigma^2 )\\).Diferentemente modelo da ANOVA tradicional apresentado acima onde uma Ãºnica equaÃ§Ã£o Ã© considerada, os GLMs sÃ£o montados basicamente em trÃªs etapas (Stroup 2013):distribuiÃ§Ã£o das observaÃ§Ãµes: Em nosso exemplo, assumimos que \\(y_{ij} \\sim B(50, \\pi_i)\\);distribuiÃ§Ã£o das observaÃ§Ãµes: Em nosso exemplo, assumimos que \\(y_{ij} \\sim B(50, \\pi_i)\\);O preditor linear: Em nosso exemplo \\(\\eta_{ij} = \\eta + \\tau_{}\\). Aqui, \\(\\eta\\) Ã© mÃ©dia geral e \\(\\tau_{}\\) Ã© o efeito tratamento. Uma sutil mas importante diferenÃ§Ã© que aqui o residual (\\(\\varepsilon_{ij}\\)) nÃ£o Ã© considerado.O preditor linear: Em nosso exemplo \\(\\eta_{ij} = \\eta + \\tau_{}\\). Aqui, \\(\\eta\\) Ã© mÃ©dia geral e \\(\\tau_{}\\) Ã© o efeito tratamento. Uma sutil mas importante diferenÃ§Ã© que aqui o residual (\\(\\varepsilon_{ij}\\)) nÃ£o Ã© considerado.funÃ§Ã£o de ligaÃ§Ã£o: Foi mensionado anteriormente que nos GLMs mÃ©dia nÃ£o Ã© necessariamente tomada como uma combinaÃ§Ã£o linear de parÃ¢metros, mas que alguma funÃ§Ã£o da mÃ©dia Ã©. Dados de distribuiÃ§Ã£o binomial sÃ£o geralmente melhor ajustados utilizando o parÃ¢metro canÃ´nico \\(log[\\pi/(1-\\pi)]\\) (funÃ§Ã£o da mÃ©dia) que mÃ©dia em si. Esta funÃ§Ã£o Ã© funÃ§Ã£o de ligaÃ§Ã£o que relaciona distribuiÃ§Ã£o das observaÃ§Ãµes com o preditor linear. em nosso caso, \\(\\eta_{ij} = log[\\pi_{}/(1-\\pi_{})]\\). Assim, quando os parÃ¢metros modelo sÃ£o ajustados nÃ£o Ã© mÃ©dia que Ã© estimada, mas funÃ§Ã£o de ligaÃ§Ã£o. Neste caso, estamos interessados em estimar probabilidade de sucesso (germinaÃ§Ã£o) para cada tratamento, entÃ£o \\(\\hat \\pi_{} = 1/(1 + e ^ {-\\hat \\eta_{}})\\).funÃ§Ã£o de ligaÃ§Ã£o: Foi mensionado anteriormente que nos GLMs mÃ©dia nÃ£o Ã© necessariamente tomada como uma combinaÃ§Ã£o linear de parÃ¢metros, mas que alguma funÃ§Ã£o da mÃ©dia Ã©. Dados de distribuiÃ§Ã£o binomial sÃ£o geralmente melhor ajustados utilizando o parÃ¢metro canÃ´nico \\(log[\\pi/(1-\\pi)]\\) (funÃ§Ã£o da mÃ©dia) que mÃ©dia em si. Esta funÃ§Ã£o Ã© funÃ§Ã£o de ligaÃ§Ã£o que relaciona distribuiÃ§Ã£o das observaÃ§Ãµes com o preditor linear. em nosso caso, \\(\\eta_{ij} = log[\\pi_{}/(1-\\pi_{})]\\). Assim, quando os parÃ¢metros modelo sÃ£o ajustados nÃ£o Ã© mÃ©dia que Ã© estimada, mas funÃ§Ã£o de ligaÃ§Ã£o. Neste caso, estamos interessados em estimar probabilidade de sucesso (germinaÃ§Ã£o) para cada tratamento, entÃ£o \\(\\hat \\pi_{} = 1/(1 + e ^ {-\\hat \\eta_{}})\\).Dois exemplos numÃ©ricos serÃ£o implementados. O primeiro, considera anÃ¡lise de dados de proporÃ§Ã£o discreta. O segundo Ã© voltado para anÃ¡lise de dados de contagem. Os dados utilizados sÃ£o oriundos de um ensaio com nove cultivares de soja conduzidas em um delineamento DBC com quatro blocos. Em cada bloco, diversas variÃ¡veis foram mensuradas em 10 plantas aleatoriamente selecionadas. Para fins didÃ¡ticos, somente variÃ¡veis de interesse serÃ£o utilizadas aqui. () proporÃ§Ã£o de legumes viÃ¡veis, obtido pela razÃ£o entre o nÃºmero de legumes viÃ¡veis (com ao menos um grÃ£o) e o nÃºmero de legumes total das 10 plantas. (ii) o nÃºmero total de legumes das 10 plantas que continha quatro grÃ£os. Para cada exemplo, anÃ¡lise serÃ¡ realizada considerando uma ANOVA normal, utilizando uma transformaÃ§Ã£o indicada e utilizando um modelo misto generalizado.","code":""},{"path":"analdata.html","id":"dados-de-proporÃ§Ã£o","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5.4 Dados de proporÃ§Ã£o","text":"Por definiÃ§Ã£o, proporÃ§Ã£o de legumes viÃ¡veis observado na j-Ã©sima repetiÃ§Ã£o -Ã©simo tratamento (\\(p_{ij}\\)) tem uma distribuiÃ§Ã£o binomial onde \\(p_{ij} \\sim B(N, \\pi_{ij})\\), onde \\(\\pi_{ij}\\) denota probabilidade de uma observaÃ§Ã£o aleatÃ³ria tratamento bloco j apresentar caracterÃ­stica de interesse â€“ neste caso legume ser viÃ¡vel. Neste caso, o objetivo Ã© estimar probabilidade de cada tratamento e utilizando testes de hipÃ³teses comparÃ¡-las. O valor esperado para o tratamento Ã© dado entÃ£o por \\(N\\pi_i\\) com variÃ¢ncia \\(N\\pi_i(1-\\pi_i)\\).Mesmo em experimentos com tamanho de amostra grande, diferenÃ§significativas \\(\\pi_i\\) resultarÃ£o em variÃ£ncias heterogÃªneas, violando o pressuposto da ANOVA. Dados oriundo de proporÃ§Ãµes (\\(p_{ij}\\)) sÃ£o frequentemente transformados para escala \\(P^*_{ij} = sen^{-1} \\sqrt{P_{ij}}\\) (Rodrigues-Soares et al. 2018).Exemplo numÃ©ricoPara o exemplo, os dados de proporÃ§Ã£o de legumes viÃ¡veis descrito anteriormente serÃ£o utilizados. figura 25 mostra proporÃ§Ã£o de legumes viÃ¡veis para cada tratamento bem como distribuiÃ§Ã£o dos pontos em torno de uma linha de probabilidade normal esperada.\r\nFigure 10.10: ProporÃ§Ã£o de legumes viÃ¡veis em nove cultivares de soja () e grÃ¡fico Q-Q plot (b).\r\nmÃ©dia nÃºmero de legumes viÃ¡veis observado foi de 0.89 (89%). cultivar C1 apresenta um valor ligeiramente menor que outras cultivares. Em adiÃ§Ã£o, distribuiÃ§Ã£o desta variÃ¡vel parece seguir uma distribuiÃ§Ã£o normal; entanto, este grÃ¡fico sÃ³ deve ser utilizado para fins de exploraÃ§Ã£o dos dados. inferÃªncia quanto normalidade deve ser realizada nos resÃ­duos.Os cÃ³digos seguintes sÃ£o utilizados para realizar anÃ¡lise considerando os trÃªs mÃ©todos. O primeiro modelo armazenado objeto convencional Ã© ajustado sem nenhuma transformaÃ§Ã£o, onde variÃ¡vel respota (NLV/NLT) Ã© dada em funÃ§Ã£o ~ fator tratamento (CULTIVAR) + bloco (REP). O segundo modelo armazenado objeto transform Ã© ajustado com variÃ¡vel resposta transformada para o arcoseno da raiz quadrada da proporÃ§Ã£o (asin(sqrt(NLV/NLT))). O terceiro modelo armazenado objeto general Ã© ajustado considerando um modelo linear misto generalizado considerando o fator bloco aleatÃ³rio. Para implementaÃ§Ã£o deste modelo considerou-se o seguinte:distribuiÃ§Ã£o das observaÃ§Ãµes: \\(y_{ij}|\\beta_j \\sim B(N, \\pi_{ij})\\), onde N Ã© o nÃºmero de legumes total em cada bloco, e \\(\\pi_{ij}\\) Ã© probabilidade da ocorrÃªncia de legumes viÃ¡veis para o -Ã©simo tratamento j-Ã©simo bloco.distribuiÃ§Ã£o das observaÃ§Ãµes: \\(y_{ij}|\\beta_j \\sim B(N, \\pi_{ij})\\), onde N Ã© o nÃºmero de legumes total em cada bloco, e \\(\\pi_{ij}\\) Ã© probabilidade da ocorrÃªncia de legumes viÃ¡veis para o -Ã©simo tratamento j-Ã©simo bloco.O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\). Note que adiÃ§Ã£o efeito de bloco como aleatÃ³rio torna nosso modelo um modelo linear misto generalizado. Este efeito foi considerado aleatÃ³rio, principalmente pela ocorrÃªncia de diferentes nÃºmeros de legumes observado em cada bloco. Stroup (2015) sugere fortemente considerar o efeito de bloco aleatÃ³rio nestas condiÃ§Ãµes.O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\). Note que adiÃ§Ã£o efeito de bloco como aleatÃ³rio torna nosso modelo um modelo linear misto generalizado. Este efeito foi considerado aleatÃ³rio, principalmente pela ocorrÃªncia de diferentes nÃºmeros de legumes observado em cada bloco. Stroup (2015) sugere fortemente considerar o efeito de bloco aleatÃ³rio nestas condiÃ§Ãµes.funÃ§Ã£o de ligaÃ§Ã£o: \\(\\eta_{ij}\\) = Logit(\\(\\pi_{ij}\\)) = \\(log[\\pi_{ij}/(1-\\pi_{ij})]\\)funÃ§Ã£o de ligaÃ§Ã£o: \\(\\eta_{ij}\\) = Logit(\\(\\pi_{ij}\\)) = \\(log[\\pi_{ij}/(1-\\pi_{ij})]\\) Os modelos ajustados foram salvos em seus respectivos objetos. Neste momento, uma investigaÃ§Ã£o quanto aos pressuposstos destes modelos Ã© necessÃ¡ria. funÃ§Ãµes shapiro.test() e leveneTest() sÃ£o utilizadas para anÃ¡lise de normalidade e homogeneidade dos resÃ­duos, respectivamente.Nada parece haver de errado com nossos modelos atÃ© aqui. De fato, Ã  5% de erro, tanto os resÃ­duais modelo convencional quanto transform sÃ£o considerados homocedÃ¡sticos e normalmente distribuÃ­dos. Vamos, agora, uma abordagem grÃ¡fica.Testes de hipÃ³tese -incluÃ­ndo os testes de normalidade e homogeneidade- sÃ£o sensÃ­veis ao tamanho da amostra. Assim, quanto menor o tamanho da amostra (em nosso caso 27), menor Ã© probabilidae de rejeiÃ§Ã£o da hipÃ³tese \\(h_o\\) (resÃ­duos normais). Por outro lado, em tamanhos de amostra grande, probabilidade de rejeiÃ§Ã£o da hipÃ³tese \\(h_o\\) Ã© maior. Este assunto Ã© bem discutido por Kozak Piepho (2017), quais demonstraram que grÃ¡ficos residuais sÃ£o melhores que os testes estatÃ­sticos para verificar pressuposiÃ§Ãµes da ANOVA.  \r\nFigure 10.11: GrÃ¡ficos Q-Q plot dos resÃ­duos obtidos na anÃ¡lise da proporÃ§Ã£o de legumes viÃ¡veis.\r\nConclusÃ£o atÃ© aqui: baseado em testes de aderÃªncia Ã  normalidade e na interpretaÃ§Ã£o grÃ¡fica, os pressupostos modelo da ANOVA convencional foram cumpridos. EntÃ£o, Ã© justo perguntar: Por que utilizar um modelo mais â€œcomplicadoâ€ se hÃ¡ evidÃªncias de que ANOVA nos dados transformados (ou mesmo nos dados originais) nÃ£o trarÃ¡ maiores problemas? Antes de qualquer conclusÃ£o vamos observar os resultados dos trÃªs modelos.O cÃ³digo abaixo Ã© utilizado para obter mÃ©dias marginais dos modelos ajustados. Na ANOVA com os dados transformados e modelo generalizado, todas inferÃªncias sÃ£o realizadas na escala transfromada mas mÃ©dias sÃ£o apresentadas na escala original.\r\n\r\nFigure 10.12: MÃ©dias estimadas, intervalos de confianÃ§e comparaÃ§Ã£o de mÃ©dias para os modelos da ANOVA tradicional (), com dados transformados (b) e generalizado (c) para proporÃ§Ã£o de legumes viÃ¡veis.\r\nComparando os resultados dos trÃªs modelos na mesma escala, fica fÃ¡cil observar que o modelo generalizado apresentou um intervalo de confianÃ§das mÃ©dias (barra azul) menor quando comparado com os procedimentos tradicionais. seta vermelha representa comparaÃ§Ã£o das mÃ©dias pelo teste de Tukey 5% de erro. Tratamentos com setas que nÃ£o se sobrepÃµe diferem estatisticamente considerando probabilidade de erro.Neste ponto identificamos limitaÃ§Ãµes modelo convencional da ANOVA. transformaÃ§Ã£o indicada nÃ£o ajudou muito. Todos os indÃ­cios levavam acreditar que nÃ£o terÃ­amos maiores problemas realizando ANOVA com os dados transformados â€“ou mesmo originaisâ€“ devido boa aproximaÃ§Ã£o normal dos erros. Estes modelos, entanto, indicaram que diferenÃ§observadas entre mÃ©dias fora resultado acaso, o que parece nÃ£o fazer muito sentido quando observamos figura 30. Neste caso, o pesquisador reportaria estes resultados em seu artigo/relatÃ³rio e apresentaria alguma justificativa para este â€œfracassoâ€. O que precisamos compreender, entanto, Ã© que mesmo com uma boa aproximaÃ§Ã£o normal dos erros, os dados ainda continuam sendo dados binomias! Se observarmos conjunto de dados veremos que cada bloco possui um nÃºmero diferente de legumes totais. Ao considerar proporÃ§Ã£o de legumes viÃ¡veis e anÃ¡lisar essa variÃ¡vel, ignoramos completamente o N em nossa amostra. Poucos se dÃ£o conta, mas proporÃ§Ã£o de 0.5 de legumes viÃ¡veis Ã© mesma considerando 5 legumes viÃ¡veis de um total de 10 legumes e 500 legumes viÃ¡veis de um total de 1000 legumes. precisÃ£o, entanto Ã© diferente. O modelo generalizado aplicado neste exemplo considera esta diferenÃ§.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nSOJA <- import(url, sheet = \"SOJA\")\nmedlvia <- mean(SOJA$NLV / SOJA$NLT) # MÃ©dia de legumes viÃ¡veis\n\n# Explorando os dados\np1 <- ggplot(SOJA, aes(x = CULTIVAR, y = NLV/NLT)) + \n             geom_boxplot(fill = \"gray\") +\n             geom_hline(yintercept = medlvia, linetype = \"dashed\")+\n             stat_summary(fun = mean, geom = \"point\", shape = 23, fill = \"red\") +\n             labs(x = \"Cultivares\", y = \"Taxa de legumes viÃ¡veis\")\np2 <- ggplot(SOJA, aes(sample = NLV/NLT))+\n             qqplotr::stat_qq_line(col = \"red\")+\n             qqplotr::stat_qq_point()+\n             qqplotr::stat_qq_band(alpha = 0.2)+\n             labs(x = \"Theoretical quantiles\", y = \"Sample quantiles\")\nplot_grid(p1, p2, labels = c(\"(a)\", \"(b)\"),\n          label_size = 10, vjust = 3, hjust = -5)\nconvencional = lm(NLV/NLT ~ CULTIVAR + REP, data = SOJA)\ntransform = lm(asin(sqrt(NLV/NLT)) ~ CULTIVAR + REP, data = SOJA)\ngeneral = glmer(cbind(NLV, NLT-NLV) ~ CULTIVAR + (1|REP),\n                family = binomial,\n                data = SOJA)\nshapiro.test(residuals(convencional))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(convencional)\n# W = 0.93971, p-value = 0.1198\nshapiro.test(residuals(transform))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(transform)\n# W = 0.95492, p-value = 0.2814\nshapiro.test(residuals(general, type = \"deviance\"))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(general, type = \"deviance\")\n# W = 0.95317, p-value = 0.2557\nleveneTest(convencional$residuals ~ SOJA$CULTIVAR)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  8  0.0925 0.9991\n#       18\nleveneTest(transform$residuals ~ SOJA$CULTIVAR)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  8  0.1512 0.9948\n#       18\nres <- tibble(Convencional = residuals(convencional),\n              Transformado = residuals(transform),\n              Generalizado = residuals(general, type = \"deviance\")) %>%\n       gather()\nggplot(res, aes(sample = value)) +\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\n       qqplotr::stat_qq_band(alpha = 0.2)+\n       facet_wrap(~key, scales = \"free\")+\n       labs(x = \"Quantis teÃ³ricos\", y =  \"Quantis observados\")\nmed_conv <- emmeans(convencional, ~ CULTIVAR,  type = \"response\")\nmed_trans <- emmeans(transform, ~ CULTIVAR, type = \"response\")\n# Warning in ref_grid(object, ...): There are unevaluated constants in the response formula\n# Auto-detection of the response transformation may be incorrect\nmed_gene <- emmeans(general, ~ CULTIVAR, type = \"response\")\n# grÃ¡ficos para as mÃ©dias\nscale_x <- scale_x_continuous(limits = c(0.75, 1))\nxlab <- \"ProporÃ§Ã£o de legumes viÃ¡veis\"\np3 <- plot(med_conv, comparisons = T, xlab = xlab) + scale_x\np4 <- plot(med_trans, comparisons = T, xlab = xlab) + scale_x\np5 <- plot(med_gene, comparisons = T, xlab = xlab) + scale_x\nplot_grid(p3, p4, p5, ncol = 3, \n          labels = c(\"(a)\", \"(b)\", \"(c)\"), hjust = -2.5)"},{"path":"analdata.html","id":"dados-de-contagem","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.5.5 Dados de contagem","text":"Dados de contagem sÃ£o muito comuns em experimentos agronÃ´micos, tendo propriedade de serem nÃ£o negativos e inteiros. Em probabilidade, estes tipos de dados seguem uma distribuiÃ§Ã£o Poisson (Cochran 1940). exemplo de Zoz et al. (2018), dados discretos (\\(D_{ij}\\)) sÃ£o frequentemente transformados para escala \\(D^*_{ij} = \\sqrt{D_{ij}}\\) ou para \\(D^*_{ij} = \\sqrt{D_{ij}+0,5}\\) quando hÃ¡ um elevado nÃºmero de zeros presente.Exemplo numÃ©ricoPara este exemplo, utilizaremos variÃ¡vel nÃºmero de legumes com quatro grÃ£os (NL4G) conjunto de dados SOJA. Os cÃ³digos abaixo sÃ£o utilizado para exploraÃ§Ã£o dos dados.  \r\nFigure 10.13: NÃºmero de legumes com quatro grÃ£os em nove cultivares de soja () e grÃ¡fico Q-Q plot (b)\r\nmÃ©dia nÃºmero de legumes viÃ¡veis observado foi aproximadamente trÃªs legumes. O grÃ¡fico Q-Q nos ajuda compreender que distribuiÃ§Ã£o desta variÃ¡vel claramente nÃ£o Ã© normal. Uma grande quantidade de zero Ã© observada, o que Ã© lÃ³gico, pois existem cultivares com caracterÃ­stica de nÃ£o apresentar legumes com quatro grÃ£os.Os cÃ³digos seguintes sÃ£o utilizados para realizar anÃ¡lise considerando os trÃªs modelos. O primeiro modelo armazenado objeto convencional_count Ã© ajustado sem nenhuma transformaÃ§Ã£o, onde variÃ¡vel respota NL4G Ã© dada em funÃ§Ã£o (~) fator tratamento (CULTIVAR) \\(+\\) bloco (REP). O segundo modelo armazenado objeto transform_count Ã© ajustado com variÃ¡vel resposta transformada para raiz quadrada nÃºmero de legumes com quatro grÃ£os (sqrt(NL4G)). O terceiro modelo armazenado objeto general_count Ã© ajustado considerando um modelo linear misto generalizado com blocos aleatÃ³rios e considerando que os dados seguem uma distribuiÃ§Ã£o Poisson. Para implementaÃ§Ã£o deste modelo considerou-se o seguinte:distribuiÃ§Ã£o das observaÃ§Ãµes: \\(y_{ij}|\\beta_j \\sim P(\\lambda_{ij})\\).O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\).funÃ§Ã£o de ligaÃ§Ã£o: \\(\\eta_{ij}\\) = Log(\\(\\lambda_{ij}\\)) Conforme o primeiro exemplo, vamos realizar um diagÃ³stico grÃ¡fico dos resÃ­duos para os modelos ajustados.\r\nFigure 10.14: GrÃ¡ficos Q-Q plot dos resÃ­duos\r\nO grÃ¡fico Q-Q para estes modelos indicou que os resÃ­duos sÃ£o distribuÃ­dos normalmente, considerando o intervalo de confianÃ§. transformaÃ§Ã£o novamente nÃ£o ajudou muito aqui. Embora nÃ£o faÃ§lÃ³gica testar normalidade dos resÃ­duos modelo generalizado (visto que assumimos uma distribuiÃ§Ã£o Poisson para os dados), o grÃ¡fico Q-Q mostra que os resÃ­duos deste modelo se aproximou mais de uma distribuiÃ§Ã£o normal comparado com os outros dois mÃ©todos.ExercÃ­cio 10\r\nUtilize o teste de Shapiro-Wilk para testar hipÃ³tese de normalidade dos resÃ­duos dos modelos ajustados anteriormente.RespostaTodos os trÃªs modelos indicaram diferenÃ§significativas (5% de erro) para mÃ©dias das cultivares. Para ANOVA convencional, o erro padrÃ£o para todos os tratamentos foi 0.94, o que nÃ£o pode ser verdade porque variÃ¢ncia e, portanto, os erros padrÃ£o, devem ser uma funÃ§Ã£o da mÃ©dia em dados de contagem.\r\nFigure 10.15: MÃ©dias estimadas, intervalos de confianÃ§e comparaÃ§Ã£o de mÃ©dias para os modelos da ANOVA tradicional (), com dados transformados (b) e generalizado (c) para o nÃºmero de legumes com 4 grÃ£os.\r\nPara ANOVA convencional e com dados transformados, o intervalo de confianÃ§de 95% para mÃ©dia dos tratamentos apresentou limite inferior negativo para cultivares C1 e C9. Isto Ã© ilÃ³gico, pois nÃ£o podemos observar nÃºmero de legumes com quatro grÃ£os negativo. Para o modelo generalizado isto nÃ£o foi observado. Novamente aqui identificamos limitaÃ§Ã£o das tÃ©cnicas convencional para anÃ¡lise de dados nÃ£o normais. Stroup (2013) demonstrou que em alguns casos, o uso de transformaÃ§Ãµes pode ser mais impreciso que ANOVA aplicada diretamente aos dados nÃ£o transformados. Observamos o mesmo aqui.utilizaÃ§Ã£o dos modelos generalizados requer certos cuidados, tais como definiÃ§Ã£o da distribuiÃ§Ã£o da variÃ¡vel Ã  nÃ­vel de observaÃ§Ã£o e funÃ§Ã£o de ligaÃ§Ã£o ideal ser utilizada. Um resumo com principais funÃ§Ãµes utilizadas pode ser visto em Stroup (2015). extensÃ£o dos modelos lineares generalizados para os modelo lineares mistos generalizados permite ainda contemplar importantes aspectos na avaliaÃ§Ã£o de ensaios multi-ambientes , tais como modelagem da matriz de variÃ¢ncia-covariÃ¢ncia. O uso de modelos mistos tambÃ©m Ã© fortemente recomendado em ensaios com delineamentos mais complexos, tais como parcelas sub-divididas ou sub-sub-divididas (Piepho Edmondson 2018).Procedimentos que contemplam anÃ¡lise considerando GLMs e GLMMs estÃ£o disponÃ­veis nos principais softwares estatÃ­sticos. O PROC GLIMMIX e PROC GENMOD SAS, e funÃ§Ã£o glmer() pacotes lme4 R podem ser utilizadas para implementaÃ§Ã£o destes modelos.","code":"\n\nmednl4g <- mean(SOJA$NL4G)\n# Explorando os dados\np1 <- ggplot(SOJA, aes(x = CULTIVAR, y = NL4G)) + \n             geom_boxplot(fill = \"gray\") +\n             geom_hline(yintercept = mednl4g, linetype = \"dashed\") +\n             stat_summary(fun = mean, geom = \"point\",\n                         shape = 23, fill = \"red\") +\n     labs(x = \"Cultivares\", y = \"NÃºmero de legumes com quatro grÃ£os\")\np2 <- ggplot(SOJA, aes(sample = NL4G)) +\n             qqplotr::stat_qq_line(col = \"red\") +\n             qqplotr::stat_qq_point() +\n             qqplotr::stat_qq_band(alpha = 0.2)+\n      labs(x = \"Quantis teÃ³ricos\", y =  \"Quantis observados\")\nplot_grid(p1, p2, labels = c(\"(a)\", \"(b)\"),\n                   label_size = 10, vjust = 3, hjust = -5)\nconvencional_cont <- lm(NL4G ~ CULTIVAR + REP, data = SOJA)\ntransform_cont <- lm(sqrt(NL4G) ~ CULTIVAR + REP, data = SOJA)\ngeneral_cont <- glmer(NL4G ~ CULTIVAR + (1|REP),\n                     family = poisson,\n                     data = SOJA)\nres <- tibble(Convencional = residuals(convencional_cont),\n              Transformado = residuals(transform_cont),\n              Generalizado = residuals(general_cont, type = \"deviance\")) %>%\n       gather()\n\nggplot(res, aes(sample = value)) +\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\n       qqplotr::stat_qq_band(alpha = 0.2)+\n       facet_wrap(~key, scales = \"free\")\n\n# MÃ©dias ajustadas do nÃºmero de grÃ£os\nmed_conv_cont <- emmeans(convencional_cont, ~ CULTIVAR, type = \"response\")\nmed_trans_cont <- emmeans(transform_cont, ~ CULTIVAR, type = \"response\")\nmed_gene_cont <- emmeans(general_cont, ~ CULTIVAR, type = \"response\")\nscale_x <- scale_x_continuous(limits = c(-2, 15))\nxlab <- \"NÃºmero de legumes com 4 grÃ£os\"\np7 <- plot(med_conv_cont, comparisons = T, xlab = xlab) + scale_x\np8 <- plot(med_trans_cont, comparisons = T, xlab = xlab) + scale_x\np9 <- plot(med_gene_cont, comparisons = T, xlab = xlab) + scale_x\nplot_grid(p7, p8, p9, ncol = 3, \n          labels = c(\"(a)\", \"(b)\", \"(c)\"),\n          hjust = -2.5)"},{"path":"analdata.html","id":"efat","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6 Experimentos bifatoriais","text":"Experimentos fatoriais sÃ£o muito comuns nas ciÃªncias agrÃ¡rias, pois permitem o estudo de dois ou mais fatores em um mesmo experimento. Diversas sÃ£o vantagens em se conduzir um experimento deste tipo. Dentre elas, podemos citar reduÃ§Ã£o de custos, quando comparado Ã  realizar um experimento para cada fator, otimizaÃ§Ã£o da Ã¡rea experimental e dos tratos culturais, bem como possibilidade de identificar o efeito de dois ou mais fatores sobre magnitude da variÃ¡vel resposta. Esta Ã©, talvez, principal vantagem destes experimentos. Ao memo tempo, entanto, Ã© fonte de um dos maiores desafios encontrados meio acadÃªmico. O surgimento de uma terceira fonte de variaÃ§Ã£o, conhecida por interaÃ§Ã£o. Vamos considerar como exemplo, um experimento que avaliou influencia de dois fatores, digamos \\(\\alpha\\) e \\(\\tau\\), em uma determinada variÃ¡vel resposta. O modelo estatÃ­stico considerado neste tipo de experimento Ã©:\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + {\\rm{ }}\\mathop \\beta \\nolimits_{k}  + \\mathop \\alpha \\nolimits_i  + \\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) Ã© o valor observado da combinaÃ§Ã£o -Ã©simo nÃ­vel fator \\(\\alpha\\) com o j-Ã©simo nÃ­vel fator \\(\\tau\\) k-Ã©simo bloco; \\(\\mu\\) Ã© mÃ©dia geral; \\(\\mathop \\beta \\nolimits_{k}\\) Ã© o efeito bloco k; \\(\\mathop \\alpha \\nolimits_i\\) Ã© o efeito -Ã©simo nÃ­vel de \\(\\alpha\\) ; \\(\\mathop \\tau \\nolimits_j\\) Ã© o efeito j-Ã©simo nÃ­vel de \\(\\tau\\) ; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) Ã© o efeito da interaÃ§Ã£o -Ã©simo nÃ­vel de \\(\\alpha\\) com o j-Ã©simo nÃ­vel de \\(\\tau\\); e \\(\\mathop \\varepsilon \\nolimits_{ijk}\\) Ã© o erro aleatÃ³rio associado \\({y_{ijk}}\\), assumindo \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits^2 )\\).Basicamente, estes fatores podem ser divididos em dois tipos: qualitativos  e quantitativos . Um fator qualitativo Ã©, como o nome jÃ¡ diz, relacionado qualidade, ou seja, diferentes em tipo mas nÃ£o em quantidade. Como exemplo, podemos citar cultivares, defensivos agrÃ­colas, prÃ¡ticas de manejo, etc. Um fator quantitativo, por outro lado, Ã© caracterizado pela quantidade utilizada experimento. Podemos citar, por exemplo, doses de adubaÃ§Ã£o. Cabe ressaltar que o termo fatorial nÃ£o indica um delineamento experimental, mas uma forma de arranjo de tratamentos na Ã¡rea parcela. Estes experimentos podem ser conduzidos tanto em DIC  quanto DBC . Assim, em cada repetiÃ§Ã£o/bloco, o tratamento ser aplicado Ã© combinaÃ§Ã£o dos nÃ­veis dos dois fatores. combinaÃ§Ã£o de diferentes tipos de fatores nÃ£o influenciarÃ¡ anÃ¡lise de variÃ¢ncia dos dados, entanto resultarÃ¡ em algumas particularidades nas anÃ¡lises complementares, como serÃ¡ visto ao longo desta seÃ§Ã£o.","code":""},{"path":"analdata.html","id":"download-dos-dados","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.1 Download dos dados","text":"Nesta seÃ§Ã£o, serÃ£o analisados dados de experimentos bifatoriais com diferentes combinaÃ§Ãµes de fatores qualitativos e quantitativos na presenÃ§de interaÃ§Ã£o significativa e nÃ£o significativa. Em todos os exemplos, serÃ¡ considerado o delineamento de blocos completos casualizados, tendo como variÃ¡vel resposta, o rendimento de grÃ£os (RG). Para isto, utilizaremos cinco conjuntos de dados que serÃ£o detalhados seguir.Nesta seÃ§Ã£o, utilizaremos funÃ§Ãµes dos pacotes metan34 e ExpDes.pt  (Ferreira, Cavalcanti, Nogueira 2018) para anÃ¡lise estatistica dos dados. O pacote ExpDes.pt contÃ©m funÃ§Ãµes Ãºteis para anÃ¡lise de variÃ¢ncia de experimentos nos delineamentos experimentais e arranjo de tratamentos mais conhecidos. funÃ§Ã£o pacote ExpDes.pt utilizada neste exemplo serÃ¡ fat2.dbc()  qual analisa dados de experimentos bifatoriais em delineamento de blocos completos casualizados. Uma explicaÃ§Ã£o detalhada serÃ¡ dada agora ao passo em que nos outros experimentos serÃ£o apresentadas apenas linhas de comandos e uma breve discussÃ£o.delcaraÃ§Ã£o dos argumentos na funÃ§Ã£o fat2.dbc() Ã© simples. Basta informarmos o nome das colunas nosso arquivo correspondentes aos argumentos requeridos pela funÃ§Ã£o. Por exemplo fator1 e fator2, represetam os fatores em em estudo, que, neste caso, sÃ£o hÃ­bridos e fontes de nitrogÃªnio. argumento bloco informamos o nome da coluna de nossos dados correspondente aos blocos. O mesmo para o argumento resp que Ã© variÃ¡vel resposta, caso, o rendimento de grÃ£os.Definidos entrada de dados, precisamos declarar quais anÃ¡lises complementares serem realizadas, em caso de significancia estatÃ­stica. argumento quali, informamos qual o tipo de fator em estudo. declaraÃ§Ã£o Ã© um vetor lÃ³gico de comprimento 2. Em nosso caso, ao declararmos quali = c(TRUE, TRUE) (padrÃ£o) estamos informando que o ambos os fatores HÃBRIDO e FONTEN sÃ£o qualitativos. O argumento mcomp Ã© utilizado para informar o teste de comparaÃ§Ã£o (ou agrupamento ) de mÃ©dias utilizado. Em nosso exemplo, vamos utilizar o teste Tukey (padrÃ£o). argumento fac.names Ã© possivel informar nomes especÃ­ficos para os fatores. Os argumentos sigF e sigT sÃ£o utilizados para informar probabilidade de erro assumida na anÃ¡lise de variÃ¢ncia e nas analises de comparaÃ§Ã£o de mÃ©dias, respectivamente. Ambos tem padrÃ£o 5%.O resultado da funÃ§Ã£o fat2.dbc() irÃ¡ depender da significÃ¢ncia das fontes de variaÃ§Ã£o experimento. Em caso de interaÃ§Ã£o nÃ£o significativa  funÃ§Ã£o realiza uma comparaÃ§Ã£o de mÃ©dias para efeitos qualitativos e ajusta modelos de regressÃ£o polinomial para fatores quantitativos (quando significativos). Em experimentos com dois fatores qualitativos, em caso de interaÃ§Ã£o significativa, funÃ§Ã£o realiza o desdobramento das mÃ©dias dos fatores. Em caso de um fator qualitativo e outro quantitativo, em caso de interaÃ§Ã£o significativa funÃ§Ã£o realiza comparaÃ§Ã£o das mÃ©dias fator qualitativo para cada nÃ­vel fator quantitativo, bem como ajusta uma regressÃ£o para cada nÃ­vel fator qualitativo.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nFAT1_CI <- import(url, sheet = \"FAT1_CI\")\nFAT1_SI <- import(url, sheet = \"FAT1_SI\")\nFAT2_CI <- import(url, sheet = \"FAT2_CI\")\nFAT2_SI <- import(url, sheet = \"FAT2_SI\")\nFAT3 <- import(url, sheet = \"FAT3\")"},{"path":"analdata.html","id":"qualitativo-vs-qualitativo","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.2 Qualitativo vs qualitativo","text":"","code":""},{"path":"analdata.html","id":"sem-interaÃ§Ã£o-significativa","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.2.1 Sem interaÃ§Ã£o significativa","text":"Como exemplo de anÃ¡lise de um experimento bifatorial com dois fatores qualitativos sem interaÃ§Ã£o  significativa, utilizaremos o conjunto de dados FAT1_SI. JÃ¡ sabemos que interaÃ§Ã£o nÃ£o serÃ¡ significativa neste exemplo. Os dois prÃ³ximos grÃ¡ficos nos ajudam compreender porque.\r\nFigure 10.16: CaracterÃ­stica da produÃ§Ã£o em um experimento bifatorial sem interaÃ§Ã£o significativa\r\nÃ‰ possÃ­vel identificar que o magnitude da variÃ¡vel resposta de um fator nÃ£o Ã© alterada pelo nÃ­vel outro fator. Por exemplo, nota-se que o hÃ­brido NUPEC_2 parece ter uma mÃ©dia maior que os outros hÃ­bridos, independentemente da fonte de nitrogÃªnio utilizada. mesmo modo, fonte de nitrogÃªnio AmonioA parece proporcionar maior produtividade, independentemente hÃ­brido testado. Estas afirmaÃ§Ãµes, entanto, sÃ³ poderÃ£o ser confirmadas pela anÃ¡lise de variÃ¢ncia e posterior comparaÃ§Ã£o de mÃ©dias.GrÃ¡ficos iterativos podem ser obtidos utilizando funÃ§Ã£o ggplotly() pacote plotly. Este pacote converte um objeto ggplot2 em um grÃ¡fico iterativo que Ã© apresentado na aba viewer ambiente de trabalho RStudio.AnÃ¡lise de variÃ¢nciaPlotagem das mÃ©dias considerando os efeitos principaisConforme jÃ¡ sabido, interaÃ§Ã£o HIBRIDO x FONTEN nÃ£o foi significativa 5% de probabilidade de erro. Assim, o procedimento ser realizado Ã© comparaÃ§Ã£o das mÃ©dias para os fatores principais apenas. Os dois grÃ¡ficos abaixo mostram mÃ©dias dos fatores principais.\r\nfunÃ§Ã£o plot_bars() utilizada para confeccionar os grÃ¡ficos desta seÃ§Ã£o retorna anÃ¡lise descritiva para os fatores considerados se o argumento verbose =  TRUE adicionado na funÃ§Ã£o.","code":"\nhf3 <- plot_factbars(FAT1_SI,\n                     FONTEN,\n                     HIBRIDO,\n                     resp = RG)\nhf4 <- plot_factbars(FAT1_SI,\n                     FONTEN,\n                     HIBRIDO,\n                     resp = RG,\n                     invert = TRUE)\n\nplot_grid(hf3, hf4, labels = c(\"hf3\", \"hf4\"))\nwith(FAT1_SI,\n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  FONTEN,\n              bloco = BLOCO,\n              resp =  RG,\n              fac.names = c(\"HIBRIDO\", \"FONTEN\")))\nhf5 <- plot_bars(FAT1_SI, HIBRIDO, RG, lab.bar = c(\"c\", \"a\", \"b\"), values = TRUE)\nhf6 <- plot_bars(FAT1_SI, FONTEN, RG, lab.bar = c(\"a\", \"b\", \"b\"), values = TRUE)\nplot_grid(hf5, hf6, labels = c(\"hf5\", \"hf6\"))"},{"path":"analdata.html","id":"com-interaÃ§Ã£o-significativa","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.2.2 Com interaÃ§Ã£o significativa","text":"O conjunto de dados utilizado neste exemplo serÃ¡ o FAT1_CI. anÃ¡lise de variÃ¢ncia Ã© realizada utilizando mesma funÃ§Ã£o anterior.AnÃ¡lise de variÃ¢nciaComo interaÃ§Ã£o  foi significativa, o prÃ³ximo passo Ã© comparaÃ§Ã£o das mÃ©dias considerando os efeitos da interaÃ§Ã£o. Os valores das mÃ©dias fixando os nÃ­veis de cada fator sÃ£o, por padrÃ£o, calculados pela funÃ§Ã£o fat2.dbc(). apresentaÃ§Ã£o destas mÃ©dias em um trabalho cientÃ­fico, por exemplo, pode ser por meio de tabelas, ou grÃ¡ficos. funÃ§Ã£o plot_factbars() Ã© usada aqui para confeccionar um grÃ¡fico de barras mostrando interaÃ§Ã£o entre os fatores.Plotagem das mÃ©dias considerando interaÃ§Ã£oExercÃ­cio 11\r\n- Confeccione um grÃ¡fico semelhante ao acima considerando o fator FONTEN mapeado em escala de cinza.Resposta","code":"\nwith(FAT1_CI,\n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  FONTEN,\n              bloco = BLOCO,\n              resp =  RG,\n              fac.names = c(\"HIBRIDO\", \"FONTEN\")))\n\nplot_factbars(FAT1_CI, FONTEN, HIBRIDO, resp = RG,\n              values = TRUE,\n              lab.bar = c(\"bB\", \"cC\", \"aB\", # AmonioA\n                          \"bC\", \"bD\", \"aC\", # NitratoA\n                          \"aA\", \"aA\", \"aA\", # SulfatoA\n                          \"cC\", \"aB\", \"bC\")) # Ureia"},{"path":"analdata.html","id":"qualitativo-vs-quantitativo","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.3 Qualitativo vs quantitativo","text":"","code":""},{"path":"analdata.html","id":"sem-interaÃ§Ã£o-significativa-1","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.3.1 Sem interaÃ§Ã£o significativa","text":"O conjunto de dados utilizado neste exemplo serÃ¡ o FAT2_SI. mesmo modo exemplo anterior, iremos confeccionar um grÃ¡fico prÃ©vio para visualizaÃ§Ã£o dos dados.VisualizaÃ§Ã£o dos dados\r\nFigure 10.17: CaracterÃ­stica de produÃ§Ã£o em um experimento bifatorial sem interaÃ§Ã£o significativa\r\nanÃ¡lise estatistica dos dadosA funÃ§Ã£o fat2.rdb()  serÃ¡ novamente utilizada neste exemplo. declaraÃ§Ã£o dos argumentos Ã© idÃªntica ao exemplo anterior. O que mudarÃ¡ aqui, Ã© que serÃ¡ necessÃ¡rio informar que o fator DOSEN Ã© quantitativo. Para isso, usa-se o argumento quali.Como interaÃ§Ã£o nÃ£o foi significativa, proceder-se-comparaÃ§Ã£o de mÃ©dias dos dois hÃ­bridos considerando mÃ©dia de todas doses de nitrogÃªnio, e o ajuste de apenas uma regressÃ£o para os dois hÃ­bridos. Como o grau polinÃ´mio significativo foi quadrÃ¡tico, declararmos fit = 2 na funÃ§Ã£o plot_lines().","code":"\nggplot(FAT2_SI, aes(x = DOSEN, y = RG)) +\n       geom_point(aes(colour = factor(HIBRIDO)), size = 1.5) +\n       geom_smooth(aes(colour = factor(HIBRIDO)), method = \"loess\")\nwith(FAT2_SI, \n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  DOSEN,\n              bloco = BLOCO,\n              resp =  RG,\n              quali = c(TRUE, FALSE),\n              fac.names = c(\"HIBRIDO\", \"DOSE\")))\nh <- plot_bars(FAT2_SI, HIBRIDO, RG,\n                 width.bar = 0.5,\n                 lab.bar = c(\"a\", \"b\"))\nd <- plot_lines(FAT2_SI, DOSEN, RG,\n                fit = 2,\n                col = FALSE,\n                xlab = \"Doses de nitrogÃªnio\",\n                ylab = \"Rendimento de grÃ£os (Mg/ha)\") +\n     geom_text(aes(0, 6.5, label=(paste(expression(\"y = 6,8326 + 0,0012x - 0,0003x\"^2*\"  R\" ^2*\" = 0,99 \")))),\n               hjust = 0,\n               col = \"black\",\n               parse = TRUE) \nplot_grid(h, d, labels = c(\"h\", \"d\"), rel_widths = c(1, 3))"},{"path":"analdata.html","id":"com-interaÃ§Ã£o-significativa-1","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.3.2 Com interaÃ§Ã£o significativa","text":"O conjunto de dados utilizado neste exemplo serÃ¡ o FAT2_CI. Por se tratar de um experimento fatorial com um fator qualitativo (hibrido) e outro quantitativo (dose), convÃ©m confeccionar um grÃ¡fico com o rendimento observado de cada hÃ­brido em cada dose. Este grÃ¡fico, alÃ©m de servir como ferramenta para identificar possÃ­veis outliers, tambÃ©m nos permite identificar resposta de cada hÃ­brido. Cabe salientar que este Ã© um grÃ¡fico meramente ilustrativo. anÃ¡lise estatistica dos dados serÃ¡ realizada posteriormente.VisualizaÃ§Ã£o dos dados\r\n\r\nFigure 10.18: Rendimento observado de cada hÃ­brido em cada dose de nitrogÃªnio\r\nPara evitar uma longa saÃ­da neste documento devido interaÃ§Ã£o significativa, impressÃ£o dos resultados foi suprimida. Rode programaÃ§Ã£o em seu console para observar os resultados.%%anÃ¡lise de indicou efeitos significativos tanto para os efeitos principais, quanto para interaÃ§Ã£o. Assim, anÃ¡lises complementares realizadas foram () comparaÃ§Ã£o das mÃ©dias pelo teste Tukey em cada nÃ­vel da dose de N; e (ii) uma regressÃ£o polinomial ajustada para cada hÃ­brido. Por padrÃ£o, o mÃ¡ximo grau polinÃ´mio ajustado Ã© 3 (modelo cÃºbico).ComparaÃ§Ã£o das mÃ©dias dos hÃ­bridos em cada dose de nitrogÃªnio.comparaÃ§Ãµes de mÃ©dias sÃ£o apresentadas como saÃ­da da funÃ§Ã£o fat2.dbc() apÃ³s anÃ¡lise de variÃ¢ncia. Neste momento, utilizaremos funÃ§Ã£o plot_factbars()  pacote metan** para plotar mÃ©dias dos hÃ­bridos em cada dose de nitrogÃªnio. apresentaÃ§Ã£o grÃ¡fica de resultados, mesmo considerando mÃ©dias, Ã© uma alternativa interessante Ã  tabela, pois permite uma interpretaÃ§Ã£o mais clara e intuitiva dos resultados.\r\nFigure 10.19: GrÃ¡fico das mÃ©dias dos hÃ­bridos em cada dose de nitrogÃªnio.\r\nAjuste de regressÃ£o para cada hÃ­bridoNo exemplo anterior, apresentamos mÃ©dias dos hÃ­bridos em cada dose de nitrogÃªnio. Agora, criaremos um grÃ¡fico com o grau polinÃ´mio significativo ajustado de cada hÃ­brido. O grau ser ajustado deve ser identificado na saÃ­da da ANOVA . Para fins didÃ¡ticos um resumo Ã© fornecido.Utilizando uma equaÃ§Ã£o, Ã© possÃ­vel estimar produtividade para uma dose de nitrogÃªnio especÃ­fica nÃ£o testada, desde que ela esteja dentro intervalo estudado. Para isto, basta substituir o x na equaÃ§Ã£o pela dose ser testada. Por exemplo, para estimar qual seria produtividade hÃ­brido NUPEC_1 se tivÃ©ssemos aplicado 60 kg de N ha\\(^{-1}\\) basta resolver: \\(y = 9,2054 + 0,0209\\times 60\\), resultando em \\(y \\approx 10.5\\) Mg ha\\(^{-1}\\). interpretaÃ§Ã£o deste resultado, entanto, deve ser cautelosa. Inconscientemente, concluirÃ­amos que produtividade hÃ­brido aumentaria 0,0209 Mg ha\\(^{-1}\\) cada kg de nitrogÃªnio aplicado por hectare. Este fato, entanto, nÃ£o Ã© observado na prÃ¡tica. Por exemplo, produtividade nÃ£o irÃ¡ aumentar infinitamente medida em que se aumenta dose de nitrogÃªnio aplicado. Ãºnica conclusÃ£o vÃ¡lida, neste caso, Ã© que produtividade aumenta linearmente atÃ© 100 kg de N ha\\(^{-1}\\). Este resultado se deu em virtude de doses testadas nÃ£o terem sido o suficiente para identificar um outro comportamento na variÃ¡vel testada. Nestes casos, indica-se para estudos futuros aumentar o nÃºmero de doses. Quando nÃ£o se conhece o intervalo de dose em que variÃ¡vel em estudo apresenta uma resposta explicÃ¡vel, estudos pilotos podem ser realizados. Neste caso, testar-se-iam o mesmo nÃºmero de tratamentos (nÃºmero de doses), entanto com um intervalo maior entre doses (por exemplo, 0, 100, 200, 300 e 400 kg de N ha\\(^{-1}\\). Possivelmente, nesta amplitude, o comportamento da produtividade nÃ£o seria linear, pois em uma determinada dose, produtividade estabilizaria.Semelhante ao exemplo das mÃ©dias nas doses de nitrogÃªnio, utilizaremos funÃ§Ã£o plot_factlines()  para plotar, agora, uma regressÃ£o  ajustada para cada hÃ­brido. Os argumentos serem informados sÃ£o os seguintes: .data, o conjunto de dados (neste caso FAT2_CI); x e y, colunas dos dados correspondentes aos eixos x e y grÃ¡fico, respectivamente; group coluna que contÃ©m os nÃ­veis dos fatores em que regressÃµes serÃ£o ajustadas; fit um vetor de comprimento igual ao nÃºmero de nÃ­veis da coluna informada em group. O nÃºmero indicado em cada posiÃ§Ã£o vetor, corresponde ao grau polinÃ´mio ajustado (mÃ¡ximo grau ajustado = 4). Em nosso exemplo, utilizaremos fit = c(2, 1) para ajustar uma regressÃ£o quadrÃ¡tica para o hÃ­brido NUPEC_1 e uma regressÃ£o linear para o hÃ­brido NUPEC_2.Observando-se figura acima, Ã© possÃ­vel identificar o comportamento quadrÃ¡tico da variÃ¡vel resposta hÃ­brido NUPEC_1. Para estes hÃ­bridos, houve um incremento positivo na produtividade atÃ© um ponto, posteriormente observa-se que produtividade tendeu reduzir. Uma explicaÃ§Ã£o biolÃ³gica para esta reduÃ§Ã£o seria que o excesso de nitrogÃªnio aplicado proporcionou um alto vigor vegetativo plantas, podendo ter ocorrido competiÃ§Ã£o entre plantas por Ã¡gua, luz e outros nutrientes, ou atÃ© mesmo tombamento das plantas. O ponto em X (dose) em que produtividade Ã© mÃ¡xima Ã© chamado de mÃ¡xima eficiÃªncia tÃ©cnica (MET) e pode ser estimado por: \\[\r\nMET = \\frac{{ - {\\beta _1}}}{{2 \\times {\\beta _2}}}\r\n\\]Substituindo com os parÃ¢metros estimados, temos:\r\n\\[\r\nMET = \\frac{{ - 0,05575}}{{2 \\times  -0,0005574}} = 50\r\n\\]Logo, dose que proporciona mÃ¡xima produtividade para o hÃ­brido NUPEC_1 Ã© aproximadamente 50 kg de N ha\\(^{-1}\\). Assim para sabermos qual Ã© esta produtividade estimada, basta substituir o x da equaÃ§Ã£o por 50, resultando em \\(y_{mÃ¡x}\\) = 12,949 Mg ha\\(^{-1}\\).Outro ponto importante que Ã© possÃ­vel de estimar utilizando uma equaÃ§Ã£o de segundo grau, Ã© mÃ¡xima eficiÃªncia econÃ´mica (MEE), ou seja, dose mÃ¡xima, neste caso de nitrogÃªnio, em que Ã© possÃ­vel aplicar obtendo-se lucro. Este ponto Ã© importante, pois partir de uma certa dose, os incrementos em produtividade nÃ£o compensariam o preÃ§o pago pelo nitrogÃªnio aplicado. Este ponto pode ser facilmente estimado por:\\[\r\nMEE = MET + \\frac{u}{{2 \\times \\beta_2 \\times m}}\r\n\\]onde u e m sÃ£o os preÃ§os nitrogÃªnio e milho em grÃ£o, respectivamente, na mesma unidade utilizada para estimativa da equaÃ§Ã£o (neste caso, preÃ§o nitrogÃªnio por kg e preÃ§o milho por tonelada). Considerando o preÃ§o de custo nitrogÃªnio como R 1,35 por kg e o preÃ§o de venda milho 600,00 por tonelada, substituindo-se na formula obtÃ©m-se:\\[\r\nMEE = 50 + \\frac{{1.35}}{{2 \\times (-0,0005574) \\times 600}} \\approx 48\r\n\\]Assim, dose mÃ¡xima de nitrogÃªnio que em que os incrementos de produtividade sÃ£o lucrativos Ã© de \\(\\approx 48\\) Kg ha\\(^{-1}\\).ExercÃ­cio 12Utilize funÃ§Ãµes pacote dplyr para selecionar apenas o hÃ­brido NUPEC_1 exemplo anterior.Utilize funÃ§Ãµes pacote dplyr para selecionar apenas o hÃ­brido NUPEC_1 exemplo anterior.Confeccione um grÃ¡fico com uma linha ajustada considerando um modelo polinomial de segundo grau.Confeccione um grÃ¡fico com uma linha ajustada considerando um modelo polinomial de segundo grau.Insira uma linha vertical tracejada e cinza que intercepta o eixo x na dose de mÃ¡xima eficiÃªncia tÃ©cnica.Insira uma linha vertical tracejada e cinza que intercepta o eixo x na dose de mÃ¡xima eficiÃªncia tÃ©cnica.Insira uma linha vertical sÃ³lida e cinza que intercepta o eixo x na dose de mÃ¡xima eficiÃªncia econÃ´mica.Insira uma linha vertical sÃ³lida e cinza que intercepta o eixo x na dose de mÃ¡xima eficiÃªncia econÃ´mica.Resposta","code":"\nggplot(FAT2_CI, aes(x = DOSEN, y = RG)) + # cria um objeto ggplot\n       geom_point(aes(colour = factor(HIBRIDO)), size = 1.5) + # adiciona pontos\n       geom_smooth(aes(colour = factor(HIBRIDO)), method = \"loess\") # adiciona banda\nwith(FAT2_CI, \n     fat2.dbc(fator1 =  HIBRIDO,\n     fator2 =  DOSEN,\n     bloco = BLOCO,\n     resp =  RG,\n     quali = c(TRUE, FALSE),\n     fac.names = c(\"HIBRIDO\", \"DOSE\")))\nplot_factbars(FAT2_CI, DOSEN, HIBRIDO,\n              resp = RG,\n              xlab = \"Doses de nitrogÃªnio\",\n              ylab = expression(paste(\"Rendimento de grÃ£os (Mg ha\"^-1,\")\")),\n              palette = \"Greys\",\n              lab.bar = c(\"a\", \"b\", # 0\n                          \"a\", \"b\", # 25\n                          \"a\", \"b\", # 50\n                          \"a\", \"b\", # 75\n                          \"a\", \"a\")) # 100\nplot_factlines(FAT2_CI, DOSEN, RG,\n               group = HIBRIDO,\n               fit = c(2, 1))"},{"path":"analdata.html","id":"quantitativo-vs-quantitativo","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.4 Quantitativo vs quantitativo","text":"Neste exemplo, iremos avaliar dados de um experimento que testou dois fatores quantitativos. O conjunto de dados utilizado Ã© o FAT3. anÃ¡lise de variÃ¢ncia, neste caso, Ã© realizada da mesma maneira que os exemplos anteriores, o que muda agora Ã© que anÃ¡lise complementar, em caso de interaÃ§Ã£o significativa serÃ¡ diferente. Por se tratar de dois fatores quantitativos, neste caso, doses de nitrogÃªnio e de potÃ¡ssio na cultura milho, Ã© de se esperar que, em caso de interaÃ§Ã£o  significativa, haja uma combinaÃ§Ã£o de doses Ã³timas que proporcione maior magnitude da variÃ¡vel resposta (rendimento de grÃ£os). Assim, uma anÃ¡lise de superfÃ­cie de resposta Ã© mais indicada para este tipo de experimento.Para anÃ¡lise neste exemplo, utilizaremos funÃ§Ã£o resp_surf(). Nesta funÃ§Ã£o estÃ£o implementadas seguintes rotinas: anÃ¡lise de variÃ¢ncia, ajuste da equaÃ§Ã£o de superfÃ­cie de resposta, determinaÃ§Ã£o dos pontos crÃ­ticos, estatisticas de ajuste e anÃ¡lise residual. O procedimento para anÃ¡lise Ã© simples. Os seguintes argumentos precisam ser declarados: .data, o conjunto de dados; fator1 o nome da coluna com o primeiro fator; fator2 o nome da coluna com o segundo fator; rep o nome da coluna com os blocos; e resp o nome da coluna com variÃ¡vel resposta que se deseja analizar. Neste exemplo, vamos armazenar os resultados objeto sresp.AnÃ¡lise estatÃ­sticaPodemos observar que interaÃ§Ã£o DOSEN x DOSEK foi significativa, assim metodologia de superficie de resposta foi corretamente utilizada. equaÃ§Ã£o de superfÃ­cide considerada Ã© um modelo com termos de segunda ordem, onde variÃ¡vel resposta Ã© estimada considerando dois preditores, neste caso doses de nitrogÃªnio e doses de potÃ¡ssio. Considerando estes fatores como e D o seguinte modelo Ã© ajustado: \\(Y_i = \\beta_0 + \\beta_1A_i+\\beta_2D_i+\\beta_3A_i^2+\\beta_4D_i^2 +\\beta_5A_iD_i+\\epsilon_i\\). Os parametros estimados estÃ£o em ParÃ¢metros estimados.doses Ã³timas sÃ£o estimadas pela seguinte equaÃ§Ã£o: \\[\r\n- 0.5 \\times ({{\\boldsymbol{}}^{ - 1}}{\\boldsymbol{X}})\r\n\\]Onde\\[\r\n{\\boldsymbol{}} = \\left( {\\begin{array}{*{20}{c}}{{\\beta _3}}&{{\\beta _5}/2}\\\\{{\\beta _5}/2}&{{\\beta _4}}\\end{array}} \\right)\r\n\\]e\\[\r\n{\\boldsymbol{X}} = \\left( \\begin{array}{l}{\\beta _1}\\\\{\\beta _2}\\end{array} \\right)\r\n\\]Em nosso exemplo,\\[\r\n{\\boldsymbol{}} = \\left( {\\begin{array}{*{20}{c}}{ - 0.11213}&{9.865e - 05}\\\\{9.865e - 05}&{ - 0.00763}\\end{array}} \\right){\\rm{; }}{{\\boldsymbol{}}^{ - 1}} = \\left( {\\begin{array}{*{20}{c}}{ - 8.91796}&{ - 0.1152}\\\\{ - 0.11527}&{ - 131.009}\\end{array}} \\right)\r\n\\]e\\[\r\n{\\boldsymbol{X}} = \\left( \\begin{array}{l}14.7502\\\\1.00569\\end{array} \\right)\r\n\\]Assim\\[\r\n- 0.5 \\times \\left[ {\\left( {\\begin{array}{*{20}{c}}{ - 8.91796}&{ - 0.1152}\\\\{ - 0.11527}&{ - 131.009}\\end{array}} \\right) \\times \\left( \\begin{array}{l}14.7502\\\\1.00569\\end{array} \\right)} \\right] = \\left( \\begin{array}{l}65.8292\\\\66.7277\\end{array} \\right)\r\n\\]","code":"\nsrmod =  resp_surf(FAT3,\n                   factor1 = DOSEN,\n                   factor2 = DOSEK,\n                   rep = BLOCO,\n                   resp = RG)\n# -----------------------------------------------------------------\n# Result for the analysis of variance \n# Model: Y = m + bk + Ai + Dj + (AD)ij + eijk \n# -----------------------------------------------------------------\n#             Df Sum Sq Mean Sq  F value   Pr(>F)    \n# BLOCO        3    158      53    3.621   0.0183 *  \n# DOSEN        3  65978   21993 1515.063  < 2e-16 ***\n# DOSEK        4  11817    2954  203.513  < 2e-16 ***\n# DOSEN:DOSEK 12   2363     197   13.563 1.21e-12 ***\n# Residuals   57    827      15                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# -----------------------------------------------------------------\n# Shapiro-Wilk's test for normality of residuals: \n# -----------------------------------------------------------------\n# W =  0.946194 p-valor =  0.002100403 \n# -----------------------------------------------------------------\n# Anova table for the response surface model \n# -----------------------------------------------------------------\n# Analysis of Variance Table\n# \n# Response: RG\n#             Df Sum Sq Mean Sq  F value    Pr(>F)    \n# DOSEN        1   3215    3215  15.4854  0.000186 ***\n# DOSEK        1   6538    6538  31.4899 3.301e-07 ***\n# I(DOSEN^2)   1  50925   50925 245.2693 < 2.2e-16 ***\n# I(DOSEK^2)   1   5098    5098  24.5541 4.440e-06 ***\n# DOSEN:DOSEK  1      1       1   0.0053  0.942298    \n# Residuals   74  15365     208                       \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# -----------------------------------------------------------------\n# Model equation for response surface model \n# Y = B0 + B1*A + B2*D + B3*A^2 + B4*D^2 + B5*A*D \n# -----------------------------------------------------------------\n# Estimated parameters \n# B0: -317.8340786\n# B1: 14.7502633\n# B2: 1.0056943\n# B3: -0.1121344\n# B4: -0.0076331\n# B5: 0.0001973\n# -----------------------------------------------------------------\n# Matrix of parameters (A) \n# -----------------------------------------------------------------\n# -0.1121344    9.87e-05 \n# 9.87e-05    -0.0076331 \n# -----------------------------------------------------------------\n# Inverse of the matrix A (invA) \n# -8.9179679    -0.1152744 \n# -0.1152744    -131.0091259 \n# -----------------------------------------------------------------\n# Vetor of parameters B1 e B2 (X) \n# -----------------------------------------------------------------\n# B1: 14.7502633\n# B2: 1.0056943\n# -----------------------------------------------------------------\n# Equation for the optimal points (A and D) \n# -----------------------------------------------------------------\n# -0.5*(invA*X)\n# Eigenvalue 1: -0.007633\n# Eigenvalue 2: -0.112135\n# Stacionary point is maximum!\n# -----------------------------------------------------------------\n# Stacionary point obtained with the following original units: \n# -----------------------------------------------------------------\n# Optimal dose (DOSEN): 65.8292\n# Optimal dose (DOSEK): 66.7277\n# -----------------------------------------------------------------\n# Fitted model \n# -----------------------------------------------------------------\n# A = DOSEN\n# D = DOSEK\n# y = -317.83408+14.75026A+1.00569D+-0.11213A^2+-0.00763D^2+2e-04A*D\n# -----------------------------------------------------------------\n# Shapiro-Wilk normality test\n# p-value:  0.4522241 \n# According to Shapiro-Wilk normality test at 5% of significance, residuals can be considered normal. \n# ------------------------------------------------------------------\nP1 = plot(srmod)\nP2 = plot(srmod, cut = 9,\n          colorkey = list(space = \"top\", width = 1),\n          xlab = \"Dose de NitrogÃªnio (Kg/ha)\",\n          ylab= \"Dose de PotÃ¡ssio (Kg/ha)\")\n\ngridExtra::grid.arrange(P1, P2, ncol = 2)"},{"path":"analdata.html","id":"experimentos-em-parcelas-subdivididas","chapter":"CapÃ­tulo 10 AnÃ¡lise de dados experimentais","heading":"10.6.5 Experimentos em parcelas subdivididas","text":"Experimentos fatoriais sÃ£o Ãºteis devido possibilidade de se testar dois ou mais fatores em um mesmo experimento. Uma desvantagem deste tipo de experimento Ã© que cada bloco deve receber todos os tratamentos, ou seja, todas combinaÃ§Ãµes dos nÃ­veis dos dois fatores. Assim, o nÃºmero de parcelas experimento e consequentemente o tamanho da Ã¡rea experimental crese drastricamente na medida em que sÃ£o incluÃ­dos fatores ou nÃ­veis de fatores experimento. Uma maneira de se contornar isto, Ã© conduÃ§Ã£o de experimentos em parcelas subdivididas.Parcelas subdivididas  sÃ£o um caso especial de estrutura de tratamentos fatorial em que um fator Ã© alocado na parcela principal e outro fator Ã© alocado na subparcela. Este tipo de estrutura de tratamentos pode ser utilizada quando um fator Ã© de dificil instalaÃ§Ã£o em pequenas parcelas, como por exemplo, semeadura mecanizada ou um sistema de irrigaÃ§Ã£o, e o segundo fator pode ser alocado em parcelas mais pequenas, como um doses de nitrogÃªnio, por exemplo.Diferentemente modelo fatorial tradicional, o modelo estatÃ­stico para anÃ¡lise de experimentos em parcelas subdivididas conta com mais uma fonte de variaÃ§Ã£o. Vamos considerar como exemplo, um experimento que avaliou influencia de dois fatores, digamos \\(\\alpha\\) e \\(\\tau\\), em uma determinada variÃ¡vel resposta, agora, conduzido em parcelas subivididas, onde o fator \\(\\alpha\\) foi alocado na parcela principal e o fator \\(\\tau\\) alocado na subparcela. O modelo estatÃ­stico considerado neste tipo de experimento Ã©:\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + {\\rm{ }}\\mathop \\alpha \\nolimits_i + \\mathop \\beta \\nolimits_{k} + \\mathop \\eta \\nolimits_{ik}  +\\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) Ã© variÃ¡vel resposta observada; \\(\\mu\\) Ã© mÃ©dia geral; \\(\\mathop \\alpha \\nolimits_i\\) Ã© o efeito -Ã©simo nÃ­vel de \\(\\alpha\\) ; \\(\\mathop \\beta \\nolimits_{k}\\) Ã© o efeito bloco k; \\(\\mathop \\eta \\nolimits_{ik}\\) Ã© o erro de parcela, mais conhecido como erro ; assumido \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits_\\eta^2 )\\); \\(\\mathop \\tau \\nolimits_j\\) Ã© o efeito j-Ã©simo nÃ­vel de \\(\\tau\\) ; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) Ã© o efeito da interaÃ§Ã£o -Ã©simo nÃ­vel de \\(\\alpha\\) com o j-Ã©simo nÃ­vel de \\(\\tau\\); e \\(\\mathop \\varepsilon \\nolimits_{ijk}\\) Ã© o erro da subparcela, mais conhecido como erro b, assumindo \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits^2 )\\).","code":"\nwith(FAT1_SI,\npsub2.dbc(fator1 =  HIBRIDO,\n          fator2 =  FONTEN,\n          bloco = BLOCO,\n          resp =  RG,\n          fac.names = c(\"HIBRIDO\", \"FONTEN\")))\n# ------------------------------------------------------------------------\n# Legenda:\n# FATOR 1 (parcela):  HIBRIDO \n# FATOR 2 (subparcela):  FONTEN \n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# $`Quadro da analise de variancia\\n------------------------------------------------------------------------\\n`\n#                GL      SQ QM      Fc Pr(>Fc)    \n# HIBRIDO         2  98.051  7 209.972   3e-06 ***\n# Bloco           3   0.128  2   0.183  0.9041    \n# Erro a          6   1.401  3                    \n# FONTEN          2  83.048  6  71.117  <2e-16 ***\n# HIBRIDO*FONTEN  4   1.350  4   0.578  0.6824    \n# Erro b         18  10.510  5                    \n# Total          35 194.488  1                    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# ------------------------------------------------------------------------\n# CV 1 = 4.31183 %\n# CV 2 = 6.818585 %\n# \n# Interacao nao significativa: analisando os efeitos simples\n# ------------------------------------------------------------------------\n# HIBRIDO\n# Teste de Tukey\n# ------------------------------------------------------------------------\n# Grupos Tratamentos Medias\n# a      NUPEC_2     13.33417 \n#  b     NUPEC_3     10.97337 \n#   c    NUPEC_1     9.311875 \n# ------------------------------------------------------------------------\n# \n# FONTEN\n# Teste de Tukey\n# ------------------------------------------------------------------------\n# Grupos Tratamentos Medias\n# a      AmonioA     13.33304 \n#  b     Ureia   10.40515 \n#  b     NitratoA    9.881223 \n# ------------------------------------------------------------------------"},{"path":"reg.html","id":"reg","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","text":"","code":""},{"path":"reg.html","id":"regressÃ£o-linear","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1 RegressÃ£o Linear","text":"anÃ¡lise de regressÃ£o  tem como objetivo verificar como uma variÃ¡vel independente influencia resposta de uma variÃ¡vel dependente. anÃ¡lise de regressÃ£o Ã© amplamente utilizada em ciÃªncias agrÃ¡rias e pode ser dividida em simples ou mÃºltipla. Na regressÃ£o simples, apenas uma variÃ¡vel independente Ã© declarada modelo:\\[\r\nY_i = {\\beta _0} + {\\beta _1}x + \\varepsilon_i  \r\n\\]Onde \\(Y_i\\) Ã© variÃ¡vel dependente, \\(x\\) Ã© variÃ¡vel independente, \\(\\beta_0\\) Ã© o intercepto, \\(\\beta_1\\) Ã© inclinaÃ§Ã£o da reta e \\(\\varepsilon\\) Ã© o erro. Na regressÃ£o linear mÃºltipla, mais de uma variÃ¡vel independente Ã© declarada modelo:\\[\r\nY_i = {\\beta _0} + {\\beta _1}x_1 + {\\beta _2}x_2 + ... + {\\beta _k}x_k + \\varepsilon_i  \r\n\\]Onde \\(Y_i\\) Ã© variÃ¡vel dependente, \\(x_1\\), \\(x_2\\),â€¦,\\(x_k\\) sÃ£o variÃ¡veis independentes, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\),â€¦,\\(\\beta_2\\) sÃ£o os parÃ¢metros da regressÃ£o e \\(\\varepsilon\\) Ã© o erro. EntÃ£o, uma regressÃ£o por ser descrita genericamente por (Draper Smith 1998)\\[\r\nY_i = f(x)+\\varepsilon_i \r\n\\]Onde \\(Y_i\\) Ã© variÃ¡vel dependente, \\(f(x)\\) Ã© funÃ§Ã£o resposta modelo e \\(\\varepsilon\\) Ã© o erro.","code":""},{"path":"reg.html","id":"estimativa","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.1 Estimativa","text":"Uma das formas de estimar os parÃ¢metros em regressÃ£o Ã© minimizando os erros. Os erros sÃ£o diferenÃ§entre o valor estimado pela funÃ§Ã£o resposta \\(f(x)\\) e o valor observado (\\(Y_i\\)), representados grÃ¡fico abaixo por pontos vermelhos. EntÃ£o, devemos encontrar valores para \\({\\boldsymbol{\\beta}}\\) que minimizem estes erros, representados pela distÃ¢ncia entre reta estimada e os valores observados. \r\nFigure 11.1: GrÃ¡fico de dispersÃ£o de alguns dados com uma linha representando tendÃªncia geral. linhas verticais representam diferenÃ§(ou residuais) entre linha e os dados observados.\r\nNa figura acima, uma reta Ã© traÃ§ada de modo que distÃ¢ncias entre ela e os pontos seja mÃ­nimo. Essa distÃ¢ncia Ã© obtida, pelo mÃ©todo dos mÃ­nimos quadrados, minimizando soma de quadrados dos resÃ­duos  (uma vez que soma dos resÃ­duos Ã© igual zero).\\[\r\nS = {\\boldsymbol{\\varepsilon '\\varepsilon }} = \\sum\\limits_{= 1}^n {{{\\left( {{Y_i} - {{\\hat Y}_i}} \\right)}^2}} = 0\r\n\\]Para encontrar os valores dos parÃ¢metros que minimiza essa soma de quadrados basta resolver o sistema de equaÃ§Ãµes normais, obtida apÃ³s derivar \\(S\\) em relaÃ§Ã£o aos parÃ¢metros. resoluÃ§Ã£o deste sistema fornece estimativas nÃ£o viesadas dos parÃ¢metros \\({\\boldsymbol\\hat\\beta}\\).\\[\r\n\\begin{array}{c}{\\boldsymbol{X'X\\beta = X'Y}}\\\\{\\boldsymbol{\\hat\\beta}} = {\\left( {{\\boldsymbol{X'X}}}\\right)^{- 1}}{\\boldsymbol{X'Y}}\\end{array}\r\n\\]Percebe-se que estimativas dos parÃ¢metros nÃ£o tem nenhuma relaÃ§Ã£o com os pressupostos  de normalidade, homocedasticidade e independÃªncia dos resÃ­duios. PorÃ©m, cumprir pressupostos Ã© importante para testar hipÃ³teses e construir intervalos de confianÃ§. Outro aspecto importante na estimaÃ§Ã£o Ã© necessidade da matriz \\(\\boldsymbol{X'X}\\) ser nÃ£o singular, pois assim Ã© possÃ­vel inverter essa matriz e resolver o sistema de equaÃ§Ãµes normais  obtendo parÃ¢metros Ãºnicos. O sistema de equaÃ§Ãµes normais tambÃ©m pode ser resolvido utilizando inversa generalizada. Neste Ãºltimo caso, os valores dos parÃ¢metros que resolvem o sistema de equaÃ§Ãµes nÃ£o sÃ£o Ãºnicos. Para maiores detalhes sobre como estimar os parÃ¢metros de regressÃµes lineares, ver Draper Smith (1998), Kutner et al. (2005) e Rencher Schaalje (2008).Vimos que uma matriz \\(\\boldsymbol{X'X}\\) nÃ£o singular Ã© necessÃ¡ria para obtermos os parÃ¢metros da nossa regressÃ£o. nÃ£o singularidade da matriz estÃ¡ relacionada com quanto variÃ¡veis independentes estÃ£o correlacionadas. Quando variÃ¡veis independentes estÃ£o aproximadamente (ou perfeitamente, o que Ã© praticamente impossÃ­vel) relacionadas dizemos que hÃ¡ elevada multicolinearidade. O principal problema da multicolinearidade estÃ¡ relacionado com estimativas dos parÃ¢metros. Quando ela Ã© elevada, um conjunto de funÃ§Ãµes resposta minimiza os erros e prediz, com precisÃ£o, os valores observados.Quando hÃ¡ multicolinearidade Ã© possÃ­vel resolver o sistema de equaÃ§Ãµes normais  utilizando inversa generaliazada de Moore-Penrose, utilizando funÃ§Ã£o ginv()  pacote MASS. Utilizando inversa generalizada minimiza-se soma dos quadrados, porÃ©m nÃ£o garante-se que os parÃ¢metros sejam Ãºnicos. EntÃ£o, qualquer inferÃªncias sobre como variÃ¡veis se relacionam passa ser duvidosa (Kutner et al. 2005).Para demonstrar como multicolinearidade afeta estimativa dos parÃ¢metros, utilizamos um exemplo hipotÃ©tico de Kutner et al. (2005). Execute programaÃ§Ã£o em casa e veja os resultados.Quando hÃ¡ multicolinearidade, conjuntos de diferentes parÃ¢metros resolvem o sitema de equaÃ§Ãµes normais e minimizam soma de quadrados. Por isso, relacionar resposta da variÃ¡vel dependente em funÃ§Ã£o das variÃ¡veis independentes passa ser impossÃ­vel. Ã‰ por isso tambÃ©m que multicolinearidade Ã© importante na anÃ¡lise de trilha. relaÃ§Ã£o entre variÃ¡veis na anÃ¡lise de trilha Ã© determinada com base valor dos coeficientes de trilha, que nada mais sÃ£o que parÃ¢metros de uma regressÃ£o mÃºltipla estimados com variÃ¡veis padronizadas.","code":"\ndata_error <- data.frame(x = seq(0, 20, 2),\n                         y = c(3,9,4,10,12,9,14,16,18,16,14))\nmod <- lm(y ~ x, data = data_error)\nggplot(data_error, aes(x, y)) + \n       geom_segment(aes(x = x, y = y, xend = x, yend = fitted(mod))) +\n       geom_point(color = \"red\") + \n       geom_smooth(se = FALSE, method = \"lm\")\nX1 <- c(2,8,6,10)\nX2 <- c(6,9,8,10)\ncor(X1, X2) # correlaÃ§Ã£o entre as variÃ¡vies independentes\n\n## Minimizando a soma de quadrados\nrequire (nls2)\nresultados <- data.frame(matrix(ncol = 4,nrow = 20))\nnames(resultados) <- c(\"b0\",\"b1\",\"b2\",\"sigma\")\n\nfor(i in 1:20){\nY <- c(23,83,63,103)\nX1 <- c(2,8,6,10)\nX2 <- c(6,9,8,10)\ngrid <- expand.grid(list(\nb0 <- seq(-i,i, by = 0.1),\nb1 <- seq(-i, i, by = 0.1),\nb2 <- seq(-i, i, by = 0.1)\n)) # Armazenando um conjunto de valores que quero dar aos parÃ¢metros\n\nResp <- nls2(Y~b0+b1*X1+b2*X2,\nstart <- grid,\nalgorithm <- \"brute-force\")\n\nb0 <- summary(Resp)$coefficients[1,1]\nb1 <- summary(Resp)$coefficients[2,1]\nb2 <- summary(Resp)$coefficients[3,1]\nsigma <- summary(Resp)$sigma\n;\nresultados$b0[i] <- b0\nresultados$b1[i] <- b1\nresultados$b2[i] <- b2\nresultados$sigma[i] <- sigma\n}\nresultados"},{"path":"reg.html","id":"ajustando-regressÃµes-com-a-funÃ§Ã£o-lm","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.2 Ajustando regressÃµes com a funÃ§Ã£o lm()","text":"funÃ§Ã£o lm()  Ã© utilizada para ajustar regresÃµes lineares simples e mÃºltipla. Os argumentos mais importantes desta funÃ§Ã£o sÃ£o formula, onde indicamos funÃ§Ã£o resposta; e data, onde indicamos o banco de dados. Vamos utiliza um exemplo simples retirado de Schenider, Schenider, Souza (2009):AtravÃ©s da funÃ§Ã£o anova(mod7.1, mod7)  pode-se verificar se o modelo Ã© ou nÃ£o significativo. hipÃ³tese \\(H_0 = 0\\) Ã© rejeitada e conclui-se que o modelo explica o comportamento da variÃ¡vel resposta. AtravÃ©s da funÃ§Ã£o summary()  obtÃ©m-se o resultado teste t para os parÃ¢metros modelo. hipÃ³tese testada neste caso Ã© \\(H_0:\\boldsymbol{\\beta} = 0\\) vs \\(H_A:\\boldsymbol{\\beta}\\ne 0\\). Por fim, funÃ§Ã£o anova() retorna um teste F que possibilita verificar contribuiÃ§Ã£o de cada parÃ¢metro em explicar variabilidade da variÃ¡vel resposta.Entre os parÃ¢metros modelo, apenas \\(\\hat{\\beta_3}\\) nÃ£o foi significativo, indicando que nÃ£o hÃ¡ necessidade dele ser incluido modelo. AtravÃ©s teste F Ã© possÃ­vel verificar qual o modelo (com ou sem \\(\\beta_3\\)) Ã© o mais parcimonioso. O teste F Ã© dado por:\\[\r\nF_{calc} = \\frac{S{Q_{Erro}}(\\Omega)- S{Q_{Erro}}(\\omega)/G{L_{Erro}}(\\Omega)- G{L_{Erro}}(\\omega)} {Q{M_{Erro}}(\\omega )}\\\r\n\\]Onde, \\(SQ_{Erro}(\\Omega)\\) e \\(SQ_{Erro}(\\omega)\\) sÃ£o somas de quadrados dos resÃ­duos  nos modelos completo e reduzido, respectivamente; \\(G{L_{Erro}}(\\omega)\\) e \\(G{L_{Erro}}(\\Omega)\\) sÃ£o os graus de liberdade resÃ­duo modelo completo e reduzido, respectivamente; e \\(Q{M_{Erro}}(\\omega )\\) Ã© o quadrado mÃ©dio resÃ­duo modelo completo. Podemos realizar o teste F utilizando funÃ§Ã£o anova():Como ambos modelos sÃ£o estatisticamente iguais, opta-se pelo modelo reduzido. O modelo que melhor se ajustou aos dados foi \\(Y = 6.726 + 2.759X_1 - 1.937X_2\\), \\(R^2_{Aj} = 0.93\\) superior 90%.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nreg <- import(url, sheet = \"REG\")\nmod7 <- lm(Y ~ X1 + X2 + X3, data = reg)\nmod7.1 <- lm(Y ~ 1, data = reg)\nanova(mod7.1, mod7) # Verificar a significÃ¢ncia do modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     12 1834.00                                  \n# 2      9   97.47  3    1736.5 53.449 4.653e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(mod7)\n# \n# Call:\n# lm(formula = Y ~ X1 + X2 + X3, data = reg)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -5.4062 -2.2378 -0.3066  1.6321  4.8468 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  7.86871    3.32628   2.366   0.0422 *  \n# X1           2.72881    0.25198  10.830 1.84e-06 ***\n# X2          -1.92182    0.25540  -7.525 3.60e-05 ***\n# X3          -0.09752    0.15686  -0.622   0.5496    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.291 on 9 degrees of freedom\n# Multiple R-squared:  0.9469,  Adjusted R-squared:  0.9291 \n# F-statistic: 53.45 on 3 and 9 DF,  p-value: 4.653e-06\nmod8 <- lm(Y ~ X1 + X2, data = reg)\nanova(mod8, mod7)\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\ncoefficients(mod8)\n# (Intercept)          X1          X2 \n#    6.726606    2.759633   -1.937615"},{"path":"reg.html","id":"seleÃ§Ã£o-de-variÃ¡veis","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.3 SeleÃ§Ã£o de variÃ¡veis","text":"Foi mostrado brevemente um exemplo de como ajustar e selecionar variÃ¡veis. PorÃ©m, exemplo apresentado, utilizou-se somente trÃªs variÃ¡veis. entanto, quando hÃ¡ um elevado nÃºmero de variÃ¡veis, selecionÃ¡-las torna-se um trabalho um pouco mais complexo. Nestes casos Ã© necessÃ¡rio utilizar algoritimos de seleÃ§Ã£o. Os mais comuns sÃ£o o Forward, Backward e Stepwise.Forward mÃ©todo forward parte-se de um modelo com uma variÃ¡vel independente, que Ã© aquela que possui maior correlaÃ§Ã£o  amostral com variÃ¡vel dependente. Posteriormente, realiza-se o teste F para verificar se ela Ã© realmente Ã© significativa. segunda variÃ¡vel independente com maior correlaÃ§Ã£o amostral com variÃ¡vel dependente Ã© adicionada ao modelo, e um teste F parcial verifica significÃ¢ncia. variÃ¡vies sÃ£o adicionadas enquanto o F parcial significativo.Backward mÃ©todo backward parte-se modelo completo. variÃ¡vies candidatas serem eliminadas sÃ£o determinadas atravÃ©s teste F parcial, como se elas fossem (hipoteticamente) Ãºltimas serem incluidas modelo.Stepwise O mÃ©todo stepwise utiliza caracterÃ­sticas dos mÃ©todos forward e backward. Neste mÃ©todo parte-se de um modelo composto pela variÃ¡vel com maior correlaÃ§Ã£o  amostral. cada variÃ¡vel adicionada por forward, Ã© relizado um backward para retirar uma das variÃ¡veis previamente adicionadas.anÃ¡lises acima foram demonstradas apenas para detalhar o funcionamento dos algoritimos de seleÃ§Ã£o, utilizando F parcial. O F parcial Ã© muito rigoroso, e por isso muitas vezes o pesquisador usa valores de \\(\\alpha\\) maiores que 5%. AlÃ©m disso, vÃ¡rias funÃ§Ãµes R estÃ£o disponÃ­veis para selecionar variÃ¡veis utilizando diferentes diferentes critÃ©rios. funÃ§Ãµes ols_step_forward_p(), ols_step_backward_p() e ols_step_both_p() pacote olsrr35 selecionam variÃ¡vies utilizando utilizado forward, backward ou stepwise, respectivamente, considerando p-valores para critÃ©rio de decisÃ£o de inclusÃ£o/remoÃ§Ã£o de variÃ¡veis., \r\n","code":"\ncor(reg[1], reg[2:4]) # CorrelaÃ§Ã£o\n#          X1         X2         X3\n# Y 0.7754301 -0.4558018 -0.2460537\nmod_for1 <- lm(Y ~ 1, data = reg)\nmod_for2 <- lm(Y ~ X1, data = reg) # Adiciona X1 \nmod_for3 <- lm(Y ~ X1 + X2, data = reg)  # Adiciona X2 \nmod_for4 <- lm(Y ~ X1 + X2 + X3, data = reg)  # Adiciona X3 \nanova(mod_for1, mod_for2, mod_for3, mod_for4)  # Seleciona o modelo \n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1\n# Model 3: Y ~ X1 + X2\n# Model 4: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n# 1     12 1834.00                                    \n# 2     11  731.23  1   1102.77 101.8264 3.318e-06 ***\n# 3     10  101.66  1    629.58  58.1331 3.243e-05 ***\n# 4      9   97.47  1      4.19   0.3865    0.5496    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# F parcial para X3\nmod_back.x3 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x3.1 <- lm(Y~X1+X2,data = reg)\nanova(mod_back.x3.1,mod_back.x3) # Menor F parcial\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\n\n# F parcial para X2\nmod_back.x2 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x2.1 <- lm(Y~X1+X3,data = reg)\nanova(mod_back.x2.1,mod_back.x2)\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X3\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     10 710.70                                  \n# 2      9  97.47  1    613.23 56.624 3.598e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# F parcial para X1\nmod_back.x1 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x1.1 <- lm(Y~X2+X3,data = reg)\nanova(mod_back.x1.1,mod_back.x1) # Maior F parcial \n# Analysis of Variance Table\n# \n# Model 1: Y ~ X2 + X3\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     10 1367.60                                  \n# 2      9   97.47  1    1270.1 117.28 1.836e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Elimina a 3, depois a 2 e depois a 1\nmod_back1 <- lm(Y~X1+X2+X3,data = reg)\nmod_back2 <- lm(Y~X1+X2,data = reg)\nanova(mod_back2,mod_back1) # elimina X3\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\n\nmod_back2 <- lm(Y~X1+X2,data = reg)\nmod_back3 <- lm(Y~X1,data = reg)\nanova(mod_back3,mod_back2) # nÃ£o elimina X2 e seleciona\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1\n# Model 2: Y ~ X1 + X2\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 731.23                                  \n# 2     10 101.66  1    629.58 61.933 1.359e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Passo 1\nmod_step1.0 <- lm(Y ~ 1, data = reg)\nmod_step1 <- lm(Y ~ X1, data = reg)\nanova(mod_step1.0, mod_step1) # adiciona X1\n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1\n#   Res.Df     RSS Df Sum of Sq      F   Pr(>F)   \n# 1     12 1834.00                                \n# 2     11  731.23  1    1102.8 16.589 0.001842 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Passo 2\nmod_step2 <- lm(Y ~ X1, data = reg)\nmod_step2.1 <- lm(Y ~ X1 + X2, data = reg)\nanova(mod_step2, mod_step2.1) # adiciona X2\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1\n# Model 2: Y ~ X1 + X2\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 731.23                                  \n# 2     10 101.66  1    629.58 61.933 1.359e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmod_step2.2 <- lm(Y ~ X2, data = reg)\nanova(mod_step2.2, mod_step2.1) # mantÃ©m X1 no modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X2\n# Model 2: Y ~ X1 + X2\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 1452.98                                  \n# 2     10  101.66  1    1351.3 132.93 4.251e-07 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Passo 3\nmod_step3 <- lm(Y ~ X1 + X2, data = reg)\nmod_step3.1 <- lm(Y ~ X1 + X2 + X3,data = reg)\nanova(mod_step3, mod_step3.1) # nÃ£o adiciona X3, seleciona o modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\nolsrr::ols_step_forward_p(mod_for4)\nolsrr::ols_step_backward_p(mod_for4)\nolsrr::ols_step_both_p(mod_for4)"},{"path":"reg.html","id":"falta-de-ajuste","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.4 Falta de ajuste","text":"Quando vÃ¡rias observaÃ§Ãµes sÃ£o realizadas para cada variÃ¡vel independente (experimentos com repetiÃ§Ã£o, por exemplo), Ã© necessÃ¡rio verificar falta de ajuste. Nestes casos, o erro Ã© dividido em duas partes: ) o erro puro,  que consiste na diferenÃ§entre mÃ©dia e observaÃ§Ãµes em cada variÃ¡vel; b) falta de ajuste,  que Ã© diferenÃ§entre mÃ©dia da variÃ¡vel independente e o valor ajustado pela regressÃ£o.\\[\r\n{Y_{ij}} - {\\hat Y_i} = \\left( {{Y_{ij}} - {{\\bar Y}{.}}} \\right) + \\left( {{{\\bar Y}{.}} - {{\\hat Y}_i}} \\right)\r\n\\]Onde \\(\\hat{Y_{ij}}-Y_{ij}\\) Ã© o erro modelo, \\((Y_{ij}-\\bar{Y_j})\\) Ã© o erro puro e \\((\\hat{Y_{ij}}-\\bar{Y_j})\\) Ã© falta de ajuste.significÃ¢ncia teste F indica que o modelo linear nÃ£o Ã© adequado para representar relaÃ§Ã£o entre variÃ¡vies dependentes e independentes. Isso indica que o modelo ajustado â€œnÃ£o se aproximaâ€ satisfatoriamente da mÃ©dia das variÃ¡vies independentes, e que falta de ajuste  Ã© elevada quando comparado ao erro puro. Agora, vamos ajustar um modelo quadrÃ¡tico:nÃ£o significÃ¢ncia teste F indica que o modelo quadrÃ¡tico Ã© adequado para representar relaÃ§Ã£o entre variÃ¡vies dependentes e independentes. Aqui o exemplo Ã© apresentado para um ajuste de polinÃ´mios, mas sua aplicaÃ§Ã£o se estende qualquer regressÃ£o (linear simples, mÃºltipla ou regressÃ£o nÃ£o linear).","code":"\nmod10 <- lm(RG ~ DOSEN, data = quantitativo) # RegressÃ£o linear\nmod10.1 <- lm(RG ~  factor(DOSEN), data = quantitativo) #falta de ajuste\nanova(mod10, mod10.1) # Erro puro\n# Analysis of Variance Table\n# \n# Model 1: RG ~ DOSEN\n# Model 2: RG ~ factor(DOSEN)\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     18 4.5135                                  \n# 2     15 0.6046  3    3.9089 32.325 8.625e-07 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmod11 <- lm(RG ~  DOSEN +I(DOSEN^2), data = quantitativo) #RegressÃ£o quadrÃ¡tica\nanova(mod11, mod10.1)\n# Analysis of Variance Table\n# \n# Model 1: RG ~ DOSEN + I(DOSEN^2)\n# Model 2: RG ~ factor(DOSEN)\n#   Res.Df     RSS Df Sum of Sq     F Pr(>F)\n# 1     17 0.70710                          \n# 2     15 0.60463  2   0.10247 1.271 0.3091"},{"path":"reg.html","id":"anÃ¡lise-dos-resÃ­duos","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.5 AnÃ¡lise dos resÃ­duos","text":"Na anÃ¡lise de regressÃ£o, os resÃ­duos  devem ser normalmente distribuÃ­dos, homocedÃ¡sticos e independentes. O diagnÃ³stico Ã© realizado por testes estatÃ­sticos ou atravÃ©s de anÃ¡lise grÃ¡fica.anÃ¡lise dos resÃ­duos  tambÃ©m pode indicar necessidade de adicionar variÃ¡vies explicativas ao modelo. Por exemplo, vamos analisar os resÃ­duos de um modelo linear ajustados dados que tem (conhecidamente) comportamento quadrÃ¡tico.Percebe-se, pelo grÃ¡fico residuals vs fitted, que os resÃ­duos nÃ£o sÃ£o aleatoriamente distribuÃ­dos em torno de zero. distribuiÃ§Ã£o sistemÃ¡tica dos resÃ­duos indica que uma variÃ¡vel que explica consideravelmente variabilidade dos dados nÃ£o foi incluÃ­da modelo (caso o termo quadrÃ¡tico polinÃ´mio).","code":"\n\nresiduos <- residuals(mod11)\nshapiro.test(residuos) # Normalidade\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuos\n# W = 0.95282, p-value = 0.412\nbartlett.test(residuos ~ DOSEN, data = quantitativo) # Homocedasticidade\n# \n#   Bartlett test of homogeneity of variances\n# \n# data:  residuos by DOSEN\n# Bartlett's K-squared = 4.3887, df = 4, p-value = 0.3559\nautoplot(mod11)# anÃ¡lise grÃ¡fica dos resÃ­duos\nresiduos <- residuals(mod10)\nautoplot(mod10) # anÃ¡lise grÃ¡fica dos resÃ­duos"},{"path":"reg.html","id":"pontos-influentes","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.1.6 Pontos influentes","text":"observaÃ§Ãµes influentes podem ser mensuradas atravÃ©s da distÃ¢ncia de Cook, DFBETA e DFFITS. O DFFITS e distÃ¢ncia de Cook medem influÃªncia das observaÃ§Ãµes sobre prediÃ§Ã£o das variÃ¡vies; e o DFBETA mede influÃªncia destas observaÃ§Ãµes sobre estimativas dos parÃ¢metros.Vamos utilizar funÃ§Ãµes grÃ¡ficas pacote olsrr para fazer o diagnÃ³stico dos pontos infuentes.\r\nFigure 11.2: DistÃ¢ncia de Cook representando influencia dos pontos na prediÃ§Ã£o das variÃ¡vies\r\nobservaÃ§Ãµes 8 e 12 sÃ£o que mais influenciam os valores preditos e estimativas dos parÃ¢metros. Os limites pela distÃ¢ncia de Cook, DFBETA e DFFITS para realizar o diagnÃ³stico sÃ£o \\(\\frac{4}{n} = \\frac{4}{20} = 0,20\\), \\(\\frac{2}{\\sqrt{n}} = \\frac{2}{\\sqrt{2}} = 0,45\\) e \\(2 \\times \\sqrt {\\frac{p}{n}} = 2 \\times \\sqrt {\\frac{3}{{20}}} = 0,77\\), respectivamente.","code":"\nolsrr::ols_plot_cooksd_bar(mod11) # DistÃ¢ncia de Cook\nolsrr::ols_plot_dfbetas(mod11) # DFBetas\nolsrr::ols_plot_dffits(mod11) # DFFits"},{"path":"reg.html","id":"regressÃ£o-nÃ£o-linear","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2 RegressÃ£o nÃ£o linear","text":"Uma regressÃ£o Ã© dita nÃ£o linear quando os parÃ¢metros nÃ£o encontram-se de forma aditiva modelo. Devido isso, o sistema de equaÃ§Ãµes normais \\({\\left( {{\\boldsymbol{X'X}}} \\right)^{{\\boldsymbol{ - 1}}}}{\\boldsymbol{\\beta = X'Y}}\\) nÃ£o pode ser resolvido analiticamente, e os parÃ¢metros precisam ser estimados utilizando mÃ©todos iterativos. ","code":""},{"path":"reg.html","id":"estimativa-1","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.1 Estimativa","text":"estimaÃ§Ã£o dos parÃ¢metros Ã© realizado pelo mÃ©todo dos mÃ­nimos quadrados . O mÃ©todo iterativo utilizado nos softwares R e SAS, por exemplo, Ã© o de Gauss-Newton. Este mÃ©todo utiliza aproximaÃ§Ãµes lineares de Taylor de primeira ordem da funÃ§Ã£o resposta, dada por\\[\r\nf\\left( {x,{\\boldsymbol{\\theta }}} \\right) = f\\left( {x,{{\\boldsymbol{\\theta }}^0}} \\right) + \\frac{{\\partial f\\left( {x,{{\\boldsymbol{\\theta }}^0}} \\right)}}{{\\partial {\\boldsymbol{\\theta }}}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)   \r\n\\]Essa aproximaÃ§Ã£o linear de primeira ordem pode ser simplificada por \\(f\\left( {\\boldsymbol{\\theta }} \\right) = f\\left( {{{\\boldsymbol{\\theta }}^0}} \\right) + {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)\\). Substituindo-na funÃ§Ã£o que minimiza soma de quadrados, temos:\\[\r\n\\begin{array}{c}S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {y - f\\left( {{\\boldsymbol{\\hat \\theta }}} \\right)} \\right)}^2}} \\\\S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {y - f\\left( {{{\\boldsymbol{\\theta }}^0}} \\right) - {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)} \\right)}^2}} \\\\S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {\\varepsilon  - {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)} \\right)}^2}} \\end{array}\r\n\\]Percebe-se que matriz \\(\\boldsymbol{F}\\), que substitui \\(\\boldsymbol{X}\\), Ã© sempre dependente de um dos parÃ¢metros modelo, e por isso o sistema de equaÃ§Ãµes nÃ£o tem resoluÃ§Ã£o analÃ­tica. O sistema de equaÃ§Ãµes nÃ£o linear acima Ã© resolvido por \\({\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^{\\boldsymbol{0}}} = {\\left( {{\\boldsymbol{F}}'{\\boldsymbol{F}}} \\right)^{ - 1}}{\\boldsymbol{F}}'{\\boldsymbol{\\varepsilon }}\\), que reorganizada como \\({\\boldsymbol{\\theta }} = {{\\boldsymbol{\\theta }}^{\\boldsymbol{0}}} + {\\left( {{\\boldsymbol{F}}'{\\boldsymbol{F}}} \\right)^{ - 1}}{\\boldsymbol{F}}'{\\boldsymbol{\\varepsilon }}\\), corresponde ao primeiro passo mÃ©todo iterativo de Gauss-Newton. Para algoritimo seja iniciado, um valor inicial para os parÃ¢metros deve ser declarado. O processo Ã© repetido atÃ© obter convergÃªncia, que ocorre quando os valores estimados em cada passo sÃ£o prÃ³ximos um dos outros.","code":""},{"path":"reg.html","id":"ajustando-o-modelo-com-a-funÃ§Ã£o-nls","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.2 Ajustando o modelo com a funÃ§Ã£o nls()","text":"funÃ§Ã£o nls()  pode ser utilizada para ajustar modelos nÃ£o lineares. Os principais argumentos da funÃ§Ã£o sÃ£o: ) formula, onde o modelo Ã© declarado; ii) data, onde os dados sÃ£o declarados e iii) start , que Ã© uma lista com os valores iniciais dos parÃ¢metros.Valores iniciais dos parÃ¢metrosO primeiro passo da anÃ¡lise Ã© encontrar os valores iniciais dos parÃ¢metros. O mÃ©todo grÃ¡fico Ã© Ãºtil para cumprir esse objetivo. Valores dos parÃ¢metros sÃ£o declarados atÃ© o ponto em que curva gerada se aproxime dos valores observados. programaÃ§Ã£o que serÃ¡ apresentada foi obtida blog RidÃ­culas, mantido pelo LEG da UFPR. Utilizaremos como exemplo o modelo logÃ­stico (uma de suas parametrizaÃ§Ãµes), dado por\\[\r\n{Y_i} = \\frac{{{\\beta _1}}}{{1 + {e^{\\left( {{\\beta _2} - {\\beta _3}{t_i}} \\right)}}}} + {\\varepsilon _i}\r\n\\]Manipulando os valores dos parÃ¢metrosAjustando o modeloOs valores iniciais serÃ£o armazenados na lista start, e esta serÃ¡ declarada argumento start da funÃ§Ã£o nls()  . funÃ§Ã£o summary()  retorna o teste de Wald para os parÃ¢metros.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nnls_tomato <- import(url, sheet = \"TOMATE\")\nnls_tomato_cord <- subset(nls_tomato,Genotipo  ==  \"Cordillera\") \n# Modelo logÃ­stico\nlogi <- function(x, b1, b2, b3){\n  b1 / (1 + exp(b2 - b3 * x))\n}\nstart=list()\nmanipulate({\n  plot(num~DAT,data = nls_tomato_cord)\n  curve(logi(x, b1=b1,b2=b2,b3=b3),add=TRUE)\n  start<<-list(b1=b1,b2=b2,b3=b3)},\n  b1=slider(0,50,initial=10),\n  b2=slider(0, 20,initial=5),\n  b3=slider(0, 1,initial=0)\n)\nnls1 <- nls(num~b1/(1+exp(b2-b3*DAT)), \n         data = nls_tomato_cord, # indica os dados\n         start = start) # indica os valores iniciais\nsummary(nls1)\n# \n# Formula: num ~ b1/(1 + exp(b2 - b3 * DAT))\n# \n# Parameters:\n#    Estimate Std. Error t value Pr(>|t|)    \n# b1 39.68149    1.50505   26.37 1.96e-07 ***\n# b2 13.53980    0.94913   14.27 7.42e-06 ***\n# b3  0.13390    0.01018   13.15 1.19e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.9108 on 6 degrees of freedom\n# \n# Number of iterations to convergence: 7 \n# Achieved convergence tolerance: 8.502e-06"},{"path":"reg.html","id":"anÃ¡lise-dos-resÃ­duos-1","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.3 AnÃ¡lise dos resÃ­duos","text":"Os resÃ­duos  dos modelos nÃ£o lineares tambÃ©m deve ser normalmente distribuÃ­dos, homocedÃ¡sticos e independentes. Os testes estatÃ­sticos e anÃ¡lise dos resÃ­duos seguem os mesmos princÃ­pios dos modelos lineares.O cumprimento dos pressupostos  dos resÃ­duos nÃ£o afeta estimativa dos parÃ¢metros, mas Ã© de extrema importÃ¢ncia para construir intervalos de confianÃ§e testar hipÃ³teses. Percebe-se, nosso exemplo, que os pressuposto foram cumpridos (p-valor>0,05). Para realizar anÃ¡lise grÃ¡fica dos resÃ­duos pode-se utilizar funÃ§Ã£o nlsResiduals()  pacote nlstools.exemplo acima, apenas uma observaÃ§Ã£o foi realizada para cada variÃ¡vel independente (dias apÃ³s o transplante, caso). Por isso optou-se por utilizar o teste de Breusch-Pagan para realizar o diagnÃ³stico de homocedasticidade dos resÃ­duos. Quando mais de uma observaÃ§Ã£o Ã© realizada em cada variÃ¡vel independente, pode-se utilizar os testes de Bartlett ou Levene.","code":"\nrequire(lmtest) # pacote para carregar teste de Breusch-Pagan (homogeneidade)\nrequire (car) # pacote para carregar teste de DW (independÃªncia)\n### Normalidade\nres_nls1 <- residuals(nls1)\nshapiro.test(res_nls1)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  res_nls1\n# W = 0.92624, p-value = 0.4464\nnls1_grad <- attr(nls1$m$fitted(), \"gradient\") # obtem matriz gradiente\nnls1_lm <- lm(num~-1+nls1_grad, data = nls_tomato_cord) \nbptest(nls1_lm) # teste de Breusch-Pagan (homogeneidade)\n# \n#   studentized Breusch-Pagan test\n# \n# data:  nls1_lm\n# BP = 1.5734, df = 2, p-value = 0.4554\ndurbinWatsonTest(nls1_lm) # teste de DW (independÃªncia)\n#  lag Autocorrelation D-W Statistic p-value\n#    1      0.08163678      1.763777   0.248\n#  Alternative hypothesis: rho != 0\nrequire(nlstools)\nres1_nls1 = nlsResiduals(nls1)\nplot(res1_nls1)\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nnls_eggplant <- import(url, sheet = \"EGGPLANT\")\nnls_eggplant <- subset(nls_eggplant, ESTUFA  ==  \"E1\") \nstart <- list(b1 = 10,b2 = 6.7,b3 = 0.073)\nnls2 <- nls(NUMERO ~ b1/(1+exp(b2-b3*DAT)), \n            data = nls_eggplant,\n            start = start)"},{"path":"reg.html","id":"medidas-de-nÃ£o-linearidade","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.4 Medidas de nÃ£o linearidade","text":"Conforme vimos acima, estimaÃ§Ã£o dos parÃ¢metros Ã© realizada pelo mÃ©todo dos mÃ­nimos quadrados  utilizando aproximaÃ§Ãµes lineares de Taylor da funÃ§Ã£o resposta. Ã‰ esta aproximaÃ§Ã£o linear que garante que os parÃ¢metros estimados sejam proximos de nÃ£o viesados (sÃ³ serÃ£o nÃ£o viesados assintoticamente). Modelos com aproximaÃ§Ã£o linear pobre tendem ter parÃ¢metros muito viesados, o que impede que eles sejam utilizados para explicar determinado fenomeno biolÃ³gico (eles tem ineterpretaÃ§Ã£o biolÃ³gica). medidas de curvatura de Bates e Watts sÃ£o amplamente utilizadas para avaliar o grau de nÃ£o linearidade da funÃ§Ã£o resposta. Para maiores detalhes, ver Bates Watts (1988), cap. 7 e Seber Wild (2003), cap. 4. Essas medidas sÃ£o facilmente implementadas atravÃ©s da funÃ§Ã£o rms.curv()  pacote MASS. Para isto, utilizaremos o modelo ajustado nls1Os valores que funÃ§Ã£o rms.curv() retorna sÃ£o \\({c^\\theta } \\times \\sqrt {{F_{\\alpha ;p,n - p}}}\\) e \\({c^\\iota } \\times \\sqrt {{F_{\\alpha ;p,n - p}}}\\). Valores baixos destas duas medidas indicam que funÃ§Ã£o tem boa aproximaÃ§Ã£o linear e, consequentemente, os parÃ¢metros sÃ£o prÃ³ximos de ser nÃ£o viesados.","code":"\n\nstart <- list(b1 = 30, b2 = 19, b3 = 0.2)\n## Medidas de nÃ£o linearidade\nnls1_hess <- deriv3(~b1/(1 + exp(b2 - b3 * DAT)), c(\"b1\",\"b2\",\"b3\"),\n                   function(DAT,b1,b2,b3)NULL) ## hessiana\nnls1_hess.1 <- nls(num ~ nls1_hess(DAT,b1,b2,b3),\n                   data = nls_tomato_cord,\n                   start = start)\nrms.curv(nls1_hess.1)\n# Parameter effects: c^theta x sqrt(F) = 0.806 \n#         Intrinsic: c^iota  x sqrt(F) = 0.1235"},{"path":"reg.html","id":"comparaÃ§Ã£o-de-parÃ¢metros","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.5 ComparaÃ§Ã£o de parÃ¢metros","text":"Os parÃ¢metros em um modelo podem ser comparados utilizando variÃ¡vies dummy. Utilizando esta tÃ©cnica, ocorrÃªncia de um determinado fator Ã© associado um nova variÃ¡vel. Como os modelos sÃ£o aninhados, pode-se utilizar o teste F para verificar significÃ¢ncia parÃ¢metro (e, consequentemente, fator) associado esta variÃ¡vel.Vamos ao exemplo. Suponha que queiramos comparar produÃ§Ã£o e taxa de producÃ£o de frutos de dois genÃ³tipos de tomate. Sabemos que quando acumuladas, poduÃ§Ã£o de olerÃ­colas tem comportamento sigmoide (LÃºcio, Nunes, Rego 2015; Lucio, Nunes, Rego 2016), o que permite deteminar essas caracteristicas atravÃ©s dos parÃ¢metros de um modelo logÃ­stico:\\[\r\n{Y_i} = \\frac{{{\\beta _1}}}{{1 + {e^{\\left( {{\\beta _2} - {\\beta _3}{t_i}} \\right)}}}} + {\\varepsilon _i}\r\n\\]modelo acima, \\(\\beta_1\\) representa assÃ­ntota, e estÃ¡ associada produÃ§Ã£o total dos genÃ³tipos; \\(\\beta_3\\) Ã© taxa de produÃ§Ã£o de frutos, e estÃ¡ associada precocidade produtiva dos genÃ³tipos (Sari 2018). Vamos testar seguintes hipÃ³teses:\\[\r\n    \\begin{array}{*{20}{c}}{{H_0} = {\\beta _{11}} = {\\beta _{12}}}&{{H_0} = {\\beta _{31}} = {\\beta _{32}}}\\\\{{H_A} = {\\beta _{11}} \\ne {\\beta _{12}}}&{{H_A} = {\\beta _{31}} \\ne {\\beta _{32}}}\\end{array}\r\n\\]Testando hipÃ³tese \\(H_0:\\beta_{11} = \\beta_{12}\\)Associamos os fatores novas variÃ¡vies atravÃ©s de uma nova coluna banco de dados. nosso caso, coluna â€œCompletoâ€ associa o fator aos parÃ¢metros modelo logÃ­stico. EntÃ£o, como o modelo logÃ­stico possui 3 parÃ¢metros, ao associarmos variÃ¡vies dummys ao fator genÃ³tipo (sÃ£o dois genÃ³tipos), o modelo completo passa ter 6 parÃ¢metros.Para tertar esta hipÃ³tese, associamos variÃ¡veis dummy s apenas aos parÃ¢metros \\(\\beta_1\\) e \\(\\beta_3\\). Neste caso, o modelo passa ter 5 parÃ¢metros (1 \\(\\beta_1\\), 2 \\(\\beta_2\\) e 2 \\(\\beta_3\\)). Podemos comparar esse modelo reduzido com o modelo completo de 6 parÃ¢metros. Se o teste F der significativo, concluÃ­mos que hÃ¡ influÃªncia fator genÃ³tipo.Os valores da assintota diferem estatisticamente entre si. Percebe-se claramente que o genÃ³tipo Cordillera foi mais produtivo que o genÃ³tipo GaÃºcho.Testando hipÃ³tese \\(H_0:\\beta_{31} = \\beta_{32}\\)Os valores da taxa de produÃ§Ã£o de frutos nÃ£o diferem estatisticamente entre si.","code":"\nnls_dummy <- \n  nls_tomato %>%\n  to_factor(Genotipo, Completo, Reduzido)\n## Modelo completo\ntomato_completo <- \n  nls(num ~ b1[Completo] / (1 + exp(b2[Completo] - b3[Completo] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94, 39.68),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.17, 0.13)))\n## Modelo reduzido (beta 1)\ntomato_reduzido.b1 <- \n  nls(num ~ b1[Reduzido] / (1 + exp(b2[Completo] - b3[Completo] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.17, 0.13)))\nanova(tomato_reduzido.b1, tomato_completo)\n# Analysis of Variance Table\n# \n# Model 1: num ~ b1[Reduzido]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n# Model 2: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n#   Res.Df Res.Sum Sq Df Sum Sq F value    Pr(>F)    \n# 1     13     55.636                                \n# 2     12      7.833  1 47.802  73.228 1.875e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Modelo reduzido (beta 3)\ntomato_reduzido.b3 <- \n  nls(num ~ b1[Completo] / (1 + exp(b2[Completo] - b3[Reduzido] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94, 39.68),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.16)))\nanova(tomato_reduzido.b3, tomato_completo)\n# Analysis of Variance Table\n# \n# Model 1: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Reduzido] * DAT))\n# Model 2: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n#   Res.Df Res.Sum Sq Df Sum Sq F value  Pr(>F)  \n# 1     13    10.1479                            \n# 2     12     7.8335  1 2.3144  3.5454 0.08417 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"reg.html","id":"representaÃ§Ã£o-grÃ¡fica-dos-modelos","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.2.6 RepresentaÃ§Ã£o grÃ¡fica dos modelos","text":"Percebe-se claramente que um modelo comum aos dois genÃ³tipos nÃ£o Ã© possÃ­vel (assintota diferem entre si). Por isso, optou-se por gerar um modelo em separado para cada genÃ³tipo. Podemos representar isso graficamente utilizando funÃ§Ã£o ggplot()  .\r\nFigure 11.3: RepresentaÃ§Ã£o grÃ¡fica em modelos nÃ£o lineares\r\n","code":"\nformula <- as.formula(\"y ~ b1/(1 + exp(b2-b3*x))\")\nstart <- list(b1 = 10.224, b2 = 6.765, b3 = 0.0725)\n\nggplot(nls_tomato, aes(x = DAT, y = num, colour = Genotipo)) + \ngeom_point() +\ngeom_smooth(method = \"nls\", \n            method.args = list(formula = formula, \n                                start = start),\n                                se = F,\n                                data = subset(nls_tomato, Genotipo == \"Cordillera\")) +\ngeom_smooth(method = \"nls\", \n            method.args = list(formula = formula, \n                                start = start),\n                                se = F,\n                                data = subset(nls_tomato, Genotipo == \"Gaucho\"))+\ntheme(legend.position = \"bottom\")+\nlabs(x = \"Dias apÃ³s o transplante\", y = \"NÃºmero de frutos\")"},{"path":"reg.html","id":"regressÃ£o-bisegmentada-com-platÃ´","chapter":"CapÃ­tulo 11 AnÃ¡lise de regressÃ£o","heading":"11.3 RegressÃ£o bisegmentada com platÃ´","text":"Continuaremos tomando como exemplo produÃ§Ã£o de olerÃ­colas de mÃºltiplas colheitas para exemplificar o uso deste tipo de regressÃ£o. Os modelos com platÃ´ sÃ£o modelos bi-segmentados cujo primeiro segmento descreve o crescimento da produÃ§Ã£o atÃ© determinado ponto, e um segundo segmento que descreve estabilizaÃ§Ã£o da produÃ§Ã£o (platÃ´). Podemos representar um modelo linear-platÃ´ por:\\[\r\n{Y_i} = \\left\\{ \\begin{array}{l}{\\beta _0} + {\\beta _1}x + {\\varepsilon _i}{\\rm{, se }}{X_i}{\\rm{  <  }}{X_0}\\\\P{\\rm{ se }}{X_i}{\\rm{  >  }}{X_0}{\\rm{ }}\\end{array} \\right.\r\n\\]O genÃ³tipo Cordillera produz uma taxa de 1,33 frutos dia\\(^-1\\) atÃ© os ~9 dias apÃ³s o inÃ­cio das colheitas, quando comeÃ§estabilizar produÃ§Ã£o (17,19 frutos). JÃ¡ o genÃ³tipo Santa Clara produz uma taxa de 0,71 frutos \\(^-1\\) atÃ© os ~14 dias apÃ³s o inÃ­cio das colheitas, quando comeÃ§estabilizar produÃ§Ã£o (12,69 frutos). Podemos verificar taxa de produÃ§Ã£o e o ponto de estabilizaÃ§Ã£o atravÃ©s de variÃ¡veis dummy .Percebe-se que taxa de produÃ§Ã£o genÃ³tipo Cordillera Ã© significativamente superior taxa de produÃ§Ã£o genÃ³tipo Santa Clara. Como consequÃªncia, o momento de estabilizaÃ§Ã£o da produÃ§Ã£o ocorre mais precocemente genÃ³tipo Cordillera. O exemplo foi realizado com dados de produÃ§Ã£o de olerÃ­colas, mas pdoe ser adaptado para qualquer estudo (desde que tenha este comportamento).RepresentaÃ§Ã£o grÃ¡fica dos modelos\r\nFigure 11.4: RepresentaÃ§Ã£o grÃ¡fica de regressÃ£o bisegmentada com platÃ´\r\n","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nplato_tomato <- import(url, sheet = \"PLATO\")\nplato_cordillera <- \n  nls(num ~ (b0 + b1 * DAT * (DAT <= P)) + (b1 * P * (DAT > P)),\n      data = subset(plato_tomato, Genotipo == \"Cordillera\"),\n      start = list(b0 = 5, b1 = 11/8, P = 15))\nplato_SantaCl <- \n  nls(num ~ (b0 + b1 * DAT * (DAT <= P)) + (b1 * P * (DAT > P)),\n      data = subset(plato_tomato, Genotipo == \"Santa.Clara\"),\n      start = list(b0 = 5, b1 = 11/8, P = 15))\nplato_dummy <- \n  plato_tomato %>%\n  to_factor(Genotipo, Completo, Reduzido)\n# Modelo completo\nplato_completo <- \n  nls(num ~ (b0[Completo] + b1[Completo] * DAT * (DAT <= P[Completo]))+\n        (b1[Completo] * P[Completo] * (DAT > P[Completo])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8, 2.5), \n                   b1 = c(1.33, 0.71), \n                   P = c(9.28, 14.36)))\n\n# Modelo reduzido (taxa de produÃ§Ã£o)\nplato_reduzido.b1 <- \n  nls(num ~ (b0[Completo] + b1[Reduzido] * DAT * (DAT <= P[Completo]))+\n        (b1[Reduzido] * P[Completo] * (DAT > P[Completo])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8,2.5), \n                   b1 = c(1), \n                   P = c(9.28,14.36)))\nanova(plato_reduzido.b1, plato_completo)\n\n# Modelo reduzido (PlatÃ´)\nplato_reduzido.P <- \n  nls(num ~ (b0[Completo] + b1[Completo] * DAT * (DAT <= P[Reduzido])) + \n        (b1[Completo] * P[Reduzido] * (DAT > P[Reduzido])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8, 2.5), \n                   b1 = c(1.33, 0.71), \n                   P = c(12)))\nanova(plato_reduzido.P, plato_completo)\nplot(num ~ DAT,\n     data = subset(plato_tomato,Genotipo == \"Cordillera\"),ylim = c(0,20),\n     xlab = \"Dias apÃ³s o transplante (DAT)\",\n     ylab = \"NÃºmero de frutos por planta\",pch = 1,lwd = 2)\npoints(num~DAT,data = subset(plato_tomato,Genotipo == \"Santa.Clara\"),pch = 2,lwd = 2)\nsegments(y0 = 4.8385, x0 = 0, y1 = 17.2083, x1 = 9.28641,lty = 1,lwd = 2)\nsegments(y0 = 17.2083, x0 = 9.28641, y1 = 17.2083, x1 = 20,lty = 1,lwd = 2)\nsegments(y0 = 2.5000, x0 = 0, y1 = 12.7655, x1 = 14.3606,lty = 2,lwd = 2)\nsegments(y0 = 12.7655, x0 = 14.3606, y1 = 12.7655, x1 = 20,lty = 2,lwd = 2)\n\n\nlegend(\"bottomright\", legend = c(\"Cordillera\", \"Santa Clara\"), lty = c(1,2),\n       pch = c(1,2), lwd = c(2,2),bty = \"n\")"},{"path":"relations.html","id":"relations","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","text":"Conhecer o grau de associaÃ§Ã£o linear entre caracteres Ã© de fundamental importÃ¢ncia em um programa de melhoramento genÃ©tico vegetal. Esta importÃ¢ncia aumenta, principalmente se algum caractere desejÃ¡vel Ã© de difÃ­cil mensuraÃ§Ã£o, ou apresenta baixa herdabilidade. O coeficiente de correlaÃ§Ã£o  produto-momento de Pearson (1920), r, vem sendo amplamente utilizado para este fim. Embora o mÃ©rito desta anÃ¡lise seja atribuÃ­Ã  Karl Pearson, o mÃ©todo foi originalmente concebido por Francis Galton, que definiu o termo correlaÃ§Ã£o como como o seguinte: duas variÃ¡veis sÃ£o ditas correlacionadas quando variaÃ§Ã£o de uma Ã© acompanhada na mÃ©dia, mais ou menos variaÃ§Ã£o da outra, e mesmo sentido (Galton 1888).","code":""},{"path":"relations.html","id":"dados","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.1 Dados","text":"Nesta sessÃ£o, e na sessÃ£o de anÃ¡lise multivariada iremos utilizar o conjunto de dados datage_2 pacote metan. Para maiores informaÃ§Ãµes veja ?data_ge2","code":"\nmaize <- data_ge2\nnumeric_var <- maize %>% select_numeric_cols()\ndatacor <- maize %>% select_cols(CD, CL, CW, PH, EH, EP, EL, ED)"},{"path":"relations.html","id":"correlaÃ§Ã£o-linear","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2 CorrelaÃ§Ã£o linear","text":"estimativa r leva em consideraÃ§Ã£o covariÃ¢ncia entre duas variÃ¡veis, representadas aqui por XY dividia pelo produto dos respectivos desvios padrÃµes de X e de Y, conforme o seguinte modelo:\\[\r\n{\\rm{r  =  }}\\frac{{\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{\\rm{[ (}}{{\\rm{X}}_{\\rm{}}}{\\rm{ - \\bar X)(}}{{\\rm{Y}}_{\\rm{}}}{\\rm{ - \\bar Y)] }}} }}{{\\sqrt {\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{{\\rm{(}}{{\\rm{X}}_{\\rm{}}}{\\rm{ - \\bar X)}}}^{\\rm{2}}}} } \\sqrt {\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{{\\rm{(}}{{\\rm{Y}}_{\\rm{}}}{\\rm{ - \\bar Y)}}}^{\\rm{2}}}} } }}\r\n\\]onde \\({\\rm{\\bar X = }}\\frac{{\\rm{1}}}{{\\rm{n}}}\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{\\rm{X}}_{\\rm{}}}}\\) e \\({\\rm{\\bar Y = }}\\frac{{\\rm{1}}}{{\\rm{n}}}\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{\\rm{Y}}_{\\rm{}}}}\\).Esta sessÃ£o Ã© focada em apresentar funÃ§Ãµes bÃ¡sicas e avanÃ§adas para visualizaÃ§Ã£o grÃ¡fica de associaÃ§Ãµes e estimativas coeficiente de correlaÃ§Ã£o. Para este fim, utilizaremos o conjunto de dados datacor, criado anteriormente.","code":""},{"path":"relations.html","id":"visualizaÃ§Ã£o-grÃ¡fica","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.1 VisualizaÃ§Ã£o grÃ¡fica","text":"seguinte funÃ§Ã£o proporciona uma visualizaÃ§Ã£o grÃ¡fica de todos os pares de correlaÃ§Ã£o possÃ­veis (scatter-plot)\r\n","code":"\npairs(datacor)"},{"path":"relations.html","id":"estimativa-dos-coeficientes-de-correlaÃ§Ã£o","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.2 Estimativa dos coeficientes de correlaÃ§Ã£o","text":"","code":"\ncorr <- corr_coef(datacor)\nprint(corr)\n# ---------------------------------------------------------------------------\n# Pearson's correlation coefficient\n# ---------------------------------------------------------------------------\n#       CD    CL    CW    PH    EH    EP    EL    ED\n# CD 1.000 0.300 0.484 0.315 0.281 0.175 0.912 0.390\n# CL 0.300 1.000 0.738 0.325 0.397 0.391 0.255 0.697\n# CW 0.484 0.738 1.000 0.505 0.519 0.425 0.458 0.737\n# PH 0.315 0.325 0.505 1.000 0.932 0.638 0.380 0.661\n# EH 0.281 0.397 0.519 0.932 1.000 0.870 0.363 0.630\n# EP 0.175 0.391 0.425 0.638 0.870 1.000 0.263 0.458\n# EL 0.912 0.255 0.458 0.380 0.363 0.263 1.000 0.385\n# ED 0.390 0.697 0.737 0.661 0.630 0.458 0.385 1.000\n# ---------------------------------------------------------------------------\n# p-values for the correlation coefficients\n# ---------------------------------------------------------------------------\n#          CD       CL       CW       PH       EH       EP       EL       ED\n# CD 0.00e+00 1.39e-04 1.54e-10 6.06e-05 3.90e-04 2.88e-02 1.97e-61 4.94e-07\n# CL 1.39e-04 0.00e+00 3.92e-28 3.45e-05 2.84e-07 4.55e-07 1.29e-03 4.74e-24\n# CW 1.54e-10 3.92e-28 0.00e+00 1.83e-11 3.76e-12 3.25e-08 1.81e-09 5.31e-28\n# PH 6.06e-05 3.45e-05 1.83e-11 0.00e+00 1.11e-69 3.10e-19 9.80e-07 5.66e-21\n# EH 3.90e-04 2.84e-07 3.76e-12 1.11e-69 0.00e+00 4.80e-49 3.28e-06 1.19e-18\n# EP 2.88e-02 4.55e-07 3.25e-08 3.10e-19 4.80e-49 0.00e+00 8.92e-04 1.83e-09\n# EL 1.97e-61 1.29e-03 1.81e-09 9.80e-07 3.28e-06 8.92e-04 0.00e+00 6.88e-07\n# ED 4.94e-07 4.74e-24 5.31e-28 5.66e-21 1.19e-18 1.83e-09 6.88e-07 0.00e+00"},{"path":"relations.html","id":"combinando-visualizaÃ§Ã£o-grÃ¡fica-e-numÃ©rica-i","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.3 Combinando visualizaÃ§Ã£o grÃ¡fica e numÃ©rica (I)","text":"Na figura acima, os pontos observados sÃ£o plotados na diagonal inferior. Na diagonal, Ã© apresentada estimativa da densidade Kernel (linha preta) e um histgrama de cada variÃ¡vel. diagonal superior contÃ©m os coeficientes de correlaÃ§Ã£o.","code":"\n\npairs.panels(datacor)"},{"path":"relations.html","id":"combinando-visualizaÃ§Ã£o-grÃ¡fica-e-numÃ©rica-ii","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.4 Combinando visualizaÃ§Ã£o grÃ¡fica e numÃ©rica (II)","text":"funÃ§Ã£o corr_plot()pacote metan retorna um grÃ¡fico semelhante ao anterior, entanto possui diversas opÃ§Ãµes, tais como mudanÃ§tamanho da letra dependendo da magnitude da correlaÃ§Ã£o e indicaÃ§Ã£o de cores para correlaÃ§Ãµes significativas.\r\nFigure 12.1: Scatter plot de uma matriz de correlaÃ§Ã£o de Pearson\r\nfunÃ§Ã£o corr_plot() pode ser utilizada com o operador %>%. Em adiÃ§Ã£o, Ã© possÃ­vel escolher variÃ¡veis serem plotadas. Para isto, basta digitar o nome das variÃ¡veis.\r\nFigure 12.2: Scatter plot de uma matriz de correlaÃ§Ã£o de Pearson\r\n\r\nFigure 12.3: Scatter plot de uma matriz de correlaÃ§Ã£o de Pearson\r\n","code":"\ncorr_plot(datacor)\nmaize %>%\n  corr_plot(CD, EL, PERK, NKR, CW,\n            shape.point = 19,\n            size.point = 2,\n            alpha.point = 0.5,\n            alpha.diag = 0,\n            pan.spacing = 0,\n            col.sign = \"gray\",\n            alpha.sign = 0.3,\n            axis.labels = TRUE,\n            progress = FALSE)\n\nmaize %>%\n  corr_plot(CD, EL, PERK, NKR, CW,\n            shape.point = 21,\n            col.point = \"black\",\n            fill.point = \"orange\",\n            size.point = 2,\n            alpha.point = 0.6,\n            maxsize = 4,\n            minsize = 2,\n            smooth = TRUE,\n            col.smooth = \"black\",\n            col.sign = \"cyan\",\n            upper = \"scatter\",\n            lower = \"corr\",\n            diag.type = \"density\",\n            col.diag = \"cyan\",\n            pan.spacing = 0,\n            lab.position = \"bl\")"},{"path":"relations.html","id":"combinando-visualizaÃ§Ã£o-grÃ¡fica-e-numÃ©rica-iii","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.5 Combinando visualizaÃ§Ã£o grÃ¡fica e numÃ©rica (III)","text":"funÃ§Ã£o corrplot.mixed()  pacote corrplot36 tambÃ©m Ã© uma boa opÃ§Ã£o para visualizaÃ§Ã£o grÃ¡fica, principalmente quando um grande nÃºmero de combinaÃ§Ãµes estÃ¡ disponÃ­vel.Criando matrix de correlaÃ§Ã£o utilizando o conjunto de dados datacor.8 variÃ¡veis | 28 combinaÃ§Ãµes","code":"\n\ncor1 <- cor(datacor)\ncorrplot.mixed(cor1,\n               upper = \"ellipse\",\n               lower = \"number\",\n               number.digits = 2)"},{"path":"relations.html","id":"combinando-visualizaÃ§Ã£o-grÃ¡fica-e-numÃ©rica-iv","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.2.6 Combinando visualizaÃ§Ã£o grÃ¡fica e numÃ©rica (IV)","text":"Criando matrix de correlaÃ§Ã£o utilizando o conjunto de dados dataset.14 variÃ¡veis | 91 combinaÃ§ÃµesA funÃ§Ã£o corrplot()  pacote corrplot permite uma poderosa personalizaÃ§Ã£o. Esta funÃ§Ã£o tem vantagem de apresentar um elevado nÃºmero de combinaÃ§Ãµes em um grÃ¡fico claro e intuitivo.\r\nFigure 12.4: GrÃ¡fico de pizza de uma matriz de correlaÃ§Ã£o de Pearson\r\n","code":"\ncor2 <- cor(numeric_var)\npval <- cor.mtest(cor2)$p\ncorrplot(cor2,\n         method = \"pie\",\n         p.mat = pval,\n         sig.level = 0.05,\n         insig = \"blank\",\n         type = \"lower\",\n         diag = F,\n         tl.col = \"black\",\n         tl.srt = 45)\ncorrplot(cor2,\n         method = \"ellipse\",\n         p.mat = pval,\n         sig.level = 0.05,\n         insig = \"blank\",\n         type = \"upper\",\n         diag = F,\n         tl.col = \"black\",\n         tl.srt = 45)"},{"path":"relations.html","id":"correlaÃ§Ãµes-genÃ©ticas","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.3 CorrelaÃ§Ãµes genÃ©ticas","text":"funÃ§Ã£o covcor_design() pode ser usada para calcular matrizes de correlaÃ§Ã£o/(co) variÃ¢ncia genÃ©ticas, fenotÃ­picas e residuais atravÃ©s mÃ©todo de AnÃ¡lise de VariÃ¢ncia (ANOVA) usando delineamento de blocos completos casualizados (DBC) ou delineamento inteiramente casualizado (DIC).correlaÃ§Ãµes fenotÃ­picas \\(r^p_{xy}\\), genotÃ­picas \\(r^g_{xy}\\) e residuais \\(r^r_{xy}\\) entre duas variÃ¡veis x e y sÃ£o calculadas conforme segue.\\[\r\nr^p_{xy} = \\frac{cov^p_{xy}}{\\sqrt{var^p_{x}var^p_{y}}} \\\r\nr^g_{xy} = \\frac{cov^g_{xy}}{\\sqrt{var^g_{x}var^g_{y}}} \\\r\nr^r_{xy} = \\frac{cov^r_{xy}}{\\sqrt{var^r_{x}var^r_{y}}}\r\n\\]Utilizando os quadrados mÃ©dios (QM) obtidos da ANOVA, variÃ¢ncias (var) e covariÃ¢ncias (cov) sÃ£o calculadas da seguinte forma:\\[\r\ncov^p_{xy} = [(QMT_{x+y} -QMT_x -QMT_y)/2]/r \\\\\r\nvar^p_x =QMT_x / r \\\\\r\nvar^p_y =QMT_y / r\r\n\\]\\[\r\ncov^r_{xy} = (QME_{x+y} - QME_x - QME_y)/2 \\\\\r\nvar^r_x = QME_x \\\\\r\nvar^r_y = QME_y\r\n\\]\\[\r\ncov^g_{xy} = [(cov^p_{xy} \\times r) - cov^r_{xy}]/r \\\\\r\nvar^g_x = (MST_x - MSE_x)/r \\\\\r\nvar^g_y = (MST_x - MSE_y)/r\r\n\\]onde QMT Ã© o quadrado mÃ©dio para tratamento, QME Ã© o quadrado mÃ©dio erro e r Ã© o nÃºmero de repetiÃ§Ãµes/blocos. funÃ§Ã£o covcor_design()  retorna uma lista com matrizes de (co)variÃ¢ncias e correlaÃ§Ãµes. Matrizes especÃ­ficas podem ser retornadas usando o tipo de argumento type. exemplo abaixo, o coeficiente de correlaÃ§Ã£o genotÃ­pico entre variÃ¡veis PH, EH, NKE e TKW serÃ¡ computado para o ambiente A1, considerando um DBC.Resposta","code":"\nmaize %>%\n  filter(ENV == \"A1\") %>%\n  covcor_design(gen = GEN,\n                rep = REP,\n                resp = c(PH, EH, NKE, TKW),\n                type = \"gcor\")\n#               PH           EH        NKE        TKW\n# PH   1.000000000 -0.006544623  0.2801806  0.2459377\n# EH  -0.006544623  1.000000000 -0.7752497  0.7247684\n# NKE  0.280180560 -0.775249741  1.0000000 -0.5117645\n# TKW  0.245937657  0.724768430 -0.5117645  1.0000000"},{"path":"relations.html","id":"intervalo-de-confianÃ§a","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.4 Intervalo de confianÃ§a","text":"","code":""},{"path":"relations.html","id":"paramÃ©trico","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.4.1 ParamÃ©trico","text":"O intervalo de confianÃ§para o coeficiente de correlaÃ§Ã£o pode ser obtido utilizando funÃ§Ã£o cor.mtest() pacote corrplot, conforme o seguinte exemplo.","code":"\nci_corr <- cor.mtest(datacor)"},{"path":"relations.html","id":"nÃ£o-paramÃ©trico","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.4.2 NÃ£o paramÃ©trico","text":"Um estimador nÃ£o paramÃ©trico intervalo de confianÃ§coeficiente de correlaÃ§Ã£o de Pearson foi proposto por Olivoto et al. (2018). Este estimador Ã© baseado tamanho da amostra e forÃ§de associaÃ§Ãµes e pode ser estimado usando funÃ§Ã£o corr_ci() pacote metan.  Ã‰ possÃ­vel estimar o intervalo de confianÃ§declarando o tamanho da amostra (n) e o coeficiente de correlaÃ§Ã£o (r), ou usando um dataframe. O cÃ³digo seguir calcula o intervalo de confianÃ§para os possÃ­veis pares de correlaÃ§Ã£o entre variÃ¡veis que contÃ©m E nome. Note o benefÃ­cio operador %>% neste caso.","code":"\nmaize %>% \n  select(contains(\"E\")) %>%\n  corr_ci(verbose = FALSE) %>%\n  plot_ci()"},{"path":"relations.html","id":"tamanho-da-amostra","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.5 Tamanho da amostra","text":"Baseado modelo proposto por Olivoto et al. (2018), o tamanho da amostra suficiente para estimativa coeficiente de correlaÃ§Ã£o considerando um intervalo de confianÃ§desejado Ã© obtido pela funÃ§Ã£o corr_ss(). Neste exemplo, vamos calcular o tamanho da amostra necessÃ¡rio para que uma correlaÃ§Ã£o de 0.6 apresente uma semi-amplitude intervalo de confianÃ§igual 0.1 ","code":"\ncorr_ss(r = 0.6, CI = 0.1)\n# ------------------------------------------------- \n# Sample size planning for correlation coefficient \n# ------------------------------------------------- \n# Level of significance: 5%\n# Correlation coefficient: 0.6\n# 95% half-width CI: 0.1\n# Required sample size: 194\n# -------------------------------------------------"},{"path":"relations.html","id":"correlaÃ§Ã£o-parcial","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.6 CorrelaÃ§Ã£o parcial","text":"Em certos casos, o coeficiente de correlaÃ§Ã£o linear simples pode nos levar equÃ­vocos na interpretaÃ§Ã£o da associaÃ§Ã£o entre duas variÃ¡veis, pois este nÃ£o considera influÃªncia das demais variÃ¡veis contidas conjunto de dados. O coeficiente de correlaÃ§Ã£o parcial  Ã© uma tÃ©cnica baseada em operaÃ§Ãµes matriciais que nos permite identificar associaÃ§Ã£o entre duas variÃ¡veis retirando-se os efeitos das demais variÃ¡veis presentes (Anderson 2003) Uma maneira generalizada para estimativa coeficiente de correlaÃ§Ã£o parcial entre duas variÃ¡veis (e j) Ã© por meio da matriz de correlaÃ§Ã£o simples que envolve estas duas variÃ¡veis e m outras variÃ¡veis das quais queremos retirar o efeito. estimativa coeficiente de correlaÃ§Ã£o parcial entre e j excluÃ­o efeito de m outras variÃ¡veis Ã© dado por:\\[\r\n{r_{ij.m} = \\frac{{- {a_{ij}}}}{{\\sqrt {{a_{ii}}{a_{jj}}}}}}\r\n\\]onde \\(r_{ij.m}\\) Ã© o coeficiente de correlaÃ§Ã£o parcial  entre variÃ¡vel e j, sem o efeito das outras m outras variÃ¡veis; \\(a_{ij}\\) Ã© o elemento da ordem ij da inversa da matriz de correlaÃ§Ã£o simples; \\(a_{ii}\\) e \\(a_{jj}\\) sÃ£o os elementos de ordens ii e jj, respectivamente, da inversa da matriz de correlaÃ§Ã£o simples.matrizes de coeficientes de correlaÃ§Ã£o linear e parcial podem ser facilmente obtida utilizando funÃ§Ã£o lpcor()  pacote metan. entrada dos dados pode ser realizada de duas maneiras. () utilizando os dados com observaÃ§Ãµes de cada variÃ¡vel; ou (ii) utilizando uma matriz de correlaÃ§Ã£o linear simples prÃ© estimada. Em nosso exemplo, vamos utilziar os mesmos dados utilizados nas funÃ§Ãµes anteriores (datacor).funÃ§Ã£o retorna 3 objetos: linear.mat que contÃ©m matriz de correlaÃ§Ã£o linear simples; partial.mat que contÃ©m matriz de correlaÃ§Ãµes parciais, results que contÃ©m todas combinaÃ§Ãµes de correlaÃ§Ã£o com seus respectivos testes de hipÃ³tese.","code":"\n\npartial <- lpcor(datacor)\nprint(partial)\n# # A tibble: 28 x 5\n#    Pairs   linear partial      t          prob\n#    <chr>    <dbl>   <dbl>  <dbl>         <dbl>\n#  1 CD x CL  0.300  0.0767  0.936 0.351        \n#  2 CD x CW  0.484  0.125   1.53  0.129        \n#  3 CD x PH  0.315  0.0196  0.238 0.812        \n#  4 CD x EH  0.281 -0.0258 -0.314 0.754        \n#  5 CD x EP  0.175 -0.0121 -0.147 0.884        \n#  6 CD x EL  0.912  0.893  24.1   0            \n#  7 CD x ED  0.390  0.0108  0.131 0.896        \n#  8 CL x CW  0.738  0.465   6.40  0.00000000198\n#  9 CL x PH  0.325 -0.309  -3.95  0.000121     \n# 10 CL x EH  0.397  0.256   3.22  0.00158      \n# # ... with 18 more rows"},{"path":"relations.html","id":"anÃ¡lise-de-trilha","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7 AnÃ¡lise de trilha","text":"Nesta sessÃ£o, primeiramente uma breve introduÃ§Ã£o Ã  anÃ¡lise de trilha Ã© apresentada. Algumas dificuldades, como , por exemplo, presenÃ§de multicolinearidade  e possÃ­veis soluÃ§Ãµes para contornÃ¡-la serÃ£o discutidas. Posteriormente exemplos numÃ©ricos serÃ£o realizados utilizando funÃ§Ãµes pacote metan.","code":""},{"path":"relations.html","id":"introduÃ§Ã£o-1","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.1 IntroduÃ§Ã£o","text":"anÃ¡lise de trilha vem se destacando na Ã¡rea melhoramento genÃ©tico, pois seleÃ§Ã£o para melhoria de um caractere desejÃ¡vel que possui difÃ­cil mensuraÃ§Ã£o e baixa herdabilidade, pode ser realizada indiretamente por outro caractere, diretamente associado este, mas que apresente alta herdabilidade e seja de fÃ¡cil mensuraÃ§Ã£o. Esta tÃ©cnica Ã© baseada em ideias originalmente desenvolvidas por Sewall Wright (Wright 1921), entanto desde sua concepÃ§Ã£o atÃ© consolidaÃ§Ã£o mÃ©todo, algumas divergÃªncias quanto fidedignidade mÃ©todo matemÃ¡tico que explica relaÃ§Ãµes de causa e efeito foram observadas. Em 1922, Henry E. Niles, em um artigo37 publicado na revista Genetics intitulado Correlation, Causation Wrightâ€™s theory path coefficients, fez uma crÃ­tica ao mÃ©todo proposto por Wright, afirmando que base filosÃ³fica mÃ©todo dos coeficientes de trilha era falha. Niles, testando o mÃ©todo de Wright, evidenciou em alguns de seus resultados coeficientes superiores |1|, afirmando [â€¦]estes resultados sÃ£o ridÃ­culos[â€¦] e que Wright teria de fornecer provas bem mais convincentes que ele estava apresentando Niles (1922).ano seguinte, 1923, Sewall Wright em seu artigo38 entitulado theory path coefficients: reply Nilesâ€™s criticism, publicado na mesma revista Genetics, consolida seu mÃ©todo ao concluir que Niles pareceu se basear em conceitos matemÃ¡ticos incorretos, resultado de uma falha em reconhecer que um coeficiente de trilha nÃ£o Ã© uma funÃ§Ã£o simÃ©trica de duas variÃ¡veis, mas que ele necessariamente tem direÃ§Ã£o. Este autor conclui seu trabalho afirmando que anÃ¡lise de trilha nÃ£o fornece uma fÃ³rmula geral para deduzir relaÃ§Ãµes causais partir conhecimento das correlaÃ§Ãµes. Ela Ã©, entanto, dentro de certas limitaÃ§Ãµes, um mÃ©todo de avaliar consequÃªncias lÃ³gicas de uma hipÃ³tese de relaÃ§Ã£o causal em um sistema de variÃ¡veis correlacionadas. Acrescenta ainda que crÃ­ticas oferecidas por Niles em nada invalidam teoria mÃ©todo ou sua aplicaÃ§Ã£o (Wright 1923). Atualmente, o mÃ©todo estatÃ­stico Ã© consolidado, e utilizado mundialmente em diversas Ã¡reas da ciÃªncia.","code":""},{"path":"relations.html","id":"estimativa-2","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.2 Estimativa","text":"decomposiÃ§Ã£o das correlaÃ§Ãµes lineares em efeitos diretos e indiretos de um conjunto de p-variÃ¡veis explicativas Ã© realizada o sistema de equaÃ§Ãµes normais \\[\r\nX'X\\hat \\beta = X'Y\r\n\\]que tem como resoluÃ§Ã£o\\[\r\n\\hat \\beta = X'X^{-1} X'Y\r\n\\]onde \\(\\hat \\beta\\) Ã© o vetor dos coeficiente de regressÃ£o parcial (\\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\), \\(\\hat \\beta_3\\),â€¦,\\(\\hat \\beta_p\\)) para p + 1; \\(X'X^{-1}\\) Ã© inversa da matriz de correlaÃ§Ã£o linear entre variÃ¡veis explicativas e \\(X'Y\\) Ã© matriz de correlaÃ§Ã£o de cada variÃ¡vel explicativa, com variÃ¡vel dependente.ApÃ³s estimativa dos coeficientes de regressÃ£o (\\(\\hat \\beta_p\\)), os efeitos diretos e indiretos conjunto de p-variÃ¡veis explicativas podem ser estimados. Considere o seguinte exemplo, onde um conjunto de variÃ¡veis explicativas (, b, c) sÃ£o utilizadas para explicar relaÃ§Ãµes de causa e efeito  na resposta de uma variÃ¡vel dependente (y). ApÃ³s estimativas dos coeficientes de regressÃ£o parcial (\\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\) e \\(\\hat \\beta_3\\)), os efeitos diretos e indiretos de sobre y sÃ£o dados por:\\[\r\nr_{:y} = \\hat \\beta_1 + \\hat \\beta_{2_{ra:b}} + \\hat \\beta_{3_{ra:c}}\r\n\\]onde \\(r_{:y}\\) Ã© correlaÃ§Ã£o linear entre e y, \\(\\hat \\beta_1\\) Ã© o efeito direto de em y; \\(\\hat \\beta_{2_{ra:b}}\\) Ã© o efeito indireto de em y via b e \\(\\hat \\beta_{3_{ra:c}}\\) Ã© o efeito indireto de em y via c. RegressÃµes semelhantes sÃ£o utilizadas para estimativa dos efeitos de b e c, conforme segue:\\[\\begin{gather*}\r\nr_{b:y} = \\hat \\beta_{1_{rb:}} + \\hat \\beta_2 + \\hat \\beta_{3_{rb:c}}\\\\\r\nr_{c:y} = \\hat \\beta_{1_{rc:}} + \\hat \\beta_{2_{rc:b}} + \\hat \\beta_3\r\n\\end{gather*}\\]Como exemplo numÃ©rico, vamos utilizar variÃ¡veis PERK, EH, CDED como variÃ¡veis explicativas e variÃ¡vel KW como dependente, conjunto de dados maize. Para seleÃ§Ã£o destas variÃ¡veis, funÃ§Ã£o select() Ã© utilizada.Utilizando os conhecimentos acumulados atÃ© agora, estes mesmos coeficientes podem ser estimados de maneira mais â€œelegantementeâ€, utilizando o cÃ³digo abaixo.","code":"\nx <- maize %>% select(PERK, EH, CDED)\ny <- maize %>% select(KW)\n\nxx <- cor(x) # CorrelaÃ§Ã£o entre as variÃ¡veis explicativas\nxy <- cor(x, y) # CorrelaÃ§Ã£o das explicativas com a dependente\nb <- solve(xx) %*% xy # Estimativa dos coeficientes\n\nPERK_KW_DIR <- b[1] # Direto de PERK em KW\nPERK_KW_EH <- b[2] * xx[1,2] # Indireto de PERK em KW via EH\nPERK_KW_CDED <- b[3] * xx[1,3] # Indireto de PERK em KW via CDED\n\nEH_KW_PERK <- b[1] * xx[2,1] # Indireto de EH em KW via PERK\nEH_KW_DIR <- b[2] # Direto de EH em KW\nEH_KW_CDED <- b[3] * xx[2,3] # Indireto de EH em KW via CDED\n\nCDED_KW_PERK <- b[1] * xx[3,1] # Indireto de CDED em KW via PERK\nCDED_KW_EH <- b[2] * xx[3,2] # Indireto de CDED em KW via EH\nCDED_KW_DIR <- b[3] # Direto de CDED em KW\n\n# Coeficientes de trilha (direto na diagonal, indireto fora da diagonal)\ncoeff <- matrix(c(PERK_KW_DIR, PERK_KW_EH, PERK_KW_CDED,\n                  EH_KW_PERK, EH_KW_DIR, EH_KW_CDED,\n                  CDED_KW_PERK, CDED_KW_EH, CDED_KW_DIR),\n                ncol = 3)\nrownames(coeff) <- colnames(coeff) <- c(\"PERK\", \"EH\", \"CDED\")\ncoeff\n#             PERK          EH        CDED\n# PERK -0.10410823 0.002222624  0.05948542\n# EH   -0.01473329 0.690110850 -0.04548514\n# CDED  0.09200901 0.010613425 -0.16102929\nn <- ncol(xx)\nCoeff <- data.frame(xx)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    Coeff[j, i] <- b[j] * xx[j, i]\n  }\n}\nrownames(coeff) <- colnames(coeff) <- c(\"PERK\", \"EH\", \"CDED\")\nCoeff\n#             PERK          EH        CDED\n# PERK -0.10410823 0.002222624  0.05948542\n# EH   -0.01473329 0.690110850 -0.04548514\n# CDED  0.09200901 0.010613425 -0.16102929"},{"path":"relations.html","id":"multicolinearidade","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.3 Multicolinearidade","text":"Embora esta anÃ¡lise revele associaÃ§Ãµes de causa e efeito, sua estimativa Ã© baseada em princÃ­pios de regressÃ£o mÃºltipla. Assim, estimativas dos parÃ¢metros podem ser enviesadas devido natureza complexa dos dados, em que resposta da variÃ¡vel dependente estÃ¡ ligada um grande nÃºmero de variÃ¡veis explicativas, que sÃ£o muitas vezes correlacionadas ou multicolineares entre si (Graham 2003). Assim, sempre que duas supostas variÃ¡veis explicativas se apresentam altamente associadas, Ã© difÃ­cil estimar relaÃ§Ãµes de cada variÃ¡vel explicativa individualmente, uma vez que vÃ¡rios parÃ¢metros resolvem o sistema de equaÃ§Ãµes normais. esta particularidade Ã© atribuÃ­da o nome de multicolinearidade  (Blalock 1963).Os principais meios utilizados para identificar o grau de multicolinearidade em uma matriz de variÃ¡veis explicativas sÃ£o os seguintes:NÃºmero de condiÃ§Ã£o (CN):  O nÃºmero de condiÃ§Ã£o Ã© calculado pela razÃ£o entre o maior e menor autovalor (\\(\\lambda\\)) da matriz de correlaÃ§Ã£o \\(X'X\\), de acordo com expressÃ£o\\[\r\n {\\rm{NC = }}\\frac{{\\lambda_{\\rm{Max}}}}{{\\lambda_{\\rm{Min}}}}\r\n\\]O grau de multicolinearidade Ã© considerado fraco se NC \\(\\leq\\) 100, moderado se 100 \\(\\leq\\) NC \\(\\leq\\) 1000 e severo quando NC > 1000.Determinante da matriz \\(X'X\\) (D):  O determinante da cada matriz de correlaÃ§Ã£o Ã© estimado pelo produto de seus respectivos autovalores, para \\(\\lambda_j > 0\\), de acordo com expressÃ£o\\[\r\n\\mathop D\\nolimits_{{\\boldsymbol{X'X}}} {\\rm{  = }}\\prod\\limits_{j = 1}^p {\\lambda j}\r\n\\]Um determinantes muito prÃ³ximo zero indica dependÃªncia linear entre caracterÃ­sticas explicativas, indicando problemas graves de multicolinearidade.Fator de inflaÃ§Ã£o de variÃ¢ncia (VIF):  Os (VIFs) sÃ£o utilizados para medir o quanto variÃ¢ncia dos coeficientes de regressÃ£o estimados (\\(\\hat \\beta_k\\)) foi inflada em comparaÃ§Ã£o quando os caracteres explicativos nÃ£o sÃ£o linearmente associados. estimativa VIF k-Ã©simo elemento de \\(\\hat \\beta\\) Ã© dada pela soma dos quocientes quadrado de cada componente autovetor pelo seu respectivo autovalor associado, de acordo com expressÃ£o\\[\r\n\\mathop {{\\rm{VIF}}}\\nolimits_{\\mathop \\beta \\nolimits_k }  = \\left( {\\frac{{\\mathop {{\\rm{(AV}}}\\nolimits_{KC1} {)^2}}}{{\\lambda 1}} + \\frac{{{{(\\mathop {{\\rm{AV}}}\\nolimits_{KC2} )}^2}}}{{\\lambda 2}} + ... + \\frac{{{{(\\mathop {{\\rm{AV}}}\\nolimits_{KCp} )}^2}}}{{\\lambda p}}} \\right)\r\n \\]onde \\(\\mathop {{\\rm{VIF}}}\\nolimits_{\\mathop \\beta \\nolimits_k }\\) Ã© o fator de inflaÃ§Ã£o de variÃ¢ncia o k-Ã©simo elemento de \\(\\beta\\) para k = 1, 2, â€¦, p; \\(\\mathop {{\\rm{EV}}}\\nolimits_{KC{\\rm{1}}}\\) Ã© o componente k-Ã©simo autovetor para k = 1, 2, â€¦, p e C = 1, 2, â€¦, p; e \\(\\lambda\\) Ã© o autovalor associado ao respectivo autovetor para \\(\\lambda\\) = 1, 2, â€¦, p. Os VIFs tambÃ©m podem ser considerados como os elementos da diagonal da inversa da matriz \\(X'X\\). Considera-se que presenÃ§de VIFs maiores que 10 Ã© um indicativo de multicolinearidade .","code":""},{"path":"relations.html","id":"diagnÃ³stico-da-multicolinearidade","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.3.1 DiagnÃ³stico da multicolinearidade","text":"funÃ§Ã£o colindiag()  pacote metan Ã© utilizada para realizar o diagnÃ³stico da multicolinearidade de uma matriz de correlaÃ§Ã£o. Os cÃ³digos seguir calculam um diagnÃ³stico completo de colinearidade de uma matriz de correlaÃ§Ã£o de caracterÃ­sticas preditivas. VÃ¡rios indicadores, como fator de inflaÃ§Ã£o de variaÃ§Ã£o, nÃºmero de condiÃ§Ã£o e determinante de matriz sÃ£o considerados (T. Olivoto, Souza, et al. 2017; Tiago Olivoto et al. 2017) O diagnÃ³stico pode ser realizado usando: () matrizes de correlaÃ§Ã£o; (ii) quadros de dados ou (iii) um objeto agrupado passado de group_by().Usando uma matriz de correlaÃ§Ã£o, estimada anteriormenteUsando um dataframeDiagnÃ³stico para cada nÃ­vel fator ENV","code":"\ncor_data <- cor(datacor)\nn <- nrow(datacor)\ncold1 <- colindiag(cor_data, n = n)\nprint(cold1)\n# Severe multicollinearity in the matrix! Pay attention on the variables listed bellow\n# CN = 1830.678\n# Matrix determinant: 2.32e-05 \n# Largest correlation: PH x EH = 0.932 \n# Smallest correlation: CD x EP = 0.175 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 3 \n# Variables with largest weight in the last eigenvalues: \n# EH > PH > EP > CL > ED > CW > CD > EL\ncold2 <- colindiag(datacor)\nprint(cold2)\n# Severe multicollinearity in the matrix! Pay attention on the variables listed bellow\n# CN = 1830.678\n# Matrix determinant: 2.32e-05 \n# Largest correlation: PH x EH = 0.932 \n# Smallest correlation: CD x EP = 0.175 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 3 \n# Variables with largest weight in the last eigenvalues: \n# EH > PH > EP > CL > ED > CW > CD > EL\ncold3 <- colindiag(data_ge2, by = ENV)\nprint(cold3)\n# # A tibble: 4 x 2\n#   ENV   data      \n#   <fct> <list>    \n# 1 A1    <colindig>\n# 2 A2    <colindig>\n# 3 A3    <colindig>\n# 4 A4    <colindig>"},{"path":"relations.html","id":"seleÃ§Ã£o-de-preditores-com-mÃ­nima-multicolinearidade","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.3.2 SeleÃ§Ã£o de preditores com mÃ­nima multicolinearidade","text":"funÃ§Ã£o non_collinear_vars() seleciona um conjunto de preditores com multicolinearidade mÃ­nima usando o fator de inflaÃ§Ã£o de variaÃ§Ã£o (VIF) como critÃ©rio para remover variÃ¡veis colineares. O algoritmo irÃ¡: () calcular o valor VIF da matriz de correlaÃ§Ã£o que contÃ©m variÃ¡veis originais; (ii) ordenar os valores VIF e excluir variÃ¡vel com maior VIF; e (iii) iterar etapa ii atÃ© que o valor VIF seja menor ou igual um valor prÃ©-estabelecido.Em anÃ¡lise de trilha, o diagnÃ³stico da multicolinearidade deve ser realizado na matriz de correlaÃ§Ã£o das variÃ¡veis explicativas. exemplo acima, supondo que anÃ¡lise de trilha fosse realizada considerando variÃ¡vel PH como dependente, o seguinte comando deveria ter sido utilizado para o diagnÃ³stico da multicolinearidade.multicol <- datacor %>% select(-PH) %>% colindiag()","code":"\nnon_collinear_vars(data_ge2)\n#          Parameter                                       values\n# 1       Predictors                                           10\n# 2              VIF                                         7.16\n# 3 Condition Number                                       56.797\n# 4      Determinant                                 0.0008810515\n# 5         Selected PERK, EP, CDED, NKR, PH, NR, TKW, EL, CD, ED\n# 6          Removed                          EH, CL, CW, KW, NKE\nnon_collinear_vars(data_ge2, EH, CL, CW, KW, NKE, max_vif = 5)\n#          Parameter          values\n# 1       Predictors               4\n# 2              VIF           2.934\n# 3 Condition Number          11.248\n# 4      Determinant    0.2400583901\n# 5         Selected NKE, EH, CL, CW\n# 6          Removed              KW"},{"path":"relations.html","id":"mÃ©todos-para-ajustar-a-multicolinearidade","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.4 MÃ©todos para ajustar a multicolinearidade","text":"Embora os problemas relacionados multicolinearidade  se apresente como uma dificuldade na estimativa de coeficientes de trilha, algumas medidas podem ser tomadas visando mitigar seus efeitos indesejÃ¡veis, quando esta detectada pelos mÃ©todos acima citados. Sabe-se atualmente, que exclusÃ£o das variÃ¡veis responsÃ¡veis por inflar variÃ¢ncia de um coeficiente de regressÃ£o Ã© um dos mÃ©todos mais indicados para reduzir multicolinearidade em matrizes de variÃ¡veis explicativas (T. Olivoto, Souza, et al. 2017). identificaÃ§Ã£o destas variÃ¡veis, entanto, pode ser uma tarefa difÃ­cil. Recentemente, T. Olivoto, Nardino, et al. (2017) propuseram utilizaÃ§Ã£o de procedimentos stepwise  juntamente com anÃ¡lise de trilha sequencial visando identificar um conjunto de variÃ¡veis com alto poder explicativo, mas que nÃ£o se apresentem altamente correlacionadas. Quando exclusÃ£o das variÃ¡veis responsÃ¡veis nÃ£o Ã© um procedimento considerado pelo pesquisador, por exemplo, devido um nÃºmero reduzido de variÃ¡veis explicativa, ou pela importÃ¢ncia em conhecer seus efeitos, uma terceira opÃ§Ã£o Ã© realizar anÃ¡lise de trilha com todas variÃ¡veis explicativas, porÃ©m com inclusÃ£o de um pequeno valor nos elementos da diagonal \\(X'X\\), conhecida como regressÃ£o em crista39. Este procedimento, entanto superestima os efeitos diretos, principalmente daquelas variÃ¡veis com alto VIF. (T. Olivoto, Souza, et al. 2017).","code":""},{"path":"relations.html","id":"anÃ¡lise-tradicional","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.5 AnÃ¡lise tradicional","text":"Esta sessÃ£o estÃ¡ focada principalmente em trÃªs objetivos: () diagnÃ³stico da multicolinearidade; (ii) seleÃ§Ã£o de variÃ¡veis  preditoras; e (iii) estimaÃ§Ã£o dos coeficientes de trilha. Embora esta seja sequÃªncia correta ser seguida para estimativa dos coeficientes de trilha, utilizaremos somente funÃ§Ã£o path_coeff(),  que possibilita todas estas abordagens. Para isto, o conjunto de dados maize serÃ¡ utilizado.Com funÃ§Ã£o acima, os coeficientes de trilha foram estimados considerando variÃ¡vel KW (massa de grÃ£os por espiga) como dependente, e todas outras variÃ¡veis numÃ©ricas conjunto de dados como explicativas. funÃ§Ã£o summary() pode ser utilizada para resumir os resultados da anÃ¡lise. Note que os coeficientes foram estimados considerando todos os nÃ­veis dos fatores ENV, GEN, e REP.mensagem de aviso gerada pela funÃ§Ã£o acima pode ser suprimidada utilizando o argumento verbose = FALSEDe acordo com o NC, VIF e determinante da matriz, multicolinearidade  na matriz das variÃ¡veis explicativa Ã© severa. Por exemplo, foram observados oito VIFs > 10 e o determinante da matriz foi de \\(8.619 \\times 10^{-12}\\). anÃ¡lise dos autovalores-autovetores (pathtodas$weightvar) indicou que, em ordem de importÃ¢ncia, variÃ¡veis que mais contribuem para multicolinearidadeque sÃ£o: CL > ED > CDED > EH > CW > PH > NKE > EP > TKW > PERK > NR > EL > NKR > CD. Conforme discutido, temos basicamente duas opÃ§Ãµes para contornar os problemas da elevada multicolineridade em nossos dados. Excluir variÃ¡veis responsÃ¡veis pela multicolinearidade, ou manter todas variÃ¡veis e incluir um fator de correÃ§Ã£o na diagonal da matrix . Vamos comeÃ§ar pela Ãºltima opÃ§Ã£o.","code":"\npathtodas <- \n  maize %>%\n  path_coeff(KW, verbose = FALSE)"},{"path":"relations.html","id":"incluindo-um-fator-de-correÃ§Ã£o-k","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.6 Incluindo um fator de correÃ§Ã£o (k)","text":"variÃ¢ncia dos coeficientes de regressÃ£o Ã© reduzida quando quando um valor k para \\(0 < k \\leq 1\\) Ã© incluÃ­na diagonal da matriz de correlaÃ§Ã£o \\(X'X\\). Com esta tÃ©cnica, estimativa dos coeficientes de regressÃ£o Ã© dado por:\\[\r\n\\hat \\beta = (X'X+Ik)^{-1} X'Y\r\n\\]escolha da magnitude de k, entanto, deve ser cautelosa. Sugere-se que o valor ser incluÃ­seja aquele menor possÃ­vel que estabilize os coeficientes de regressÃ£o (\\(\\beta_p\\)). Felizmente, grande parte deste trabalho jÃ¡ foi realizado quando utilizamos funÃ§Ã£o path_coeff()  na sessÃ£o anterior. Um conjunto de estimativas de \\(\\beta_p\\) foi estimado com 101 valores de k, (\\(k = 0, 0.01, ..., 1\\)) e representado graficamente.\r\nFigure 12.5: Valores de beta obtidos com 101 valores de k\r\nprint()O grÃ¡fico acima nos proporciona uma interpretaÃ§Ã£o sobre qual Ã© o valor de k mais indicado ser utilizado. Para fins didÃ¡tidos, escolheremos, por enquanto, o valor de k igual 0.04, valor qual os coeficientes de regressÃ£o da maioria das variÃ¡vies se estabiliza. Para incluir este valor de correÃ§Ã£o, utilizaremos novamente funÃ§Ã£o path_coeff(), entanto, agora, incluiremos o argumento correction = 0.04.Com inclusÃ£o fator de correÃ§Ã£o (k = 0.04) na diagonal de \\(X'X\\), multicolinearidade foi classificada como moderada (NC = 141). Inevitavelmente, temos duas opÃ§Ãµes para obtermos menores nÃ­veis de multicollinearidade. primeira Ã© aumentar o valor de k, digamos, para 0.1. Isto iria reduzir ainda mais o nÃ­vel de multicolinearidade  em nossa matriz, entanto, o viÃ©s na estimativa dos coeficientes aumentaria. segunda (e mais razoÃ¡vel) opÃ§Ã£o, Ã© exclusÃ£o das variÃ¡veis que mais causam problemas de multicolinearidade. Por exemplo, podemos considerar variÃ¡veis com maior peso nos Ãºltimos autovalores, ou aquelas com maior VIF. variÃ¡veis CL e EL apresentam alta correlaÃ§Ã£o (veja seÃ§Ã£o estimativas dos coeficientes de correlaÃ§Ã£o), assim poderÃ­amos manter apenas uma destas variÃ¡veis. mesma interpretaÃ§Ã£o pode ser considerada para CDED. Esta Ã© uma co-variÃ¡vel (razÃ£o diÃ¢metro sabugo e diÃ¢metro da espiga, CDED = CD/ED). Vamos considerar entÃ£o exclusÃ£o destas variÃ¡veis.","code":"\npathtodas$plot\npathtodas_k <- \n  maize %>% \n  path_coeff(KW, correction = 0.04, verbose = FALSE)"},{"path":"relations.html","id":"excluindo-variÃ¡veis","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.7 Excluindo variÃ¡veis","text":"O ajuste novo modelo excluindo estas variÃ¡veis Ã© facilmente realizado. Para isto, iremos utilizar dois argumentos da funÃ§Ã£o path_coeff()  nÃ£o vistos atÃ© agora: pred e exclude. variÃ¡veis informadas em pred podem ser variÃ¡veis preditoras (default) ou variÃ¡veis serem excluÃ­das, se exclude = TRUE. Vamos ao exemplo.Abaixo, um resumo das trÃªs abordagens realizadas atÃ© agora, utilizando o resumo apresentado na tabela abaixo.Conforme tambÃ©m observado por Hoerl Kennard (1970) e T. Olivoto, Souza, et al. (2017), exclusÃ£o de variÃ¡veis responsÃ¡veis pela multicolinearidade  foi mais eficiente que inclusÃ£o k, proporcionando menores nÃ­veis de multicolinearidade e maior precisÃ£o modelo (maior \\(R^2\\) e menor residual). Os nÃ­veis de multicolinearidade ao excluir variÃ¡veis ainda preocupam. Vimos que tanto identificaÃ§Ã£o das variÃ¡veis responsÃ¡veis pela multicolinearidae quanto o ajuste modelo declarando preditores especÃ­ficos Ã© um procedimento relativamente simples utilizaÃ§Ã£o funÃ§Ã£o path_coeff(). Mas, e se algum procedimento estatistico-computacional facilitasse ainda mais essa tarefa? Vamos, partir de agora, considerar isso.T. Olivoto, Nardino, et al. (2017) sugeriram utilizaÃ§Ã£o de regressÃµes stepwise para seleÃ§Ã£o de um conjunto de preditores com minima multicolinearidade em anÃ¡lise de trilha. Esta opÃ§Ã£o estÃ¡ disponÃ­vel na funÃ§Ã£o path_coeff(). Baseado em um algoritmo heurÃ­stico iterativo executado pelo argumento brutstep = TRUE, um conjunto de preditores com mÃ­nima multicolienaridade Ã© selecionado com base nos valores de VIF. Posteriormente, uma sÃ©rie de regressÃµes stepwise sÃ£o ajustadas. primeira regressÃ£o stepwise Ã© ajustada considerando \\(p-1\\) variÃ¡veis preditoras selecionadas, sendo p o nÃºmero de variÃ¡veis selecionadas processo iterativo. O segundo modelo ajusta uma regressÃ£o considerando \\(p-2\\) variÃ¡vies selecionadas, e assim por diante atÃ© o Ãºltimo modelo, que considera apenas duas variÃ¡veis selecionadas. Vamos ao exemplo.","code":"\npath_exclude <- \n  maize %>% \n  path_coeff(resp = KW,\n             pred = c(PERK, EH, CDED),\n             exclude = TRUE,\n             verbose = FALSE)"},{"path":"relations.html","id":"seleÃ§Ã£o-de-variÃ¡veis-em-anÃ¡lise-de-trilha","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.8 SeleÃ§Ã£o de variÃ¡veis em anÃ¡lise de trilha","text":" TrÃªs objetos sÃ£o criados por esta funÃ§Ã£o: Summary, Models e Selectedpred. O objeto Summary contÃ©m um resumo procedimento, listando o nÃºmero modelo, o valor AIC, o diagnÃ³stico da multicolinearidade  e os valores de R2 e residual. O objeto Models, contÃ©m todos os modelos ajustados, e o objeto Selectedpred, contÃ©m o nome das variÃ¡vies preditoras selecionadas processo iterativo. Podemos notar que o algorÃ­tmo selecionou um conjunto com 10 preditores (PERK, EP, CDED, NKR, PH, NR, TKW, EL, CD, ED) que apresenta multicolinearidade em niveis aceitÃ¡veis. Assim, qualquer um destes modelos poderia ser utilizado sem maiores problemas em relaÃ§Ã£o Ã  isto. O procedimento stepwise realizado com diferentes nÃºmeros de variÃ¡veis selecionados tambÃ©m permite seleÃ§Ã£o de um modelo mais parcimonio, tarefa que ficarÃ¡ critÃ©rio pesquisador.","code":"\npath_step <- \n  maize %>% \n  path_coeff(resp = KW,\n             brutstep = TRUE)\n# --------------------------------------------------------------------------\n# The algorithm has selected a set of 10 predictors with largest VIF = 7.16. \n# Selected predictors: PERK EP CDED NKR PH NR TKW EL CD ED \n# A forward stepwise-based selection procedure will fit 8 models.\n# --------------------------------------------------------------------------\n# Adjusting the model 1 with 9 predictors (12.5% concluded)\n# Adjusting the model 2 with 8 predictors (25% concluded)\n# Adjusting the model 3 with 7 predictors (37.5% concluded)\n# Adjusting the model 4 with 6 predictors (50% concluded)\n# Adjusting the model 5 with 5 predictors (62.5% concluded)\n# Adjusting the model 6 with 4 predictors (75% concluded)\n# Adjusting the model 7 with 3 predictors (87.5% concluded)\n# Adjusting the model 8 with 2 predictors (100% concluded)\n# Done!\n# --------------------------------------------------------------------------\n# Summary of the adjusted models \n# --------------------------------------------------------------------------\n#    Model  AIC Numpred    CN Determinant    R2 Residual maxVIF\n#  MODEL_1 1099       9 50.99     0.00216 0.945   0.0545   6.96\n#  MODEL_2 1098       8 45.48     0.00396 0.945   0.0550   6.80\n#  MODEL_3 1097       7 37.46     0.02618 0.944   0.0555   6.42\n#  MODEL_4 1103       6 35.70     0.03481 0.942   0.0582   5.71\n#  MODEL_5 1116       5 26.39     0.08049 0.936   0.0642   5.71\n#  MODEL_6 1129       4 21.07     0.17146 0.930   0.0705   5.71\n#  MODEL_7 1148       3  1.34     0.97871 0.919   0.0808   1.02\n#  MODEL_8 1232       2  1.57     0.95068 0.860   0.1402   1.05\n# --------------------------------------------------------------------------"},{"path":"relations.html","id":"anÃ¡lise-de-trilha-para-cada-nÃ­vel-de-um-fator","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.7.9 AnÃ¡lise de trilha para cada nÃ­vel de um fator","text":"Em alguns casos, Ã© de interesse estimar os coeficientes de trilha para cada nÃ­vel de um fator, por exemplo, para cada ambiente em um ensaio multi-ambiente. Utilizando o argumento , isto Ã© facilmente realizado. . Para fins didÃ¡ticos vamos estimar os coeficientes para cada ambiente conjunto de dados maize. ","code":"\npath_levels <- \n  maize %>%\n  group_by(ENV) %>%\n  path_coeff(resp = KW,\n             pred = c(TKW, NKE, PERK))\n# Weak multicollinearity. \n# Condition Number = 4.697\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 2.296\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 4.244\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 1.884\n# You will probably have path coefficients close to being unbiased."},{"path":"relations.html","id":"correlaÃ§Ã£o-canÃ´nica","chapter":"CapÃ­tulo 12 RelaÃ§Ãµes lineares entre variÃ¡veis","heading":"12.8 CorrelaÃ§Ã£o canÃ´nica","text":"CorrelaÃ§Ãµes canÃ´nicas podem ser implementadas com funÃ§Ã£o can_corr(). Primeiro, renomearemos variÃ¡veis relacionadas Ã  planta PH EH e EP com o sufixo _PLA para mostrar usabilidade select helper contains().$$","code":"\ndata_cc <- \n  rename(data_ge2,\n         PH_PLA = PH,\n         EH_PLA = EH,\n         EP_PLA = EP)\n\n# Digitar nome das variÃ¡veis\ncc1 <- \n  can_corr(data_cc,\n           FG = c(PH_PLA, EH_PLA, EP_PLA),\n           SG = c(EL, ED, CL, CD, CW, KW))\n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of first group (FG)\n# ---------------------------------------------------------------------------\n#           PH_PLA    EH_PLA    EP_PLA\n# PH_PLA 1.0000000 0.9318282 0.6384123\n# EH_PLA 0.9318282 1.0000000 0.8695460\n# EP_PLA 0.6384123 0.8695460 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within first group \n# ---------------------------------------------------------------------------\n# The multicollinearity in the matrix should be investigated.\n# CN = 977.586\n# Largest VIF = 229.164618380199\n# Matrix determinant: 0.0025852 \n# Largest correlation: PH_PLA x EH_PLA = 0.932 \n# Smallest correlation: PH_PLA x EP_PLA = 0.638 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# EH_PLA > PH_PLA > EP_PLA \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of second group (SG)\n# ---------------------------------------------------------------------------\n#           EL        ED        CL        CD        CW        KW\n# EL 1.0000000 0.3851451 0.2554068 0.9118653 0.4581728 0.6685601\n# ED 0.3851451 1.0000000 0.6974629 0.3897128 0.7371305 0.8241426\n# CL 0.2554068 0.6974629 1.0000000 0.3003636 0.7383379 0.4709310\n# CD 0.9118653 0.3897128 0.3003636 1.0000000 0.4840299 0.6259806\n# CW 0.4581728 0.7371305 0.7383379 0.4840299 1.0000000 0.7348622\n# KW 0.6685601 0.8241426 0.4709310 0.6259806 0.7348622 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within second group \n# ---------------------------------------------------------------------------\n# Weak multicollinearity in the matrix\n# CN = 66.084\n# Matrix determinant: 0.0028626 \n# Largest correlation: EL x CD = 0.912 \n# Smallest correlation: EL x CL = 0.255 \n# Number of VIFs > 10: 0 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# KW > EL > ED > CD > CL > CW \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between FG and SG\n# ---------------------------------------------------------------------------\n#               EL        ED        CL        CD        CW        KW\n# PH_PLA 0.3801960 0.6613148 0.3251648 0.3153910 0.5047388 0.7534439\n# EH_PLA 0.3626537 0.6302561 0.3971935 0.2805118 0.5193136 0.7029469\n# EP_PLA 0.2634237 0.4580196 0.3908239 0.1750448 0.4248098 0.4974193\n# ---------------------------------------------------------------------------\n# Correlation of the canonical pairs and hypothesis testing \n# ---------------------------------------------------------------------------\n#              Var   Percent       Sum      Corr  Lambda     Chisq DF   p_val\n# U1V1 0.630438540 78.617161  78.61716 0.7940016 0.30668 177.29224 18 0.00000\n# U2V2 0.163384310 20.374406  98.99157 0.4042083 0.82985  27.97651 10 0.00182\n# U3V3 0.008086721  1.008433 100.00000 0.0899262 0.99191   1.21794  4 0.87514\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the first group \n# ---------------------------------------------------------------------------\n#               U1        U2         U3\n# PH_PLA  2.609792  5.490798   7.575090\n# EH_PLA -2.559005 -7.646096 -12.812234\n# EP_PLA  1.191023  2.428742   6.604968\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the second group \n# ---------------------------------------------------------------------------\n#             V1         V2          V3\n# EL -0.01008726 -1.0481893  0.60553720\n# ED  0.14629899  0.7853469 -1.30457763\n# CL -0.09112023 -1.2989864 -0.07497186\n# CD -0.29105227  1.1513083 -1.50589651\n# CW -0.12527616 -0.0361706  0.21180796\n# KW  1.16041981 -0.1022916  1.34278026\n# ---------------------------------------------------------------------------\n# Canonical loads of the first group \n# ---------------------------------------------------------------------------\n#               U1          U2         U3\n# PH_PLA 0.9856022 -0.08351129 -0.1470178\n# EH_PLA 0.9085216 -0.41771278 -0.0102277\n# EP_PLA 0.6319736 -0.71449671  0.3001730\n# ---------------------------------------------------------------------------\n# Canonical loads of the second group \n# ---------------------------------------------------------------------------\n#           V1          V2         V3\n# EL 0.4759982 -0.11260907 -0.2944636\n# ED 0.8294407 -0.18663860 -0.4477426\n# CL 0.3749015 -0.74801793 -0.4937819\n# CD 0.3951578  0.02985218 -0.5415818\n# CW 0.6225367 -0.41451273 -0.2698904\n# KW 0.9570820 -0.07344796 -0.1498587\n\n# Use select helpers\ncc2 <- \n  can_corr(data_cc,\n           FG = contains(\"_PLA\"),\n           SG = c(EL, ED, CL, CD, CW, KW))\n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of first group (FG)\n# ---------------------------------------------------------------------------\n#           PH_PLA    EH_PLA    EP_PLA\n# PH_PLA 1.0000000 0.9318282 0.6384123\n# EH_PLA 0.9318282 1.0000000 0.8695460\n# EP_PLA 0.6384123 0.8695460 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within first group \n# ---------------------------------------------------------------------------\n# The multicollinearity in the matrix should be investigated.\n# CN = 977.586\n# Largest VIF = 229.164618380199\n# Matrix determinant: 0.0025852 \n# Largest correlation: PH_PLA x EH_PLA = 0.932 \n# Smallest correlation: PH_PLA x EP_PLA = 0.638 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# EH_PLA > PH_PLA > EP_PLA \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of second group (SG)\n# ---------------------------------------------------------------------------\n#           EL        ED        CL        CD        CW        KW\n# EL 1.0000000 0.3851451 0.2554068 0.9118653 0.4581728 0.6685601\n# ED 0.3851451 1.0000000 0.6974629 0.3897128 0.7371305 0.8241426\n# CL 0.2554068 0.6974629 1.0000000 0.3003636 0.7383379 0.4709310\n# CD 0.9118653 0.3897128 0.3003636 1.0000000 0.4840299 0.6259806\n# CW 0.4581728 0.7371305 0.7383379 0.4840299 1.0000000 0.7348622\n# KW 0.6685601 0.8241426 0.4709310 0.6259806 0.7348622 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within second group \n# ---------------------------------------------------------------------------\n# Weak multicollinearity in the matrix\n# CN = 66.084\n# Matrix determinant: 0.0028626 \n# Largest correlation: EL x CD = 0.912 \n# Smallest correlation: EL x CL = 0.255 \n# Number of VIFs > 10: 0 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# KW > EL > ED > CD > CL > CW \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between FG and SG\n# ---------------------------------------------------------------------------\n#               EL        ED        CL        CD        CW        KW\n# PH_PLA 0.3801960 0.6613148 0.3251648 0.3153910 0.5047388 0.7534439\n# EH_PLA 0.3626537 0.6302561 0.3971935 0.2805118 0.5193136 0.7029469\n# EP_PLA 0.2634237 0.4580196 0.3908239 0.1750448 0.4248098 0.4974193\n# ---------------------------------------------------------------------------\n# Correlation of the canonical pairs and hypothesis testing \n# ---------------------------------------------------------------------------\n#              Var   Percent       Sum      Corr  Lambda     Chisq DF   p_val\n# U1V1 0.630438540 78.617161  78.61716 0.7940016 0.30668 177.29224 18 0.00000\n# U2V2 0.163384310 20.374406  98.99157 0.4042083 0.82985  27.97651 10 0.00182\n# U3V3 0.008086721  1.008433 100.00000 0.0899262 0.99191   1.21794  4 0.87514\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the first group \n# ---------------------------------------------------------------------------\n#               U1        U2         U3\n# PH_PLA  2.609792  5.490798   7.575090\n# EH_PLA -2.559005 -7.646096 -12.812234\n# EP_PLA  1.191023  2.428742   6.604968\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the second group \n# ---------------------------------------------------------------------------\n#             V1         V2          V3\n# EL -0.01008726 -1.0481893  0.60553720\n# ED  0.14629899  0.7853469 -1.30457763\n# CL -0.09112023 -1.2989864 -0.07497186\n# CD -0.29105227  1.1513083 -1.50589651\n# CW -0.12527616 -0.0361706  0.21180796\n# KW  1.16041981 -0.1022916  1.34278026\n# ---------------------------------------------------------------------------\n# Canonical loads of the first group \n# ---------------------------------------------------------------------------\n#               U1          U2         U3\n# PH_PLA 0.9856022 -0.08351129 -0.1470178\n# EH_PLA 0.9085216 -0.41771278 -0.0102277\n# EP_PLA 0.6319736 -0.71449671  0.3001730\n# ---------------------------------------------------------------------------\n# Canonical loads of the second group \n# ---------------------------------------------------------------------------\n#           V1          V2         V3\n# EL 0.4759982 -0.11260907 -0.2944636\n# ED 0.8294407 -0.18663860 -0.4477426\n# CL 0.3749015 -0.74801793 -0.4937819\n# CD 0.3951578  0.02985218 -0.5415818\n# CW 0.6225367 -0.41451273 -0.2698904\n# KW 0.9570820 -0.07344796 -0.1498587"},{"path":"multivariate.html","id":"multivariate","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"CapÃ­tulo 13 AnÃ¡lise multivariada","text":"melhoramento genÃ©tico de plantas, diversas variÃ¡veis sÃ£o mensuradas em cada genÃ³tipo, visando maior seguranÃ§na distinÃ§Ã£o de tais genÃ³tipos. Embora em alguns casos possa fazer sentido isolar cada variÃ¡vel e estudÃ¡-la separadamente, geralmente, uma anÃ¡lise que englobe todas variÃ¡veis fornece um maior nÃºmero de informaÃ§Ãµes. Como todo o conjunto de variÃ¡veis Ã© medido em cada genÃ³tipo, variÃ¡veis serÃ£o relacionadas em maior ou menor grau. Consequentemente, se cada variÃ¡vel Ã© analisada isoladamente, estrutura completa dos dados pode nÃ£o ser revelada. anÃ¡lise multivariada Ã© anÃ¡lise estatÃ­stica simultÃ¢nea de uma coleÃ§Ã£o de variÃ¡veis que utilzia informaÃ§Ãµes sobre relaÃ§Ãµes entre estas. Ã‰ muito provÃ¡vel que anÃ¡lise de cada variÃ¡vel separadamente nÃ£o revele padrÃµes interessantes que anÃ¡lise multivariada proporciona.concepÃ§Ã£o da anÃ¡lise multivariada  Ã© provavelmente o trabalho realizado por Francis Galton e Karl Pearson final sÃ©culo XIX sobre quantificaÃ§Ã£o da relaÃ§Ã£o entre descendentes e caracterÃ­sticas parentais e o desenvolvimento coeficiente de correlaÃ§Ã£o (Galton 1888). Naquele tempo, o processamento computacional era muito limitado para suportar o peso das vastas quantidades de aritmÃ©tica envolvidas na aplicaÃ§Ã£o dos mÃ©todos multivariados que estavam sendo propostos. Assim, os desenvolvimentos eram principalmente matemÃ¡ticos e pesquisa multivariada era, na Ã©poca, em grande parte, um ramo de Ã¡lgebra linear. entanto, chegada e rÃ¡pida expansÃ£o uso de computadores eletrÃ´nicos na segunda metade sÃ©culo XX, levou Ã  crescente aplicaÃ§Ã£o prÃ¡tica dos mÃ©todos existentes de anÃ¡lise multivariada, renovando o interesse desenvolvimento de novas tÃ©cnicas.Nos primeiros anos sÃ©culo XXI, ampla disponibilidade de computadores pessoais e laptops relativamente baratos e extremamente poderosos, aliados softwares estatÃ­sticos flexÃ­veis fez com que todos os mÃ©todos de anÃ¡lise multivariada pudessem ser aplicados rotineiramente, mesmo para grandes conjuntos de dados, como os gerados em um programa de melhoramento genÃ©tico â€“por exemplo, dados de marcadores moleculares e sequenciamento gÃªnico.","code":""},{"path":"multivariate.html","id":"correlaÃ§Ãµes-canÃ´nicas","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.1 CorrelaÃ§Ãµes canÃ´nicas","text":"CorrelaÃ§Ãµes canonicas podem ser computadas utilizando funÃ§Ã£o can_corr() pacote metan. O primeiro argumento da funÃ§Ã£o Ã© o conjunto de dados (opcional) que deve conter variÃ¡veis numÃ©ricas que serÃ£o usadas na estimativa das correlaÃ§Ãµes canÃ´nicas. Os grupos de variÃ¡veis sÃ£o definidos pelos argumentos FG (primeiro/menor grupo) e SG (segundo/maior grupo). Por padrÃ£o, um diagnÃ³tico da multicolinearidade Ã© realizado em cada grupo de variÃ¡vel. exemplo abaixo, os coeficientes foram armazenados objeto cc1. Note que o argumento verbose = FALSE foi utilizado para previnir uma longa saÃ­da.Na funÃ§Ã£o can_corr(), os dados tambÃ©m podem ser passados diretamente pelos argumentos FG e SG, por exemplo, FG = maize[, 4:6]. Alternativamente, dados podem ser passados da funÃ§Ã£o split_factors(). Nesse caso, correlaÃ§Ãµes canÃ´nicas serÃ£o estimadas para cada nÃ­vel da variÃ¡vel de agrupamento nessa funÃ§Ã£o.","code":"\ncc1 = can_corr(maize,\n               FG = c(PH, EH, EP),\n               SG = c(EL, ED, CL, CD, CW, KW, NR),\n               verbose = FALSE)\nprint(cc1$Sigtest, digits = 2)\n#        Var Percent Sum Corr Lambda Chisq DF  p_val\n# U1V1 0.632    76.2  76 0.79   0.30 181.8 21 0.0000\n# U2V2 0.187    22.5  99 0.43   0.80  32.5 12 0.0012\n# U3V3 0.011     1.3 100 0.10   0.99   1.6  5 0.9015"},{"path":"multivariate.html","id":"anÃ¡lise-de-agrupamento","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2 AnÃ¡lise de agrupamento","text":"anÃ¡lise de agrupamento  Ã© um procedimento multivariado muito Ãºtil melhoramento de plantas. O princÃ­pio bÃ¡sico Ã© agrupar indivÃ­duos (genÃ³tipos) de acordo com suas semelhanÃ§(variÃ¡veis analizadas). Esta sessÃ£o Ã© focada na estimativa de matrizes de distÃ¢ncias e na implementaÃ§Ã£o de algorÃ­timos aglomerativos de agrupamento hierÃ¡rquicos para confecÃ§Ã£o de dendrogramas.  funÃ§Ã£o clustering()  pacote metan serÃ¡ utilizada para este fim.Existem muitos mÃ©todos para calcular informaÃ§Ãµes de (di)similaridade. opÃ§Ãµes incluidas na funÃ§Ã£o sÃ£o: â€œeuclideanâ€ (padrÃ£o), â€œmaximumâ€, â€œmanhattanâ€, â€œcanberraâ€, â€œbinaryâ€, â€œminkowskiâ€, â€œpearsonâ€, â€œkendallâ€ e â€œspearmanâ€. Estas trÃªs Ãºltimas sÃ£o distÃ¢ncias baseadas em correlaÃ§Ã£o. Para maiores informaÃ§Ãµes veja ?clustering.","code":""},{"path":"multivariate.html","id":"todas-as-linhas-e-todas-as-variÃ¡veis-numÃ©ricas","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.1 Todas as linhas e todas as variÃ¡veis numÃ©ricas","text":"Por padrÃ£o, funÃ§Ã£o computa distancias para cada combinaÃ§Ã£o de linhas conjunto de dados, utilizando todas variÃ¡veis numÃ©ricas conjunto. Isto significa que, considerando o conjunto de dados maize, com 156 observaÃ§Ãµes, 12090 distancias serÃ£o computadas baseadas nas 15 variÃ¡veis numÃ©ricas conjunto.","code":"\nd1 <- clustering(maize)"},{"path":"multivariate.html","id":"com-base-na-mÃ©dia-de-cada-genÃ³tipo","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.2 Com base na mÃ©dia de cada genÃ³tipo","text":"Supondo que o pesquisador deseja computar distancias entre cada genÃ³tipo (o que Ã© lÃ³gico em melhoramento genÃ©tico vegetal) e que esta distancia deve ser computada apenas com algumas variÃ¡veis numÃ©ricas conjunto de dados, o seguinte cÃ³digo deverÃ¡ ser utilizado. Para selecionar variÃ¡veis serem utilziadas, basta apenas fornecer uma lista de nomes separadas por vÃ­rgula e sem o uso conhecido (e ultrapassado) \"\". Em adiÃ§Ã£o, para que distÃ¢ncia seja computada entre os genÃ³tipos, basta passar os dados mÃ©dios de cada genÃ³tipo computados com means_by() . Neste caso, mÃ©dia de cada genÃ³tipo Ã© calculada internamente para cada variÃ¡vel numÃ©rica e distÃ£ncia Ã© computada utilizando estas mÃ©dias. funÃ§Ã£o plot() pode ser usada para plotar um dendrograma. Uma linha Ã© desenhada ponto de corte sugerido de acordo com Mojena (1977).dendrograma exibido acima, cada folha corresponde um genÃ³tipo. Ã€ medida que subimos na Ã¡rvore, genÃ³tipos que sÃ£o semelhantes uns aos outros sÃ£o combinados em ramos, que vÃ£o sendo fundidos uma altura cada vez maior maior. altura da fusÃ£o, fornecida eixo vertical, indica (di)similaridade/distÃ¢ncia entre dois genÃ³tipos. Quanto maior altura da fusÃ£o, menos semelhantes sÃ£o os genÃ³tipos.ApÃ³s confecÃ§Ã£o dendrograma , convÃ©m avaliar se distÃ¢ncias (ou seja, alturas) na Ã¡rvore refletem distÃ¢ncias originais com precisÃ£o. Uma maneira de medir o quÃ£o bem o dendrograma gerado reflete seus dados Ã© calcular correlaÃ§Ã£o entre distÃ¢ncias cofenÃ©ticas e matriz de de distÃ¢ncia originais. procedimento anterior o dendrograma nÃ£o foi mostrado, entanto, o coeficiente de correlaÃ§Ã£o cofenÃ©tico foi calculado. Para isto basta incluir o seguinte comando:Quanto mais prÃ³ximo o valor coeficiente de correlaÃ§Ã£o de 1, mais precisamente o dendrograma  refletirÃ¡ distÃ¢ncias originais. Valores acima de 0,75 sÃ£o considerados bons. O mÃ©todo de ligaÃ§Ã£o â€œaverageâ€, ou UPGMA (padrÃ£o na funÃ§Ã£o clustering() parece produzir altos valores dessa estatÃ­stica. Esta pode ser uma das razÃµes por que ele Ã© tÃ£o popular.","code":"\nmgen <- \n  maize %>% \n  means_by(GEN) %>% \n  column_to_rownames(\"GEN\")\nd2 <- clustering(mgen, NKR, TKW, NKE)\nplot(d2, horiz = FALSE, ylab = \"DistÃ¢ncia euclidiana\")\nd2$cophenetic\n# [1] 0.8640355"},{"path":"multivariate.html","id":"seleÃ§Ã£o-de-variÃ¡veis-1","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.3 SeleÃ§Ã£o de variÃ¡veis","text":"funÃ§Ã£o clustering()  tambÃ©m conta com um algorÃ­timo de seleÃ§Ã£o de variÃ¡veis . O objetivo Ã© selecionar um grupo de variÃ¡veis que mais contribuam para explicar variabilidade dos dados originais. Digamos que se algumas poucas variÃ¡veis pudessem ser utilizadas para agrupar os genÃ³tipos sem que haja perda de informaÃ§Ã£o, recursos humanos e financeiros poderiam ser poupados. Assim ao envÃ©s de avaliarmos 15 variÃ¡veis (em nosso exemplo), poderiamos avaliar somente aquelas que realmente contribuiem para distinÃ§Ã£o dos genÃ³tipos.O algoritmo de seleÃ§Ã£o de variÃ¡veis Ã© executado quando o argumento selvar = TRUE Ã© incluÃ­na funÃ§Ã£o. seleÃ§Ã£o das variÃ¡veis Ã© baseada na soluÃ§Ã£o de autovalores/autovetores baseada nos seguintes passos: 1: calcular matriz de distÃ¢ncia e correlaÃ§Ã£o cofenÃ©tica com variÃ¡veis originais (todas variÃ¡veis numÃ©ricas conjunto de dados); 2: calcular os autovalores e autovetores da matriz de correlaÃ§Ã£o entre variÃ¡veis; 3: deletar variÃ¡vel com o maior peso (maior autovetor menor autovalor); 4: calcular matriz de distÃ¢ncia e correlaÃ§Ã£o cofenÃ©tica com variÃ¡veis que restaram; 5: calcular correlaÃ§Ã£o de Mantel entre matriz de distÃ¢ncias obtidas e matriz de distÃ¢ncia original; 6: iterar os passos 2 5 p - 2 vezes, onde p Ã© o nÃºmero de variÃ¡veis originais. final das iteraÃ§Ãµes, um resumo dos modelos Ã© retornado. distÃ¢ncia Ã© calculada com variÃ¡veis que geraram o modelo com maior correlaÃ§Ã£o copenÃ©tica. Sugerimos uma avaliaÃ§Ã£o criteriosa com o objetivo de escolher um modelo parcimonioso, ou seja, aquele com menor nÃºmero de variÃ¡veis, que apresente correlaÃ§Ã£o cofenÃ©tica aceitÃ¡vel e alta similaridade com distÃ¢ncias originais.saÃ­da acima nos mostra o progresso algorÃ­tmo, indicando qual foi variÃ¡vel excluÃ­da em cada passo. Isto pode ser omitido, indicando verbose = FALSE na funÃ§Ã£o. O resumo modelo (Summary adjusted models) nos fornece duas informaÃ§Ãµes muito Ãºteis. Primeira: ao reduzir o nÃºmero de variÃ¡veis utilziadas na estimativa das distÃ¢ncias, o coeficiente de correlaÃ§Ã£o cofenÃ©tico nÃ£o reduziu significativamente, apresentando variaÃ§Ã£o apenas na terceira casa decimal. segunda, e talvez mais importante, Ã© correlaÃ§Ã£o de mantel realizada com matriz de distÃ¢ncias em cada passo da anÃ¡lise com matriz de distÃ¢ncias inicial, que foi computada com todas variÃ¡veis. Percebe-se que utilizando apenas duas variÃ¡veis, distÃ¢ncias calculadas foram praticamente idÃªnticas (r = 0.992) Ã s distÃ¢ncias calculadas com todas variÃ¡veis. Por padrÃ£o, o algorÃ­tmo estima matriz de distÃ¢ncias considerando variÃ¡veis modelo com maior coeficiente de correlaÃ§Ã£o cofenÃ©tica. Em nosso exemplo, variÃ¡veis utilizadas foram ED, CW, KW, NKR, TKW, e NKE. Veja que reduzimos em um terÃ§o o nÃºmero de variÃ¡veis necessÃ¡rias para diferenciar os tratamentos, sem perda de informaÃ§Ã£o. Salienta-se, entanto, que este resultado pode nÃ£o ser reproduzido em um conjunto de dados diferente, cabendo usuÃ¡rio decidir qual Ã© o melhor modelo ser utilizado.","code":"\nd3 <- clustering(mgen, selvar = TRUE)\n# Calculating model 1 with 15 variables. EH excluded in this step (7.1%).\n# Calculating model 2 with 14 variables. EP excluded in this step (14.3%).\n# Calculating model 3 with 13 variables. CDED excluded in this step (21.4%).\n# Calculating model 4 with 12 variables. PH excluded in this step (28.6%).\n# Calculating model 5 with 11 variables. CL excluded in this step (35.7%).\n# Calculating model 6 with 10 variables. NR excluded in this step (42.9%).\n# Calculating model 7 with 9 variables. PERK excluded in this step (50%).\n# Calculating model 8 with 8 variables. EL excluded in this step (57.1%).\n# Calculating model 9 with 7 variables. CD excluded in this step (64.3%).\n# Calculating model 10 with 6 variables. ED excluded in this step (71.4%).\n# Calculating model 11 with 5 variables. KW excluded in this step (78.6%).\n# Calculating model 12 with 4 variables. CW excluded in this step (85.7%).\n# Calculating model 13 with 3 variables. NKR excluded in this step (92.9%).\n# Calculating model 14 with 2 variables. TKW excluded in this step (100%).\n# Done! \n# -------------------------------------------------------------------------- \n# \n# Summary of the adjusted models \n# -------------------------------------------------------------------------- \n#     Model excluded cophenetic remaining cormantel    pvmantel\n#   Model 1        -  0.8656190        15 1.0000000 0.000999001\n#   Model 2       EH  0.8656191        14 1.0000000 0.000999001\n#   Model 3       EP  0.8656191        13 1.0000000 0.000999001\n#   Model 4     CDED  0.8656191        12 1.0000000 0.000999001\n#   Model 5       PH  0.8656189        11 1.0000000 0.000999001\n#   Model 6       CL  0.8655939        10 0.9999996 0.000999001\n#   Model 7       NR  0.8656719         9 0.9999982 0.000999001\n#   Model 8     PERK  0.8657259         8 0.9999977 0.000999001\n#   Model 9       EL  0.8657904         7 0.9999972 0.000999001\n#  Model 10       CD  0.8658997         6 0.9999964 0.000999001\n#  Model 11       ED  0.8658274         5 0.9999931 0.000999001\n#  Model 12       KW  0.8643556         4 0.9929266 0.000999001\n#  Model 13       CW  0.8640355         3 0.9927593 0.000999001\n#  Model 14      NKR  0.8648384         2 0.9925396 0.000999001\n# --------------------------------------------------------------------------\n# Suggested variables to be used in the analysis \n# -------------------------------------------------------------------------- \n# The clustering was calculated with the  Model 10 \n# The variables included in this model were...\n#  ED CW KW NKR TKW NKE \n# --------------------------------------------------------------------------"},{"path":"multivariate.html","id":"escolha-do-nÃºmero-de-clusters","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.4 Escolha do nÃºmero de clusters","text":"Um dos problemas com o agrupamento  hierÃ¡rquico Ã© que ele nÃ£o nos informa quantos clusters existem ou onde cortar o dendrograma  para formar os clusters. VocÃª pode cortar Ã¡rvore hierÃ¡rquica uma determinada altura, digamos, na mÃ©dia das distÃ¢ncias, entanto esta decisÃ£o Ã© puramente impirica. Por exemplo, se o ponto de corte muito alto, tendemos agrupar genÃ³tipos que podem, de fato, nÃ£o ser semelhantes. Um ponto de corte muito baixo, por outro lado, pode resultar em fracassos na seleÃ§Ã£o, pois consideramos que os genÃ³tipos sÃ£o distintos, podendo nÃ£o o serem. Procedimentos estatisticos Ã  exemplo de Milligan Cooper (1985), Scott Symons (1971), Krzanowski Lai (1988), Halkidi, Batistakis, Vazirgiannis (2001) e Hubert Arabie (1985) sÃ£o recomendados para esta escolha. Estes algorÃ­timos estÃ£o implementados pacote NbClust (Charrad et al. 2014) pode ser utilizado para este fim. Por padrÃ£o, funÃ§Ã£o fornece o ponto de corte calculado pelo mÃ©todo de Mojena (1977).Procedimentos baseado em reamostragens bootstrap que calcula probabilidade de erro de cada galho dendrograma  podem ser utilizados. O cÃ³digo abaixo estima p-valores para cada junÃ§Ã£o dendrograma modelo d3. Para maiore detalhes sobre o mÃ©toddo veja Suzuki Shimodaira (2006).O resultado procedimento bootstrap indicou formaÃ§Ã£o de dois clusters, com probabilidade de erro de 5%. Assim, nas prÃ³ximas funÃ§Ãµes, serÃ£o demostrados os diferentes dendrogramas  que podem ser gerados","code":"\npv_clust <- pvclust(t(d3$data), nboot = 100, method.dist = \"euclidean\")\n# Bootstrap (r = 0.5)... Done.\n# Bootstrap (r = 0.67)... Done.\n# Bootstrap (r = 0.83)... Done.\n# Bootstrap (r = 1.0)... Done.\n# Bootstrap (r = 1.17)... Done.\n# Bootstrap (r = 1.33)... Done.\nplot(pv_clust, hang = -1, cex = 0.5)\npvrect(pv_clust, alpha = 0.95)"},{"path":"multivariate.html","id":"dendrogramas-personalizados","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.5 Dendrogramas personalizados","text":"O pacote factoextra oferece um conjunto de funÃ§Ãµes para estender objetos dendrogramas em R. Um exemplo simples Ã© dado abaixo.","code":"\nde1 <- fviz_dend(d3$hc,\n                 k = 2,\n                 xlab = \"GenÃ³tipos\",\n                 ylab =  \"DistÃ¢ncia euclidiana\",\n                 main = \"\") \nde2 <- fviz_dend(d3$hc,\n                 k = 2,\n                 type = \"circular\")\nde3 <- fviz_dend(d3$hc,\n                 k = 2,\n                 type = \"phylogenic\",\n                 repel = TRUE)\nde4 <- fviz_dend(d1$hc,\n                 k = 5,\n                 xlab = \"GenÃ³tipos\",\n                 ylab =  \"DistÃ¢ncia euclidiana\",\n                 palette = \"jco\",\n                 show_labels = FALSE,\n                 main = \"\")\ncp1 <- plot_grid(de1, de2, de3, nrow = 1,\n                 labels = c(\"de1\", \"de2\", \"de3\"))\nplot_grid(cp1, de4, rows = 2, labels = c(\"\", \"de4\"))"},{"path":"multivariate.html","id":"distancias-para-cada-ambiente","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.2.6 Distancias para cada ambiente","text":"O seguinte cÃ³digo computa distÃ¢ncias para cada nÃ­vel fator AMB conjunto de dados maize. funÃ§Ã£o group_by()  Ã© utilizada para criar uma lista onde cada elemento conterÃ¡ os dados de cada ambiente. Note que o argumento keep_factors = TRUE foi utilizado para manter colunas de fatores. Assim Ã© possÃ­vel computar mÃ©dia para cada genÃ³tipo quando informado o argumento means_by = GEN na funÃ§Ã£o clustering() . O resultado desta funÃ§Ã£o Ã© entÃ£o passado para funÃ§Ã£o pairs_mantel().  Esta funÃ§Ã£o Ã© utilizada para avaliar associaÃ§Ã£o entre quatro matrizes de distÃ¢ncia.","code":"\nd4 <- \n  maize %>%\n  group_by(ENV) %>%\n  clustering(NKR, TKW, NKE, nclust = 4)\npairs_mantel(d4, names = c(\"A1\", \"A2\", \"A3\", \"A4\"),\n             maxsize = 5, minsize = 3)"},{"path":"multivariate.html","id":"componentes-principais","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3 Componentes principais","text":"","code":""},{"path":"multivariate.html","id":"conceito","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.1 Conceito","text":"O objetivo bÃ¡sico da anÃ¡lise de componentes principais (Principal Component Analysis, PCA) Ã© descrever variaÃ§Ã£o em um conjunto de variÃ¡veis correlacionadas \\(x^T = (x_1, ..., x_q)\\), em termos de um novo conjunto de variÃ¡veis nÃ£o correlacionadas, \\(y^T = (y_1, ..., y_q)\\), onde cada variÃ¡vel Ã© uma combinaÃ§Ã£o linear das variÃ¡veis x. novas variÃ¡veis sÃ£o ordenadas em ordem decrescente de â€œimportÃ¢nciaâ€, sentido de que \\(y_1\\) Ã© responsÃ¡vel pelo mÃ¡ximo possÃ­vel da variaÃ§Ã£o dos dados originais entre todas combinaÃ§Ãµes lineares de x. EntÃ£o \\(y_2\\) Ã© escolhido para explicar o mÃ¡ximo possÃ­vel da variaÃ§Ã£o restante, sujeito ser nÃ£o correlacionado com \\(y_1\\), e assim por diante. novas variÃ¡veis definidas por este processo, \\(y_1, ..., y_q\\), sÃ£o os componentes principais. principal vantagem da anÃ¡lise de componentes principais Ã© que os primeiros componentes serÃ£o responsÃ¡veis por uma proporÃ§Ã£o substancial da variaÃ§Ã£o nas variÃ¡veis originais, e podem, conseqÃ¼entemente, serem usados para fornecer um resumo de dimensÃµes inferiores dessas variÃ¡veis.","code":""},{"path":"multivariate.html","id":"pacotes-r","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.2 Pacotes R","text":"Diversas funÃ§Ãµes de diferentes pacotes estÃ£o disponÃ­veis software R para realizaÃ§Ã£o da PCA, entanto, funÃ§Ã£o prcomp() pacote stats (nativa R) serÃ¡ utilizada aqui. Para confecÃ§Ã£o de biplots e extraÃ§Ã£o dos resultados, funÃ§Ãµes pacote factoextra serÃ£o utilizadas. O primeiro passo Ã© realizar instalaÃ§Ã£o e o carregamento pacote.","code":""},{"path":"multivariate.html","id":"formato-dos-dados","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.3 Formato dos dados","text":"Para realizar aplicaÃ§Ã£o da tÃ©cnica de componentes principais, os dados em data_ge2 serÃ£o utilizados. Precisamos, primeiramente, organizar os dados de modo que tenhamos uma matriz de dupla entrada, contendo os genÃ³tipos nas linhas e variÃ¡veis nas colunas.Em PCA, variÃ¡veis sÃ£o frequentemente dimensionadas (isto Ã©, padronizadas). Isto Ã© particularmente recomendado quando variÃ¡veis sÃ£o medidas em diferentes escalas (quilogramas, quilÃ´metros, centÃ­metros, â€¦). Caso contrÃ¡rio, os resultados da PCA podem ser severamente afetados. Geralmente variÃ¡veis sÃ£o escalonadas para mÃ©dia zero e variÃ¡ncia unitÃ¡ria, de acordo com seguinte fÃ³rmula.\\[\r\nx_s = \\frac{x_i- mean(x)}{sd(x)}\r\n\\]onde \\(mean(x)\\) Ã© mÃ©dia dos valores de x e \\(sd(x)\\) Ã© o desvio padrÃ£o. funÃ§Ã£o nativa R scale() pode ser utilizada para padronizar os dados. Isto pode ser realizado, entanto, utilizando o argumento scale. = TRUE na funÃ§Ã£o prcomp().","code":"\n\ndata_pca <- \n  data_ge2 %>% \n  means_by(GEN) %>% \n  column_to_rownames(\"GEN\")"},{"path":"multivariate.html","id":"cÃ³digo-r","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.4 CÃ³digo R","text":"Para ajudar na interpretaÃ§Ã£o PCA realizado acima, usaremos funÃ§Ãµes pacote factoextra. Essas funÃ§Ãµes incluem:\r\nget_eigenvalue(res.pca): Extrai os autovalores/variÃ¢ncias dos componentes principais\r\nfviz_eig(res.pca): Para visualizar os autovalores\r\nget_pca_ind(res.pca), get_pca_var(res.pca): Para extrair os resultados dos genÃ³tipos e variÃ¡veis, respectivamente.\r\nfviz_pca_ind(res.pca), fviz_pca_var(res.pca): Para visualizar os resultados dos genÃ³tipos e variÃ¡veis, respectivamente.\r\nfviz_pca_biplot(res.pca): Para confecÃ§Ã£o de biplots.","code":"\nres.pca <- prcomp(data_pca, scale. = TRUE)"},{"path":"multivariate.html","id":"autovalores","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.5 Autovalores","text":"Os autovalores medem quantidade de variÃ¢ncia retida por cada componente principal. Autovalores sÃ£o grandes para os primeiros PCs e pequenos para os PCs subseqÃ¼entes. Ou seja, os primeiros PCs correspondem Ã s direÃ§Ãµes com quantidade mÃ¡xima de variaÃ§Ã£o conjunto de dados. Examinamos os autovalores para determinar o nÃºmero de componentes principais serem considerados. Os autovalores e proporÃ§Ã£o de variÃ¢ncias (isto Ã©, informaÃ§Ã£o) retidos pelos componentes principais (PCs) sÃ£o extraÃ­dos usando funÃ§Ã£o get_eigenvalue().soma de todos os autovalores resulta em uma variÃ¢ncia total de 15 proporÃ§Ã£o de variaÃ§Ã£o explicada por cada autovalor Ã© dada na segunda coluna. Por exemplo, 7.623 dividido por 15 Ã© igual 0.5082, ou, cerca de 50.82% da variaÃ§Ã£o Ã© explicada por este primeiro autovalor. porcentagem acumulada explicada Ã© obtida adicionando proporÃ§Ãµes sucessivas de variaÃ§Ã£o explicadas. Por exemplo, 50.82% mais 18.13% Ã© igual 68.95% e assim por diante. Assim, cerca de 59,627% da variaÃ§Ã£o Ã© explicada pelos dois primeiros autovalores.Um autovalor maior que 1 indica que os componentes principais sÃ£o responsÃ¡veis por mais variÃ¢ncias que contabilizadas por uma das variÃ¡veis originais nos dados padronizados. Isso Ã© comumente usado como um ponto de corte para o qual os PCs sÃ£o mantidos (Kaiser 1961). entanto, isto vale apenas quando os dados sÃ£o padronizados.Infelizmente, nÃ£o hÃ¡ maneira objetiva bem aceita de decidir quantos componentes principais sÃ£o suficientes. Em nossa anÃ¡lise, os trÃªs primeiros componentes principais explicam 83.62% da variaÃ§Ã£o. Esta Ã© uma percentagem aceitavelmente grande. Um mÃ©todo alternativo para determinar o nÃºmero de componentes principais Ã© olhar para um Scree Plot, que Ã© o grÃ¡fico de autovalores ordenados maior para o menor. O nÃºmero de componentes Ã© determinado ponto, alÃ©m qual os autovalores remanescentes sÃ£o todos relativamente pequenos.\r\nFigure 13.1: Autovalores e variÃ¢ncia acumulada na anÃ¡lise de componentes principais\r\n","code":"\neig.val <- get_eigenvalue(res.pca)\np1 <- fviz_eig(res.pca)\np2 <- fviz_eig(res.pca,\n               addlabels = TRUE,\n               geom = \"bar\",\n               barfill = \"orange\",\n               barcolor = \"black\",\n               xlab = \"Componentes Principais\",\n               ylab = \"Percentagem da variÃ¢ncia explicada\",\n               main = \"\")\nplot_grid(p1, p2)"},{"path":"multivariate.html","id":"grÃ¡fico-das-variÃ¡veis","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.6 GrÃ¡fico das variÃ¡veis","text":"Um mÃ©todo simples para extrair os resultados, para variÃ¡veis, de uma saÃ­da da funÃ§Ã£o prcomp()  Ã© usar funÃ§Ã£o get_pca_var() . Esta funÃ§Ã£o fornece uma lista de matrizes contendo todos os seguintes resultados:coord: coordenadas de variÃ¡veis para criar um grÃ¡fico de dispersÃ£o.cos2: representa qualidade da representaÃ§Ã£o para variÃ¡veis mapa de fatores. Ã‰ equivalente ao quadrado das coordenadas.contrib: contÃ©m contribuiÃ§Ãµes (em percentagem) das variÃ¡veis para os componentes principais.","code":""},{"path":"multivariate.html","id":"cÃ­rculo-de-correlaÃ§Ã£o","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.6.1 CÃ­rculo de correlaÃ§Ã£o","text":"correlaÃ§Ã£o entre uma variÃ¡vel e um componente principal (PC) Ã© usada como coordenadas da variÃ¡vel PC. Um grÃ¡fico pode ser obtido utilizando funÃ§Ã£o fviz_pca_var(), O grÃ¡fico acima mostra relaÃ§Ãµes entre todas variÃ¡veis. Ele pode ser interpretado da seguinte forma:VariÃ¡veis positivamente correlacionadas sÃ£o agrupadas.VariÃ¡veis negativamente correlacionadas sÃ£o posicionadas em lados opostos da origem grÃ¡fico (quadrantes opostos).distÃ¢ncia entre variÃ¡veis e origem mede qualidade das variÃ¡veis mapa de fatores. VariÃ¡veis que estÃ£o longe da origem (prÃ³ximas ao cÃ­rculo) estÃ£o bem representadas mapa de fatores.","code":"\np1 <- fviz_pca_var(res.pca)\np2 <- fviz_pca_var(res.pca,\n                   col.circle = \"red\",\n                   xlab = \"PCA1 (50,8%)\",\n                   ylab = \"PCA2 (18,1%)\",\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   title = \"GrÃ¡fico das variÃ¡veis\",\n                   repel = TRUE)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"representaÃ§Ã£o-e-contribuiÃ§Ã£o-das-variÃ¡veis","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.6.2 RepresentaÃ§Ã£o e contribuiÃ§Ã£o das variÃ¡veis","text":"qualidade e contribuiÃ§Ã£o das variÃ¡veis pode ser exibida mapa de fatores utilizando o argumento col.var = \"cos2\" e col.var = \"contrib\", respectivamente. Note que Ã© possÃ­vel tambÃ©m colorir variÃ¡veis utilizando uma variÃ¡vel categÃ³rica. Para fins didÃ¡ticos criaremos uma variÃ¡vel categÃ³rica com trÃªs neÃ­veis, baseado algorÃ­tmo kmeans aplicado nas coordenadas das variÃ¡veis.","code":"\np1 <- fviz_pca_var(res.pca,\n                   col.var = \"cos2\",\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   repel = TRUE)\nres.km <- kmeans(get_pca_var(res.pca)$coord, centers = 3, nstart = 25)\ngrp <- as.factor(res.km$cluster)\np2 <- fviz_pca_var(res.pca,\n                   col.var = grp,\n                   palette = c(\"green\", \"blue\", \"red\"),\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   repel = TRUE) +\n      theme(legend.position = \"bottom\")\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"grÃ¡fico-de-indivÃ­duos","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.7 GrÃ¡fico de indivÃ­duos","text":"Os resultados, para indivÃ­duos, podem ser extraÃ­dos usando funÃ§Ã£o get_pca_ind(). Similarmente ao get_pca_var(), esta funÃ§Ã£o fornece uma lista de matrizes contendo todos os resultados para os indivÃ­duos (coordenadas, correlaÃ§Ã£o entre indivÃ­duos e eixos, quadrado das distÃ¢ncias e contribuiÃ§Ãµes). funÃ§Ã£o fviz_pca_ind() Ã© usada para produzir o grÃ¡fico de indivÃ­duos.","code":"\np1 <- fviz_pca_ind(res.pca)\np2 <- fviz_pca_ind(res.pca,\n                   pointsize = \"cos2\", \n                   pointshape = 21,\n                   fill = \"#E7B800\",\n                   repel = TRUE)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"grÃ¡fico-biplot","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.3.8 GrÃ¡fico biplot","text":"Um grÃ¡fico biplot combina os marcadores de variÃ¡veis e indivÃ­duos em um mesmo grÃ¡fico. funÃ§Ã£o fviz_pca_biplot() Ã© utilizada para gerar este tipo de grÃ¡fico. O primeiro biplot (p1) Ã© confeccionado com o exemplo em res.pca. Um exemplo um pouco mais complexo serÃ¡ mostrado utilizando o banco de dados R iris. Neste exemplo, os indivÃ­duos sÃ£o coloridos por grupos (cor discreta) e variÃ¡veis por suas contribuiÃ§Ãµes para os componentes principais (cores de gradiente). AlÃ©m disso, alteraremos transparÃªncia das variÃ¡veis por suas contribuiÃ§Ãµes usando o argumento alpha.var. ","code":"\np1 <- fviz_pca_biplot(res.pca)\n\niris.pca <- prcomp(iris[,-5], scale. = TRUE)\np2 <- fviz_pca_biplot(iris.pca, \n                # indivÃ­duos\n                geom.ind = \"point\",\n                fill.ind = iris$Species, col.ind = \"black\",\n                pointshape = 21, pointsize = 2,\n                palette = \"jco\",\n                addEllipses = TRUE,\n                repel = TRUE,\n                # variÃ¡veis\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                legend.title = list(fill = \"Especies\",\n                                    color = \"ContribuiÃ§Ã£o\",\n                                    alpha = \"ContribuiÃ§Ã£o\"))\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"), rel_widths = c(0.4, 0.6))"},{"path":"multivariate.html","id":"k-means","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.4 K-means","text":"","code":""},{"path":"multivariate.html","id":"conceito-1","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.4.1 Conceito","text":"idÃ©ia bÃ¡sica por trÃ¡s agrupamento  k-means consiste em definir clusters para que variaÃ§Ã£o total dentro cluster seja minimizada. Existem vÃ¡rios algoritmos k-means disponÃ­veis. O algoritmo padrÃ£o e o mais utilizado Ã© o algoritmo de Hartigan-Wong (Hartigan Wong 1979), que define variaÃ§Ã£o total dentro cluster como soma das distÃ¢ncias Euclidianas quadrÃ¡ticas entre os itens e o centrÃ³ide correspondente. Os passos bÃ¡sicos para anÃ¡lise sÃ£o os que seguem:Especifique o nÃºmero de clusters (k) serem criados;Selecione aleatoriamente k objetos conjunto de dados como os centros de clusters iniciais ou mÃ©dias;Atribua cada observaÃ§Ã£o ao seu centrÃ³ide mais prÃ³ximo, com base na distÃ¢ncia euclidiana entre o objeto e o centrÃ³ide;Para cada um dos k clusters, atualize o centrÃ³ide cluster calculando os novos valores mÃ©dios de todos os pontos de dados cluster. O centrÃ³ide k-Ã©simo cluster Ã© um vetor de comprimento p (p = nÃºmero de variÃ¡veis) contendo mÃ©dias de todas variÃ¡veis para observaÃ§Ãµes k-Ã©simo cluster;Iterativamente, minimize soma de quadrados total dentro cluster; isto Ã©, iterar etapas 3 e 4 atÃ© que atribuiÃ§Ãµes cluster parem de mudar ou atÃ© que o nÃºmero mÃ¡ximo de iteraÃ§Ãµes sejam atingidas. Por padrÃ£o, o software R usa 10 como o valor padrÃ£o para o nÃºmero mÃ¡ximo de iteraÃ§Ãµes.","code":""},{"path":"multivariate.html","id":"pacotes-r-1","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.4.2 Pacotes R","text":"Para implementar o mÃ©todo k-means, funÃ§Ã£o kmeans() pacote stats (nativa R) serÃ¡ utilizada. Para confecÃ§Ã£o grÃ¡fico, funÃ§Ã£o fviz_cluster()  pacote factoextra serÃ¡ utilizada.","code":""},{"path":"multivariate.html","id":"formato-dos-dados-e-cÃ³digos-r","chapter":"CapÃ­tulo 13 AnÃ¡lise multivariada","heading":"13.4.3 Formato dos dados e cÃ³digos R","text":"O formato dos dados utilizados Ã© o mesmo dos utilizados para realizar anÃ¡lise de componentes principais. Assim, neste exemplo, somente criaremos um novo conjunto renomeando os dados organizados anteriormente (data_pca) para data_km. O prÃ³ximo passo Ã© ajustar o modelo k-means e armazenar em um objeto, em nosso exemplo, kmres. Posteriormente, funÃ§Ã£o fviz_cluster() Ã© utilizada para confeccionar o grÃ¡fico.","code":"\ndata_km <- data_pca\nkm.res <- kmeans(data_km, 2, nstart = 25)\np1 <- fviz_cluster(km.res, data = data_pca)\np2 <- fviz_cluster(km.res,\n                   data = data_pca,\n                   ellipse = TRUE,\n                   main = \"\",\n                   ellipse.type = \"confidence\",\n                   ggtheme = theme_metan())+\n      theme(legend.position = \"bottom\")\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"interaction.html","id":"interaction","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","text":"Uma cultura pode ser vista como um sistema complexo com resultados (por exemplo, rendimento de grÃ£os) que sÃ£o afetados por informaÃ§Ãµes genÃ©ticas, fisiolÃ³gicas, pedoclimÃ¡ticas e de manejo. Melhoristas e geneticistas se esforÃ§continuamente para aumentar produtividade das culturas visando suprir demanda mundial cada vez maior por alimentos. Ã‰ na fase final de um programa de melhoramento de plantas que muito esforÃ§o e recursos precisam ser investidos na avaliaÃ§Ã£o dos genÃ³tipos (g) serem selecionados. Geralmente algumas centenas de genÃ³tipos precisam ser avaliados em um grande nÃºmero de ambientes (e). Estes ensaios sÃ£o conhecidos como ensaios multi-ambientes () e os dados destes experimentos resultam em uma matriz M de dimensÃµes \\(g \\times e\\) . Ã‰ nesta fase processo que surge um dos maiores desafios da anÃ¡lise de : compreender interaÃ§Ã£o genÃ³tipo-vs-ambiente buscando novas formas de explorÃ¡-la e utilizÃ¡-la favor da seleÃ§Ã£o de genÃ³tipos com estabilidade  produtiva satisfatÃ³ria.FunÃ§Ãµes pacote metan, acrÃ´nimo para multi environment trial analysis serÃ£o utilizadas para anÃ¡lise de dados de ensaios multi-ambientes.  O foi desenvolvido em linguagem R e Ã© distribuÃ­sob licenÃ§GPL (General Public Licence) 3.0. Isto significa que qualquer pessoa pode: () utilizar o cÃ³digo sem nenhuma restriÃ§Ã£o/pagamento; (ii) estudar o cÃ³digo e adaptÃ¡-lo Ã s suas necessidades; (iii) sugerir modificaÃ§Ãµes/melhorias cÃ³digo de modo aperfeiÃ§oÃ¡-lo para uma comunidade maior de usuÃ¡rios, mantendo, porÃ©m, os direitos autor. O pacote metan fornece funÃ§Ãµes Ãºteis para analisar dados de ensaios multi-ambientes usando mÃ©todos paramÃ©tricos e nÃ£o paramÃ©tricos, incluindo, mas nÃ£o limitados :AnÃ¡lise grÃ¡fica da interaÃ§Ã£o genÃ³tipo-vs-ambiente;AnÃ¡lise de variÃ¢ncia individualProcedimentos de validaÃ§Ã£o cruzada para modelos da famÃ­lia AMMI e BLUP;Estimativas usando AMMI com diferentes nÃºmeros de termos multiplicativos;Ãndices de estabilidade baseados em AMMI;Biplots baseados modelo GGE;PrediÃ§Ã£o baseada em modelos de efeito misto;Ãndices de estabilidade baseados em BLUP;Componentes de variÃ¢ncia e parÃ¢metros genÃ©ticos em modelos de efeito misto;Ferramentas grÃ¡ficas para confecÃ§Ã£o de biplots.EstatÃ­sticas de estabilidade paramÃ©trica e nÃ£o paramÃ©trica.Nesta seÃ§Ã£o, usaremos o conjunto de dados data_ge disponÃ­vel pacote metan. Para mais informaÃ§Ãµes, por favor, consulte ?data_ge. Outros conjuntos de dados podem ser usados desde que seguintes colunas estejam conjunto de dados: ambiente, genÃ³tipo, bloco e variÃ¡vel(eis) resposta.","code":""},{"path":"interaction.html","id":"anÃ¡lise-grÃ¡fica-da-interaÃ§Ã£o","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.1 AnÃ¡lise grÃ¡fica da interaÃ§Ã£o","text":"funÃ§Ã£o ge_plot() pode ser usada para visualizar o desempenho genÃ³tipo atravÃ©s dos ambientes. O losango preto mostra mÃ©dia para cada ambiente.Para identificar o genÃ³tipo vencedor em cada ambiente, podemos usar funÃ§Ã£o ge_winners().Ou obtenha classificaÃ§Ã£o dos genÃ³tipos em cada ambiente.Para mais detalhes sobre os testes, podemos usar ge_details()","code":"\na <- ge_plot (data_ge, ENV, GEN, GY)\nb <- ge_plot (data_ge, ENV, GEN, GY) + ggplot2::coord_flip ()\narrange_ggplot(a, b)\nge_winners(data_ge2, ENV, GEN, resp = everything())\n# # A tibble: 4 x 16\n#   ENV   PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR   CDED \n#   <fct> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n# 1 A1    H3    H1    H1    H6    H6    H8    H6    H6    H6    H2    H4    H8   \n# 2 A2    H2    H1    H1    H6    H2    H2    H6    H2    H2    H2    H6    H13  \n# 3 A3    H13   H13   H6    H4    H13   H6    H2    H7    H13   H13   H4    H6   \n# 4 A4    H5    H5    H10   H7    H11   H5    H7    H5    H7    H11   H9    H10  \n# # ... with 3 more variables: PERK <chr>, TKW <chr>, NKE <chr>\nge_winners(data_ge2, ENV, GEN, resp = everything (), type = \"ranks\")\n# # A tibble: 52 x 16\n#    ENV   PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR   CDED \n#    <fct> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#  1 A1    H3    H1    H1    H6    H6    H8    H6    H6    H6    H2    H4    H8   \n#  2 A1    H9    H4    H10   H11   H13   H9    H11   H8    H13   H13   H5    H9   \n#  3 A1    H4    H9    H4    H10   H10   H6    H9    H9    H9    H3    H6    H7   \n#  4 A1    H5    H10   H7    H4    H9    H10   H10   H7    H2    H7    H11   H12  \n#  5 A1    H2    H7    H12   H5    H8    H13   H5    H5    H1    H12   H3    H11  \n#  6 A1    H10   H5    H11   H9    H2    H7    H4    H13   H4    H8    H2    H10  \n#  7 A1    H7    H3    H6    H3    H3    H12   H8    H4    H3    H6    H1    H6   \n#  8 A1    H13   H11   H9    H7    H1    H11   H3    H12   H8    H10   H13   H13  \n#  9 A1    H6    H13   H13   H1    H7    H5    H7    H10   H5    H4    H8    H5   \n# 10 A1    H11   H6    H5    H12   H4    H1    H1    H3    H10   H1    H12   H1   \n# # ... with 42 more rows, and 3 more variables: PERK <chr>, TKW <chr>, NKE <chr>\nge_details(data_ge2, ENV, GEN, resp = everything())\n# # A tibble: 10 x 16\n#    Parameters PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR  \n#    <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#  1 Mean       \"2.4~ \"1.3~ \"0.5~ \"15.~ \"49.~ \"29.~ \"15.~ \"24.~ \"172~ \"16.~ \"32.~\n#  2 SE         \"0.0~ \"0.0~ \"0\"   \"0.1\" \"0.2~ \"0.1~ \"0.0~ \"0.5\" \"2.6~ \"0.1~ \"0.2~\n#  3 SD         \"0.3~ \"0.2~ \"0.0~ \"1.2~ \"2.7~ \"2.3\" \"1.1~ \"6.2~ \"32.~ \"1.6~ \"3.4~\n#  4 CV         \"13.~ \"21.~ \"10.~ \"8.2~ \"5.5~ \"7.9~ \"7.3~ \"25.~ \"18.~ \"10.~ \"10.~\n#  5 Min        \"1.7~ \"0.7~ \"0.3~ \"11.~ \"43.~ \"23.~ \"12.~ \"11.~ \"105~ \"12.~ \"23.~\n#  6 Max        \"3.0~ \"1.8~ \"0.6~ \"17.~ \"54.~ \"34.~ \"18.~ \"38.~ \"250~ \"21.~ \"42 ~\n#  7 MinENV     \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~\n#  8 MaxENV     \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~\n#  9 MinGEN     \"H10~ \"H8 ~ \"H8 ~ \"H12~ \"H9 ~ \"H12~ \"H12~ \"H11~ \"H9 ~ \"H9 ~ \"H12~\n# 10 MaxGEN     \"H1 ~ \"H1 ~ \"H1 ~ \"H6 ~ \"H6 ~ \"H6 ~ \"H5 ~ \"H5 ~ \"H6 ~ \"H13~ \"H4 ~\n# # ... with 4 more variables: CDED <chr>, PERK <chr>, TKW <chr>, NKE <chr>"},{"path":"interaction.html","id":"anÃ¡lise-de-variÃ¢ncia-individual","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.2 AnÃ¡lise de variÃ¢ncia individual","text":"funÃ§Ã£o anova_ind()  pode ser utilziada para realizar uma anÃ¡lise de variÃ¢ncia para cada ambiente, conforme o seguinte cÃ³digo.","code":"\nind <- anova_ind(data_ge, ENV, GEN, REP, GY)\nprint(ind$GY$individual)\n# # A tibble: 14 x 12\n#    ENV    MEAN   MSG   FCG     PFG    MSB    FCB     PFB    MSE    CV    h2\n#    <chr> <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <dbl> <dbl>\n#  1 E1     2.52 0.337  2.34 5.94e-2 0.0652  0.453 6.43e-1 0.144  15.1  0.573\n#  2 E10    2.18 0.296 11.1  1.10e-5 0.654  24.5   7.28e-6 0.0267  7.51 0.910\n#  3 E11    1.37 0.151  1.44 2.44e-1 0.377   3.59  4.86e-2 0.105  23.7  0.304\n#  4 E12    1.61 0.320  5.98 6.47e-4 0.0919  1.72  2.08e-1 0.0535 14.4  0.833\n#  5 E13    2.91 0.713  7.18 2.10e-4 0.0767  0.772 4.77e-1 0.0994 10.8  0.861\n#  6 E14    1.78 0.131  1.73 1.53e-1 0.104   1.37  2.78e-1 0.0753 15.4  0.423\n#  7 E2     3.18 0.207  1.16 3.76e-1 0.698   3.91  3.88e-2 0.179  13.3  0.136\n#  8 E3     4.06 0.335  1.87 1.23e-1 0.489   2.73  9.21e-2 0.179  10.4  0.466\n#  9 E4     3.68 0.531  3.86 7.12e-3 0.116   0.846 4.46e-1 0.138  10.1  0.741\n# 10 E5     3.91 0.526  7.93 1.10e-4 0.219   3.30  6.02e-2 0.0664  6.59 0.874\n# 11 E6     2.66 0.135  2.30 6.35e-2 0.160   2.73  9.22e-2 0.0586  9.09 0.565\n# 12 E7     1.99 0.337  3.70 8.73e-3 0.381   4.19  3.22e-2 0.0910 15.2  0.730\n# 13 E8     2.54 0.215  7.72 1.31e-4 0.817  29.4   2.15e-6 0.0278  6.57 0.870\n# 14 E9     3.06 0.679  6.12 5.62e-4 0.583   5.25  1.60e-2 0.111  10.9  0.837\n# # ... with 1 more variable: AS <dbl>"},{"path":"interaction.html","id":"baseada-em-regressÃ£o","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.3 Baseada em regressÃ£o","text":"Eberhart Russell (1966) popularizaram anÃ¡lise de estabilidade baseada em regressÃ£o. Nesse procedimento, anÃ¡lise de adaptabilidade e estabilidade Ã© realizada por meio de ajustes de equaÃ§Ãµes de regressÃ£o onde variÃ¡vel dependente Ã© estimada em funÃ§Ã£o de um Ã­ndice ambiental, conforme o seguinte modelo:\\[\r\n\\mathop Y\\nolimits_{ij}  = {\\beta _{0i}} + {\\beta _{1i}}{I_j} + {\\delta _{ij}} + {\\bar \\varepsilon _{ij}}\r\n\\]onde \\({\\beta _{0i}}\\) Ã© mÃ©dia geral genÃ³tipo (= 1, 2, â€¦, ); \\({\\beta _{1i}}\\) Ã© respota linear genÃ³tipo ao Ã­ndice ambiental; Ij Ã© o Ã­ndice ambiental (j = 1, 2, â€¦, e), onde \\({I_j} = [(y_{.j}/g)- (y_{..}/ge)]\\), \\({\\delta _{ij}}\\) Ã© o desvio da regressÃ£o, e \\({\\bar \\varepsilon _{ij}}\\) Ã© o erro experimental.\r\nO modelo Ã© ajustado com funÃ§Ã£o ge_reg() . Os mÃ©todos S3 plot() e summary() podem ser utilizados para explorar os resultados.","code":"\nreg_model <- ge_reg(data_ge, ENV, GEN, REP, GY)\nreg_model$GY$anova\n# # A tibble: 17 x 6\n#    SV                       Df `Sum Sq` `Mean Sq` `F value`  `Pr(>F)`\n#    <chr>                 <dbl>    <dbl>     <dbl>     <dbl>     <dbl>\n#  1 \"Total\"                 139  324.       2.33      NA     NA       \n#  2 \"GEN\"                     9   13.0      1.44       6.28   3.05e- 7\n#  3 \"ENV + (GEN x ENV)\"     130  311.       2.39      NA     NA       \n#  4 \"ENV (linear)\"            1  280.     280.        NA     NA       \n#  5 \" GEN x ENV (linear)\"     9    3.61     0.402      1.75   8.58e- 2\n#  6 \"Pooled deviation\"      120   27.6      0.230     NA     NA       \n#  7 \"G1\"                     12    1.11     0.0924     1.06   3.92e- 1\n#  8 \"G10\"                    12    7.54     0.629      7.22   1.66e-11\n#  9 \"G2\"                     12    2.95     0.246      2.82   1.14e- 3\n# 10 \"G3\"                     12    0.699    0.0582     0.669  7.81e- 1\n# 11 \"G4\"                     12    2.23     0.186      2.14   1.48e- 2\n# 12 \"G5\"                     12    1.49     0.124      1.42   1.55e- 1\n# 13 \"G6\"                     12    1.27     0.106      1.22   2.71e- 1\n# 14 \"G7\"                     12    3.25     0.270      3.11   3.72e- 4\n# 15 \"G8\"                     12    2.54     0.211      2.43   5.15e- 3\n# 16 \"G9\"                     12    4.54     0.378      4.34   2.42e- 6\n# 17 \"Pooled error\"          280   24.4      0.0870    NA     NA\nreg_model$GY$regression\n# # A tibble: 10 x 6\n#    GEN       Y slope deviations  RMSE    R2\n#    <chr> <dbl> <dbl>      <dbl> <dbl> <dbl>\n#  1 G1     2.60 1.06    -0.00142 0.162 0.966\n#  2 G10    2.47 1.12     0.177   0.424 0.823\n#  3 G2     2.74 1.05     0.0497  0.265 0.913\n#  4 G3     2.96 1.03    -0.0128  0.129 0.977\n#  5 G4     2.64 0.937    0.0298  0.231 0.917\n#  6 G5     2.54 0.887    0.00902 0.188 0.937\n#  7 G6     2.53 0.861    0.00304 0.174 0.942\n#  8 G7     2.74 0.819    0.0579  0.278 0.852\n#  9 G8     3.00 1.03     0.0382  0.246 0.922\n# 10 G9     2.51 1.19     0.0938  0.329 0.897\nplot(reg_model)"},{"path":"interaction.html","id":"Ã­ndice-de-confianÃ§a-genotÃ­pico","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.4 Ãndice de confianÃ§a genotÃ­pico","text":"Annicchiarico (1992) propÃ´s um mÃ©todo de estabilidade em que o parÃ¢metro de estabilidade Ã© medido pela superioridade genÃ³tipo em relaÃ§Ã£o Ã  mÃ©dia de cada ambiente, de acordo com o seguinte modelo:\\[\r\n{Z_ {ij}} = \\frac{{{Y_ {}}}} {{{{\\bar Y} _ {. J}}}} \\times 100\r\n\\]O Ã­ndice de confianÃ§genotÃ­pico genÃ³tipo (\\(W_i\\)) Ã© entÃ£o estimado da seguinte forma:\\[\r\nW_i = Z_{.} / E - \\alpha \\times sd (Z_{.})\r\n\\]Onde \\(\\alpha\\) Ã© o quantil da distribuiÃ§Ã£o normal padrÃ£o uma dada probabilidade de erro (\\(\\alpha \\approx 1.64\\) 0.05). O mÃ©todo Ã© implementado usando funÃ§Ã£o Annicchiarico() . O Ã­ndice de confianÃ§Ã© estimado considerando todos os ambientes, os ambientes favorÃ¡veis (Ã­ndice positivo) e os ambientes desfavorÃ¡veis (Ã­ndice negativo), como segue:","code":"\nann <- Annicchiarico(data_ge, ENV, GEN, REP, GY)\nann$GY$general\n# # A tibble: 10 x 6\n#    GEN       Y Mean_rp Sd_rp    Wi  rank\n#    <chr> <dbl>   <dbl> <dbl> <dbl> <dbl>\n#  1 G1     2.60    96.5  7.38  91.5     6\n#  2 G10    2.47    90.3 18.9   77.5    10\n#  3 G2     2.74   103.  12.1   94.5     4\n#  4 G3     2.96   111.   4.59 108.      1\n#  5 G4     2.64    99.1  8.03  93.7     5\n#  6 G5     2.54    95.5  7.74  90.2     8\n#  7 G6     2.53    95.5  7.61  90.4     7\n#  8 G7     2.74   105.  12.5   96.0     3\n#  9 G8     3.00   113.   8.87 107.      2\n# 10 G9     2.51    91.6 13.8   82.3     9"},{"path":"interaction.html","id":"Ã­ndice-de-superioridade-genotÃ­pico","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.5 Ãndice de superioridade genotÃ­pico","text":"funÃ§Ã£o superiority() implementa o mÃ©todo nÃ£o-paramÃ©trico proposto por Lin Binns (1988), que considera que medida de superioridade geral da cultivar para dados de cultivar x localizaÃ§Ã£o Ã© definida como quadrado mÃ©dio da distÃ¢ncia entre resposta da cultivar e mÃ©dia de resposta mÃ¡xima em todas localidades, de acordo com o seguinte modelo.\\[\r\nP_i = \\sum \\limits_{j = 1} ^ n {(y_ {ij} - y _ {. J}) ^ 2 / (2n)}\r\n\\]\r\nonde n Ã© o nÃºmero de ambientes. Da mesma forma que o Ã­ndice de confianÃ§genotÃ­pico, o Ã­ndice de superioridade Ã© calculado por todos os ambientes, para os favorÃ¡veis e para os desfavorÃ¡veis.","code":"\nsuper <- superiority(data_ge, ENV, GEN, GY)\nsuper$GY$index\n# # A tibble: 10 x 8\n#    GEN       Y   Pi_a   R_a   Pi_f   R_f    Pi_u   R_u\n#    <chr> <dbl>  <dbl> <dbl>  <dbl> <dbl>   <dbl> <dbl>\n#  1 G1     2.60 0.169      5 0.228      4 0.125       6\n#  2 G10    2.47 0.344     10 0.475     10 0.245      10\n#  3 G2     2.74 0.126      3 0.149      3 0.108       5\n#  4 G3     2.96 0.0410     1 0.0723     1 0.0175      2\n#  5 G4     2.64 0.173      6 0.289      5 0.0853      4\n#  6 G5     2.54 0.240      8 0.382      8 0.133       7\n#  7 G6     2.53 0.238      7 0.377      7 0.134       8\n#  8 G7     2.74 0.149      4 0.318      6 0.0214      3\n#  9 G8     3.00 0.0412     2 0.0882     2 0.00588     1\n# 10 G9     2.51 0.291      9 0.390      9 0.217       9"},{"path":"interaction.html","id":"estratificaÃ§Ã£o-ambiental","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.6 EstratificaÃ§Ã£o ambiental","text":"Um mÃ©todo que combina anÃ¡lise de estabilidade e estratificaÃ§Ã£o ambiental usando anÃ¡lise fatorial foi proposto por Murakami Cruz (2004). Este mÃ©todo Ã© implementado com funÃ§Ã£o ge_factanal(), como segue:maneira mais fÃ¡cil de calcular os Ã­ndices de estabilidade acima mencionados Ã© usando funÃ§Ã£o ge_stats(). Se vocÃª deseja exportar um resumo dos resultados, maneira mais simples Ã© usando funÃ§Ã£o summary().Este comando criarÃ¡ um arquivo de texto chamado ge_stats summary.txt  diretÃ³rio de trabalho atual.","code":"\nfato <- ge_factanal(data_ge, ENV, GEN, REP, GY)\nplot(fato)\nprint(fato$GY$PCA)\n# # A tibble: 14 x 4\n#    PCA   Eigenvalues  Variance Cumul_var\n#    <chr>       <dbl>     <dbl>     <dbl>\n#  1 PC1      5.60e+ 0  4.00e+ 1      40.0\n#  2 PC2      2.51e+ 0  1.79e+ 1      58.0\n#  3 PC3      2.41e+ 0  1.72e+ 1      75.1\n#  4 PC4      1.37e+ 0  9.80e+ 0      84.9\n#  5 PC5      1.13e+ 0  8.05e+ 0      93.0\n#  6 PC6      4.87e- 1  3.48e+ 0      96.5\n#  7 PC7      3.03e- 1  2.16e+ 0      98.6\n#  8 PC8      1.29e- 1  9.25e- 1      99.6\n#  9 PC9      6.24e- 2  4.46e- 1     100  \n# 10 PC10     1.72e-16  1.23e-15     100  \n# 11 PC11     1.41e-16  1.01e-15     100  \n# 12 PC12    -1.74e-16 -1.25e-15     100  \n# 13 PC13    -2.67e-16 -1.91e-15     100  \n# 14 PC14    -3.01e-16 -2.15e-15     100\nprint(fato$GY$FA)\n# # A tibble: 14 x 8\n#    Env        FA1     FA2      FA3     FA4     FA5 Communality Uniquenesses\n#    <chr>    <dbl>   <dbl>    <dbl>   <dbl>   <dbl>       <dbl>        <dbl>\n#  1 E1    -0.881    0.327   0.00927 -0.0631  0.274        0.963      0.0369 \n#  2 E10   -0.942   -0.158  -0.0820   0.113   0.174        0.962      0.0380 \n#  3 E11   -0.929   -0.233  -0.0336  -0.242   0.110        0.989      0.0111 \n#  4 E12   -0.848    0.135   0.0263   0.0941  0.241        0.805      0.195  \n#  5 E13   -0.940    0.108  -0.0842  -0.0637 -0.235        0.961      0.0391 \n#  6 E14   -0.150   -0.123  -0.916   -0.0872  0.265        0.954      0.0463 \n#  7 E2    -0.198   -0.0521 -0.126   -0.969   0.0328       0.997      0.00266\n#  8 E3    -0.0806   0.910   0.341   -0.0173 -0.110        0.963      0.0370 \n#  9 E4     0.209    0.543  -0.272   -0.728  -0.120        0.957      0.0433 \n# 10 E5    -0.777    0.392  -0.269   -0.0470 -0.267        0.904      0.0963 \n# 11 E6    -0.524    0.569  -0.309   -0.174  -0.238        0.781      0.219  \n# 12 E7    -0.244    0.342  -0.520    0.297   0.619        0.918      0.0820 \n# 13 E8     0.00161 -0.0589 -0.914   -0.226  -0.143        0.911      0.0891 \n# 14 E9    -0.0794  -0.291  -0.0183  -0.0539  0.927        0.954      0.0463\nprint(fato$GY$env_strat)\n# # A tibble: 14 x 6\n#    Env   Factor  Mean   Min   Max    CV\n#    <chr> <chr>  <dbl> <dbl> <dbl> <dbl>\n#  1 E1    FA1     2.52 1.97   2.90 13.3 \n#  2 E10   FA1     2.18 1.54   2.57 14.4 \n#  3 E11   FA1     1.37 0.899  1.68 16.4 \n#  4 E12   FA1     1.61 1.02   2    20.3 \n#  5 E13   FA1     2.91 1.83   3.52 16.8 \n#  6 E5    FA1     3.91 3.37   4.81 10.7 \n#  7 E3    FA2     4.06 3.43   4.57  8.22\n#  8 E6    FA2     2.66 2.34   2.98  7.95\n#  9 E14   FA3     1.78 1.43   2.06 11.7 \n# 10 E8    FA3     2.54 2.05   2.88 10.5 \n# 11 E2    FA4     3.18 2.61   3.61  8.25\n# 12 E4    FA4     3.68 3.02   4.27 11.5 \n# 13 E7    FA5     1.99 1.39   2.55 16.8 \n# 14 E9    FA5     3.06 1.94   3.72 15.6\nstat_ge <- ge_stats(data_ge, ENV, GEN, REP, GY)\nsummary(stat_ge, export = TRUE)"},{"path":"interaction.html","id":"o-modelo-ammi","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7 O modelo AMMI","text":"O modelo linear mais simples com efeito de interaÃ§Ã£o usado na anÃ¡lise de EMA Ã©\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + \\mathop \\alpha \\nolimits_i  + \\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + \\mathop \\gamma \\nolimits_{jk}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) Ã© variÃ¡vel resposta observada k-Ã©simo bloco -Ã©simo genÃ³tipo j-Ã©simo ambiente (= 1, 2, â€¦, g; j = 1, 2, â€¦, e; k = 1, 2, â€¦, b); \\(\\mu\\) Ã© mÃ©dia geral; \\(\\mathop\\alpha\\nolimits_i\\) Ã© o efeito principal genÃ³tipo ; \\(\\mathop \\tau \\nolimits_j\\) Ã© o principal efeito ambiente j; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) Ã© o efeito de interaÃ§Ã£o genÃ³tipo com o ambiente j; \\(\\mathop \\gamma \\nolimits_{jk}\\) Ã© o efeito bloco k ambiente j; e \\({\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\\) Ã© o erro aleatÃ³rio assumindo \\(..d \\sim N(0, \\sigma^2 )\\).MÃ©todos que combinam diferentes princÃ­pios estatÃ­sticos ganharam espaÃ§o na anÃ¡lise de  por volta da dÃ©cada 1960, com destaque especial ao estudo de Gollob (1968), que propÃ´s um mÃ©todo que combina os benefÃ­cios da anÃ¡lise de fatores e anÃ¡lise de variÃ¢ncia em um Ãºnico mÃ©todo para estudar estabilidade . Naquela Ã©poca este mÃ©todo era conhecido como FANOVA. Atualmente este mesmo mÃ©todo foi popularizado por Gauch (1988) com o acrÃ´nimo AMMI.anÃ¡lise AMMI utiliza anÃ¡lise aditiva de variÃ¢ncia aos fatores principais (genÃ³tipo e ambiente) e decomposiÃ§Ã£o por valores singulares ao residual modelo aditivo, isto Ã©, o efeito da interaÃ§Ã£o genÃ³tipo-vs-ambiente somado ao erro experimental. Esta matriz dos efeitos nÃ£o aditivos, entÃ£o, pode ser aproximadamente exibida por meio de biplots  Gabriel (1971). Este mÃ©todo tem ganhado destaque nas Ãºltimas dÃ©cadas, principalmente devido rÃ¡pida evoluÃ§Ã£o computacional, o que tornou possÃ­vel complexas decomposiÃ§Ãµes de matrizes de alta ordem.De posse de uma matriz de dupla entrada oriunda de ensaios multiambientes, estimativa da variÃ¡vel resposta -Ã©simo genÃ³tipo j-Ã©simo ambiente Ã© obtida utilizando AMMI de acordo com o seguinte modelo:\\[\r\n{y_{ij}} = \\mu  + {\\alpha_i} + {\\tau_j} + \\sum\\limits_{k = 1}^k {{\\lambda _k}{a_{ik}}} {t_{jk}} + {\\rho _{ij}} + {\\varepsilon _{ij}}\r\n\\]onde \\({\\lambda_k}\\) Ã© o valor singular para o k-Ã©simo eixo componente principal; \\(a_{ik}\\) Ã© o -Ã©simo elemento k-Ã©simo autovetor de genÃ³tipos; \\(t_{jk}\\) Ã© o j-Ã©simo elemento k-Ã©simo autovetor de ambientes. Um resÃ­duo \\(\\rho _{ij}\\) permanece, se todos os k-PCAs nÃ£o sÃ£o considerados, onde k = \\(min(G-1; E-1)\\).","code":""},{"path":"interaction.html","id":"ajuste-do-modelo","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7.1 Ajuste do modelo","text":"O modelo AMMI Ã© ajustado com funÃ§Ã£o performs_ammi(). O primeiro argumento Ã© os dados, nosso exemplo data_ge. Os prÃ³ximos argumentos (ENV, GEN e REP) sÃ£o os nomes das colunas que contÃ©m os nÃ­veis dos fatores ambiente, genÃ³tipo, repetiÃ§Ã£o, respectivamente. argumento resp sÃ£o declaradas variÃ¡veis resposta. Uma Ãºnica variÃ¡vel pode ser analizada (como em nosso exemplo) ou, um vetor de variÃ¡veis, usando, por exemplo resp = c(GY, HM).Note que os argumentos inseridos na funÃ§Ã£o obedecem ordem dos argumentos requiridos na funÃ§Ã£o [veja args(waas)]. Se obedecida esta ordem de avaliaÃ§Ã£o, nÃ£o Ã© necessÃ¡rio declarar qual argumento estÃ¡ sendo inserido. Por exemplo, se mudÃ¡ssemos ordem de entrada, terÃ­amos um cÃ³digo semelhante waas(data_ge, gen = GEN, env =  ENV, REP, resp = c(PH, ED, TKW, NKR)).","code":"\n\nAMMI_model <- performs_ammi(data_ge, ENV, GEN, REP, GY)\n# variable GY \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df  Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV  13 279.574 21.5057   62.33 0.00e+00          .           .\n#   REP(ENV)  28   9.662  0.3451    3.57 3.59e-08          .           .\n#        GEN   9  12.995  1.4439   14.93 2.19e-19          .           .\n#    GEN:ENV 117  31.220  0.2668    2.76 1.01e-11          .           .\n#        PC1  21  10.749  0.5119    5.29 0.00e+00       34.4        34.4\n#        PC2  19   9.924  0.5223    5.40 0.00e+00       31.8        66.2\n#        PC3  17   4.039  0.2376    2.46 1.40e-03       12.9        79.2\n#        PC4  15   3.074  0.2049    2.12 9.60e-03        9.8          89\n#        PC5  13   1.446  0.1113    1.15 3.18e-01        4.6        93.6\n#        PC6  11   0.932  0.0848    0.88 5.61e-01          3        96.6\n#        PC7   9   0.567  0.0630    0.65 7.53e-01        1.8        98.4\n#        PC8   7   0.362  0.0518    0.54 8.04e-01        1.2        99.6\n#        PC9   5   0.126  0.0252    0.26 9.34e-01        0.4         100\n#  Residuals 252  24.367  0.0967      NA       NA          .           .\n#      Total 536 389.036  0.7258      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!"},{"path":"interaction.html","id":"analise-residual","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7.2 Analise residual","text":"O pacote metan conta com uma opÃ§Ã£o para anÃ¡lise residual modelo AMMI ajustado. GrÃ¡ficos podem ser obtidos utilizando o seguinte comando.figura acima, obtida com funÃ§Ã£o autoplot(), mostra 4 grÃ¡ficos. Os dois primeiros sÃ£o os mais importantes. O primeiro (Residual vs fitted) pode ser utilizado para identificar homogeneidade das variÃ¢ncias. Uma distribuiÃ§Ã£o aleatÃ³ria dos pontos grÃ¡fico deve ser observada. Quando um padrÃ£o de distibuiÃ§Ã£o Ã© observado â€“como, por exemplo, distribuiÃ§Ã£o dos pontos em forma de funilâ€“ uma investigaÃ§Ã£o deve ser realizada, pois este padrÃ£o indica possiblidade de heterogeneidade das variÃ¢ncias. O segundo grÃ¡fico (Normal Q-Q) nos informa quanto normalidade dos resÃ­duos, ou seja, Ã© desejado que os pontos sejam distribuÃ­dos ao redor da linha diagonal.","code":"\nplot(AMMI_model)"},{"path":"interaction.html","id":"escolha-do-nÃºmero-de-termos-multiplicativos","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7.3 Escolha do nÃºmero de termos multiplicativos","text":"Conforme jÃ¡ discutido, anÃ¡lise AMMI aplica tÃ©cnica de decomposiÃ§Ã£o por valores singulares na matriz dos efeitos nÃ£o aditivos modelo (). Logo, esta matriz pode ser aproximada pela pelo seguinte modelo: \\(= U \\lambda V^T\\), onde onde U Ã© uma matriz g \\(\\times\\) e contendo os vetores singulares de \\(AA^T\\) e formam base ortonormal para os efeitos de genÃ³tipos; \\(V^T\\) Ã© uma matriz e \\(\\times\\) e que contÃ©m os vetores singulares de \\(\\mathbf{^TA}\\) e formam base ortonormal para os efeitos de ambientes; e \\(\\lambda\\) Ã© uma matriz diagonal e \\(\\times\\) e contendo k-valores singulares de \\(^TA\\) , onde k = \\(min(G-1; E-1)\\). Assim, diferentes modelos (dependendo nÃºmero de termos multiplicativos utilizados) podem ser utilziados para predizer o rendimento genÃ³tipo ambiente j. tabela abaixo mostra os possÃ­veis modelos. modelo AMMI0 apenas os efeitos aditivos sÃ£o considerados. modelo AMMI1, o primeiro termo multiplicativo Ã© considerado, e assim por diante, atÃ© o modelo AMMIF, onde \\(min(G-1;E-1)\\) termos sÃ£o considerados.escolha nÃºmero de termos multiplicativos ser utilizado Ã© baseada em basicamente dois critÃ©rios de sucesso de anÃ¡lise: Postdiscritive sucess e Predictive sucess. Por definiÃ§Ã£o, Predictive sucess significa literalmente afirmaÃ§Ã£o prÃ©via que acontecerÃ¡ em algum momento futuro. Neste contexto, testes de validaÃ§Ã£o cruzada (cross-validation) podem ser utilizadas para avaliar o sucesso preditivo dos membros de modelos da familia AMMI (T. Olivoto, LÃºcio, Da silva, Marchioro, et al. 2019). Por outro lado, Postdiscritive sucess significa fazer uma afirmaÃ§Ã£o ou deduÃ§Ã£o sobre algo que aconteceu passado. Na escolha nÃºmero de termos multiplicativos da anÃ¡lise AMMI este sucesso pode ser calculado utilizando testes como o proposto por Gollob (1968).","code":""},{"path":"interaction.html","id":"postdiscritive-sucess","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7.3.1 Postdiscritive sucess","text":"objeto anova  gerado pela funÃ§Ã£o waas()  testes de hipÃ³teses sÃ£o realizados e probabilidades de erro sÃ£o atribuÃ­das para cada modelo considerando distribuiÃ§Ã£o de graus de liberdade proposto por Gollob (1968). Assim Ã© possÃ­vel identificar qual Ã© o nÃºmero ideal de termos ser considerado na prediÃ§Ã£o. Em nosso exemplo, dois termos foram significativos 5% de probabilidade de erro.","code":""},{"path":"interaction.html","id":"predictive-sucess","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.7.3.2 Predictive sucess","text":"O pacote metan fornece uma soluÃ§Ã£o completa para validaÃ§ao cruzada modelo AMMI. Utilizando funÃ§Ã£o cv_ammif() , por exemplo, Ã© possÃ­vel realizar um teste de cross-validation para famÃ­lia de modelos AMMI (AMMI0-AMMIF) usando dados com repetiÃ§Ãµes. Automaticamente, primeira validaÃ§Ã£o Ã© realizada considerando AMMIF (todos possÃ­veis IPCAs sÃ£o usados). Considerando esse modelo, o conjunto de dados original Ã© dividido em dois conjuntos de dados: dados de modelagem e dados de validaÃ§Ã£o. O conjunto de dados â€œmodelagemâ€ possui todas combinaÃ§Ãµes (genÃ³tipo vs ambiente) com R-1 repetiÃ§Ãµes. O conjunto de dados â€œvalidaÃ§Ã£oâ€ tem uma repetiÃ§Ã£o. O diagrama abaixo representa o procedimento realizado.divisÃ£o conjunto de dados em dados de modelagem e validaÃ§Ã£o depende design informado. Considerando um delineamento de blocos completos casualizados (DBC), blocos completos sÃ£o aleatoriamente selecionados dentro de ambientes, como mostrado por T. Olivoto, LÃºcio, Da silva, Marchioro, et al. (2019). O bloco restante serve dados de validaÃ§Ã£o. Se design = \"CRD\" informado, assim declarando que um delineamento intericamente casualizado (DIC)  foi usado, observaÃ§Ãµes sÃ£o aleatoriamente selecionadas para cada tratamento (combinaÃ§Ã£o genÃ³tipo-vs-ambiente). Este Ã© o mesmo procedimento sugerido por Gauch (1988). Os valores estimados para o membro da famÃ­lia AMMI em estudo sÃ£o entÃ£o comparados com os dados de â€œvalidaÃ§Ã£oâ€ e um erro de prediÃ§Ã£o \\(\\hat{z}_{ij}\\) Ã© estimado para cada tratamento. raiz quadrada quadrado mÃ©dio da diferenÃ§de prediÃ§Ã£o (RMSPD) Ã© calculado. Este procedimento Ã© repetido n vezes, utilizando o argumento nboot = n. Ao final procedimento, o algorÃ­timo armazena n estimativas RMSPD para o modelo em questÃ£o, e um novo modelo Ã© entÃ£o testado seguindo os mesmos passos.Como exemplo, funÃ§Ã£o cv_ammi() Ã© usada para calcular um procedimento de validaÃ§Ã£o cruzada para os modelos AMMI0, AMMI2 e AMMIF (9 eixos).AMMI0 para AMMIF\r\nfunÃ§Ã£o cv_ammif() Ã© usada para calcular um procedimento de validaÃ§Ã£o cruzada para todos os membros modelo da famÃ­lia AMMI. Nesse caso, AMMI0-AMMI9.ValidaÃ§Ã£o cruzada para previsÃ£o de BLUP\r\nfunÃ§Ã£o cv_blup () fornece uma validaÃ§Ã£o cruzada de dados baseados em replicaÃ§Ã£o usando modelos mistos. Por padrÃ£o, blocos completos sÃ£o selecionados aleatoriamente para cada ambiente. Usando o argumento `random â€™, Ã© possÃ­vel escolher os efeitos aleatÃ³rios modelo, como mostrado abaixo.ValidaÃ§Ã£o cruzada para previsÃ£o de BLUP\r\nfunÃ§Ã£o cv_blup () fornece uma validaÃ§Ã£o cruzada de dados baseados em replicaÃ§Ã£o usando modelos mistos. Por padrÃ£o, blocos completos sÃ£o selecionados aleatoriamente para cada ambiente. Usando o argumento `random â€™, Ã© possÃ­vel escolher os efeitos aleatÃ³rios modelo, como mostrado abaixo.GenÃ³tipo e genÃ³tipo versus ambiente como efeitos aleatÃ³riosGenÃ³tipo e genÃ³tipo versus ambiente como efeitos aleatÃ³riosAmbiente, replicaÃ§Ã£o dentro ambiente e interaÃ§Ã£o como efeitos aleatÃ³riosUm modelo aleatÃ³rio (todos os termos como efeitos aleatÃ³rios)","code":"\nAMMI0 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 0) # AMMI0\nAMMI2 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 2) # AMMI2\nAMMI9 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 9) # AMMIF\nAMMIF <- cv_ammif(data_ge, ENV, GEN, REP, GY)\nBLUP_g <- cv_blup (dados_ge, ENV, GEN, REP, GY, random = \"gen\")\nBLUP_e <- cv_blup (dados_ge, ENV, GEN, REP, GY, random = \"env\")\nBLUP_ge <- cv_blup (dados_ge, ENV, GEN, REP, GY, random = \"all\")"},{"path":"interaction.html","id":"imprimindo-os-meios-das-estimativas-rmspd","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.8 Imprimindo os meios das estimativas RMSPD","text":"tabela acima mostra estatÃ­sticas descritivas (mÃ©dia, desvio padrÃ£o, erro padrÃ£o da mÃ©dia e quantis 2,5% e 97,5%) das 100 estimativas RMSPD para cada modelo, e sÃ£o apresentadas partir modelo mais preciso (menor mÃ©dia RMSPD) para o modelo menos preciso (maior mÃ©dia RMSPD).","code":"\nbind_mod <- bind_cv(AMMIF, BLUP_g, bind = \"means\")\nprint(bind_mod$RMSPD)\n# # A tibble: 11 x 6\n#    MODEL        mean     sd      se  Q2.5 Q97.5\n#    <fct>       <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#  1 BLUP_g_RCBD 0.405 0.0249 0.00176 0.357 0.453\n#  2 AMMI2       0.411 0.0242 0.00171 0.369 0.460\n#  3 AMMI4       0.416 0.0234 0.00166 0.373 0.462\n#  4 AMMI3       0.416 0.0215 0.00152 0.377 0.457\n#  5 AMMI5       0.422 0.0232 0.00164 0.377 0.464\n#  6 AMMI6       0.422 0.0219 0.00155 0.381 0.462\n#  7 AMMI7       0.425 0.0199 0.00141 0.381 0.458\n#  8 AMMI8       0.427 0.0204 0.00145 0.387 0.462\n#  9 AMMIF       0.429 0.0213 0.00150 0.390 0.469\n# 10 AMMI1       0.430 0.0251 0.00178 0.384 0.479\n# 11 AMMI0       0.430 0.0283 0.00200 0.370 0.485"},{"path":"interaction.html","id":"plotagem-dos-valores-rmspd","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9 Plotagem dos valores RMSPD","text":"Os valores das estimativas RMSPD obtidas processo de validaÃ§Ã£o cruzada podem ser plotados usando funÃ§Ã£o plot ().Seis estatÃ­sticas sÃ£o mostradas neste boxplot. mÃ©dia (losango preto), mediana (linha preta), dobradiÃ§inferior e superior que correspondem ao primeiro e terceiro quartis (percentis 25 e 75, respectivamente). O bigode superior se estende da dobradiÃ§atÃ© o maior valor nÃ£o mais que \\(1,5 \\times {IQR}\\) partir da dobradiÃ§(em que IQR Ã© o intervalo entre quartis). O bigode mais baixo se estende da dobradiÃ§ao menor valor, mÃ¡ximo \\(1,5 \\times {IQR}\\) da dobradiÃ§. Dados alÃ©m final dos bigodes sÃ£o considerados pontos extremos. Se condiÃ§Ã£o violin = TRUE, uma plotagem de violino Ã© adicionada junto com o boxplot. Um grÃ¡fico de violino Ã© uma exibiÃ§Ã£o compacta de uma distribuiÃ§Ã£o contÃ­nua exibida da mesma maneira que um grÃ¡fico de caixa.","code":"bind1 <- bind_cv (AMMI0, AMMI2, AMMI9)\r\nbind2 <- bind_cv (AMMIF, BLUP_g, BLUP_e, BLUP_ge)\r\na <- plot(bind1, violino = TRUE)\r\nb <- plot(bind2,\r\nÂ Â Â Â Â Â Â Â Â Â width.boxplot = 0.6,\r\nÂ Â Â Â Â Â Â Â Â Â order_box = TRUE,\r\nÂ Â Â Â Â Â Â Â Â Â plot_theme = theme_metan_minimal ())\r\narrange_ggplot(a, b, labels = letters[1:2])"},{"path":"interaction.html","id":"valores-estimados-pelo-modelo-ammi","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.1 Valores estimados pelo modelo AMMI","text":"Em nosso exemplo, o modelo AMMI2 foi o que apresentou o menor RMSPD, sendo entÃ£o o mais indicado para estimar variÃ¡vel GY. estimativa considerando dois termos multiplicativos pode realizada utilizando funÃ§Ã£o predict(), tendo como argumentos o modelo AMMI ajustado (AMMI_model) e o nÃºmero de termos multiplicativos considerados na estimaÃ§Ã£o (naxis). seguintes variÃ¡veis sÃ£o retornadas: ENV Ã© o ambiente; GEN Ã© o genÃ³tipo; Y Ã© o valor observado; resOLS Ã© o residual (\\(\\hat{z}_{ij}\\)) estimado pelos MÃ­nimos Quadrados OrdinÃ¡rios, onde \\(\\hat{z}_{ij} = y_{ij} - \\bar{y}_{.} - \\bar{y}_{.j} + \\bar{y}_{..}\\); Ypred Ã© o valor estimado pelos mÃ­nimos quadrados ordinÃ¡rios (\\(\\hat{y}_{ij} = y_{ij} -\\hat{z}_{ij}\\)); ResAMMI Ã© o residual estimado pelo modelo AMMI (\\(\\hat{}_{ij}\\)) considerando o nÃºmero de termos multiplicativos informado na funÃ§Ã£o (neste caso 2), onde \\(\\hat{}_{ij} = \\lambda_1a_{i1}t_{j1}\\); YpredAMMI Ã© o valor estimado pelo modelo AMMI \\(\\hat{ya}_{ij} = \\bar{y}_{.} + \\bar{y}_{.j} - \\bar{y}_{..}+\\hat{}_{ij}\\); e AMMI0 Ã© o valor estimado quando nenhum termo multiplicativo Ã© usado, ou seja, \\(\\hat{y}_{ij} = \\bar{y}_{.} + \\bar{y}_{.j} - \\bar{y}_{..}\\).","code":"\npredicted <- predict(AMMI_model, naxis = 2)\npredicted$GY\n# # A tibble: 140 x 8\n#    ENV   GEN       Y RESIDUAL Ypred ResAMMI[,1] YpredAMMI[,1] AMMI0\n#    <fct> <fct> <dbl>    <dbl> <dbl>       <dbl>         <dbl> <dbl>\n#  1 E1    G1     2.37  -0.0843  2.45     0.0693           2.52  2.45\n#  2 E1    G10    1.97  -0.344   2.32    -0.360            1.96  2.32\n#  3 E1    G2     2.90   0.311   2.59     0.0735           2.66  2.59\n#  4 E1    G3     2.89   0.0868  2.80    -0.00963          2.79  2.80\n#  5 E1    G4     2.59   0.100   2.49     0.0144           2.50  2.49\n#  6 E1    G5     2.19  -0.196   2.38    -0.0317           2.35  2.38\n#  7 E1    G6     2.30  -0.0797  2.38     0.0238           2.40  2.38\n#  8 E1    G7     2.77   0.186   2.59     0.186            2.77  2.59\n#  9 E1    G8     2.90   0.0493  2.85     0.0852           2.94  2.85\n# 10 E1    G9     2.33  -0.0307  2.36    -0.0515           2.31  2.36\n# # ... with 130 more rows"},{"path":"interaction.html","id":"Ã­ndices-de-estabilidade-baseados-em-ammi","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.2 Ãndices de estabilidade baseados em AMMI","text":"(T. Olivoto, LÃºcio, Da silva, Marchioro, et al. 2019) demonstraram que mÃ©dia ponderada dos escores absolutos (WAAS, Weighted Average Absolute Scores), pode ser utilizada como um Ã­ndice quantitativo de estabilidade na anÃ¡lise AMMI. Utilizando funÃ§Ã£o get_model_data() Ã© possÃ­vel obter facilmente este Ã­ndice para diversas variÃ¡veis com poucas linhas de cÃ³digo. Veja o exemplo abaixo, com quatro variÃ¡veis. Este Ã­ndice tambÃ©m Ã© computado em uma estrutura de modelo misto. Veja o exemplo aqui.AlÃ©m Ã­ndice WAAS mostrado acima, os seguintes Ã­ndices de estabilidade baseados em AMMI podem ser calculados usando funÃ§Ã£o AMMI_indexes():AMMI stability value, ASV, (Purchase, Hatting, Deventer 2000).\\[\r\nASV = \\sqrt {{{\\left[ {\\frac{{IPCA{1_{ss}}}}{{IPCA{2_{ss}}}} \\times \\left( {IPCA{1_{score}}} \\right)} \\right]}^2} + {{\\left( {IPCA{2_{score}}} \\right)}^2}}\r\n\\]Soma dos valores absolutos dos escores IPCA, SIPC\\[\r\nSIP{C_i} = \\sum\\nolimits_{k = 1}^P {\\left| {\\mathop {\\lambda }\\nolimits_k^{0.5} {a_{ik}}} \\right|}\r\n\\]MÃ©dia dos autovetores elevados ao quadrado, EV\\[\r\nE{V_i} = \\sum\\nolimits_{k = 1}^P {\\mathop \\nolimits_{ik}^2 } /P\r\n\\]descritos por Sneller, Kilgore-Norquest, Dombek (1997), onde P Ã© o nÃºmero de IPCA retido por meio de testes Fvalor absoluto da contribuiÃ§Ã£o relativa dos IPCAs para interaÃ§Ã£o (Zali et al. 2012).\\[\r\nZ{a_i} = \\sum\\nolimits_{k = 1}^P {{\\theta _k}{a_{ik}}}\r\n\\]Onde \\({\\theta _k}\\) Ã© o percentual da soma de quadrados explicada pelo k -Ã©simo IPCA. Ã­ndices de selecÃ§Ã£o simultÃ¢neas (SSI) sÃ£o calculados pela soma dos ranques dos Ã­ndices Za ASV, SIPC e EV e o ranque da variÃ¡vel dependente (Farshadfar 2008) que resulta em ssiASV, ssiSIPC, ssiEV, e ssiZa, respectivamente.funÃ§Ã£o AMMI_index () tem dois argumentos. O primeiro (x) Ã© o modelo, que deve ser um objeto da classe waas. O segundo, (order.y) Ã© ordem para variÃ¡vel resposta. Por padrÃ£o, ele Ã© definido como nulo, o que significa que variÃ¡vel resposta Ã© ordenada em ordem decrescente. Se x Ã© uma lista com mais de uma variÃ¡vel, entÃ£o order.y deve ser um vetor com o mesmo comprimento de x. Cada elemento vetor deve ser um dos â€œhâ€ ou â€œlâ€. Se â€œhâ€ usado, variÃ¡vel resposta serÃ¡ ordenada em ordem decrescente. Se â€œlâ€ usado, variÃ¡vel resposta serÃ¡ ordenada em ordem crescente da mÃ©dia dos genÃ³tipos. Usando o operador %>%  Ã© possÃ­vel estruturar uma sequÃªncia lÃ³gica de operaÃ§Ãµes. Vamos construir esse modelo.","code":"\nwaas(data_ge2, ENV, GEN, REP,\n     resp = c(PH, ED, TKW, NKR)) %>%\n get_model_data(what = \"WAAS\")\n# variable PH \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  7.719  2.5728 127.913 4.25e-07          .           .\n#   REP(ENV)   8  0.161  0.0201   0.897 5.22e-01          .           .\n#        GEN  12  1.865  0.1554   6.929 6.89e-09          .           .\n#    GEN:ENV  36  5.397  0.1499   6.686 5.01e-14          .           .\n#        PC1  14  4.466  0.3190  14.230 0.00e+00       82.8        82.8\n#        PC2  12  0.653  0.0545   2.430 8.40e-03       12.1        94.9\n#        PC3  10  0.277  0.0277   1.240 2.76e-01        5.1         100\n#  Residuals  96  2.153  0.0224      NA       NA          .           .\n#      Total 191 22.692  0.1188      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable ED \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  306.0  101.99  43.386 2.70e-05          .           .\n#   REP(ENV)   8   18.8    2.35   0.906 5.15e-01          .           .\n#        GEN  12  212.9   17.74   6.838 8.95e-09          .           .\n#    GEN:ENV  36  398.2   11.06   4.263 7.60e-09          .           .\n#        PC1  14  212.2   15.16   5.840 0.00e+00       53.3        53.3\n#        PC2  12  134.7   11.23   4.330 0.00e+00       33.8        87.1\n#        PC3  10   51.3    5.13   1.980 4.38e-02       12.9         100\n#  Residuals  96  249.1    2.59      NA       NA          .           .\n#      Total 191 1583.2    8.29      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable TKW \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  37013   12338   11.13 3.16e-03          .           .\n#   REP(ENV)   8   8869    1109    1.21 3.03e-01          .           .\n#        GEN  12  44633    3719    4.05 4.41e-05          .           .\n#    GEN:ENV  36 164572    4571    4.98 1.73e-10          .           .\n#        PC1  14 104276    7448    8.11 0.00e+00       63.4        63.4\n#        PC2  12  33361    2780    3.03 1.20e-03       20.3        83.6\n#        PC3  10  26935    2694    2.93 3.00e-03       16.4         100\n#  Residuals  96  88171     918      NA       NA          .           .\n#      Total 191 507829    2659      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable NKR \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  237.0   79.01  15.843 0.000997          .           .\n#   REP(ENV)   8   39.9    4.99   0.635 0.746348          .           .\n#        GEN  12  227.8   18.99   2.418 0.008726          .           .\n#    GEN:ENV  36  602.7   16.74   2.132 0.001839          .           .\n#        PC1  14  337.4   24.10   3.070 0.000600         56          56\n#        PC2  12  192.2   16.02   2.040 0.028500       31.9        87.9\n#        PC3  10   73.1    7.31   0.930 0.509500       12.1         100\n#  Residuals  96  753.7    7.85      NA       NA          .           .\n#      Total 191 2463.8   12.90      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!\n# Class of the model: waas\n# Variable extracted: WAAS\n# # A tibble: 13 x 5\n#    gen      PH    ED   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl>\n#  1 H1    0.318 0.667 2.72  0.929\n#  2 H10   0.230 0.973 2.15  0.506\n#  3 H11   0.201 0.649 1.26  0.836\n#  4 H12   0.364 0.315 0.558 0.228\n#  5 H13   0.363 0.838 0.514 0.946\n#  6 H2    0.342 1.08  4.41  0.404\n#  7 H3    0.374 0.486 4.10  0.252\n#  8 H4    0.294 0.378 3.07  0.281\n#  9 H5    0.168 0.567 0.738 0.611\n# 10 H6    0.270 0.409 1.64  1.60 \n# 11 H7    0.228 0.384 3.44  0.518\n# 12 H8    0.315 0.653 4.91  0.941\n# 13 H9    0.146 0.907 5.50  0.888\nstab_indexes <- \n  data_ge %>%\n  waas(ENV, GEN, REP, GY, verbose = FALSE) %>%\n  AMMI_indexes()\nprint(stab_indexes)\n# Variable GY \n# ---------------------------------------------------------------------------\n# AMMI-based stability indexes\n# ---------------------------------------------------------------------------\n# # A tibble: 10 x 7\n#    GEN       Y   ASV  SIPC     EV     ZA  WAAS\n#    <chr> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n#  1 G1     2.60 0.346 0.463 0.0149 0.100  0.151\n#  2 G10    2.47 1.23  2.07  0.210  0.437  0.652\n#  3 G2     2.74 0.249 1.54  0.179  0.216  0.283\n#  4 G3     2.96 0.113 0.552 0.0207 0.0797 0.106\n#  5 G4     2.64 0.594 1.04  0.0521 0.219  0.326\n#  6 G5     2.54 0.430 0.997 0.0433 0.186  0.270\n#  7 G6     2.53 0.265 1.14  0.0911 0.172  0.233\n#  8 G7     2.74 0.663 1.79  0.191  0.303  0.428\n#  9 G8     3.00 0.574 1.18  0.0669 0.225  0.327\n# 10 G9     2.51 0.983 1.50  0.131  0.336  0.507"},{"path":"interaction.html","id":"biplots","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.3 Biplots","text":"O pacote metan conta com grÃ¡ficos gerados pelo pacote ggplot2, o que lhe confere um alto nÃ­vel de personalizaÃ§Ã£o. funÃ§Ã£o utilizada para obtenÃ§Ã£o dos diferentes tipos de biplots serÃ¡ plot_scores(). Para maiores detalhes veja ?plot_._scores.","code":""},{"path":"interaction.html","id":"biplot-tipo-2-gy-x-pc1","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.3.1 Biplot tipo 2: GY x PC1","text":"O biplot conhecido como AMMI1 Ã© confeccionado utilizando o argumento type = 2 na funÃ§Ã£o plot_scores(). O biplot AMMI1 Ã© utilizado para identificar tanto estabilidade quando produtividade dos genÃ³tipos. Neste tipo de biplot, os genÃ³tipos com escores PC1 prÃ³ximos de zero e Ã  direita da linha vertical, sÃ£o considerados os mais estÃ¡veis e com rendimento superior mÃ©dia geral.\r\nFigure 14.1: Biplot AMMI1 gerado pelo pacote metan\r\n","code":"\np3 <-plot_scores(AMMI_model)\np4 <-plot_scores(AMMI_model,\n                 col.segm.env = \"transparent\") +\n                 theme_gray() +\n                 theme(legend.position = c(0.1, 0.9),\n                       legend.background = element_rect(fill = NA))\n\narrange_ggplot(p3, p4, labels = c(\"p3\",\"p4\"))"},{"path":"interaction.html","id":"biplot-tipo-1-ipca1-x-ipca2","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.3.2 biplot tipo 1: IPCA1 x IPCA2","text":"O biplot conhecido como AMMI2 Ã© confeccionado utilizando o argumento type = 1 (padrÃ£o) na funÃ§Ã£o plot_scores(). O biplot AMMI2 representa os dois primeiros IPCAs oriundos da decomposiÃ§Ã£o por valor singular da matriz dos efeitos da interaÃ§Ã£o e Ã© utilizado para realizar inferÃªncias quanto aos padrÃµes da interaÃ§Ã£o genÃ³tipo vs ambiente. Neste caso, os dois primeiros IPCAs explicam 66.2% da da soma de quadrados da interaÃ§Ã£o.\r\nFigure 14.2: Biplot AMMI2, gerado pelo pacote metan\r\n","code":"\n\np1 <-plot_scores(AMMI_model, type = 2)\np2 <-plot_scores(AMMI_model,\n                 type = 2,\n                 polygon = TRUE,\n                 col.gen = \"black\",\n                 col.env = \"gray70\",\n                 col.segm.env = \"gray70\",\n                 axis.expand = 1.5)\narrange_ggplot(p1, p2, labels = c(\"p1\",\"p2\"))"},{"path":"interaction.html","id":"rendimento-nominal-x-ipca1","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.9.3.3 Rendimento nominal x IPCA1","text":"Com o objetivo de identificar possÃ­veis mega-ambientes, bem como visualizar o padrÃ£o -won-conjunto de dados, um grÃ¡fico com o rendimento nominal (\\(\\mathop {\\hat y} \\nolimits_ {ij}^*\\) ) em funÃ§Ã£o dos escores PCA1 dos ambientes Ã© tambÃ©m confecionado pela funÃ§Ã£o plot_scores() utilizando o argumento type = 4. Neste grÃ¡fico, cada genÃ³tipo Ã© representado por uma linha reta com equaÃ§Ã£o \\(\\hat y_{ij}^* = \\mu_i + PCA1_i \\times PCA1_j\\) , onde \\(\\hat y_{ij}^*\\) Ã© o rendimento nominal para o genÃ³tipo ambiente J; \\(\\mu\\) Ã© mÃ©dia geral genÃ³tipo ; \\(PC{1_i}\\) Ã© o escore PCA1 genÃ³tipo e \\(PC{1_j}\\) Ã© o escore PCA1 ambiente j. O genÃ³tipo vencedor em um determinado ambiente possui o maior rendimento nominal nesse ambiente.\r\nFigure 14.3: GrÃ¡fico tipo â€˜-won-whereâ€™ baseado modelo AMMI\r\n","code":"\nplot_scores(AMMI_model,\n            type = 4,\n            size.tex.pa = 2,\n            x.lab = \"Rendimento nominal (Mg/ha)\",\n            y.lab = \"Escore dos ambientes no PCA1\")"},{"path":"interaction.html","id":"o-modelo-gge","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10 O modelo GGE","text":"O modelo GGE (Genotype plus Genotype-vs-Environment interaction) tem sido amplamente utilizado para avaliaÃ§Ã£o de genÃ³tipos e identificaÃ§Ã£o de mega-ambientes em ensaios multi-ambientais (MET). Este modelo considera um biplot que Ã© construÃ­pelos dois primeiros componentes principais (PC1 e PC2) derivados da decomposiÃ§Ã£o por valores singulares de dados oriundos de um MET centrados ambiente (Yan et al. 2007).Comunmente, o rendimento mÃ©dio genÃ³tipo ambiente j Ã© descrito pelo seguinte modelo linear geral, ignorando quaisquer erros aleatÃ³rios\\[\r\n\\hat y_{ij} + \\mu + \\alpha_i + \\beta_j + \\phi_{ij}\r\n\\]onde \\(\\hat y_{ij}\\) Ã© o rendimento mÃ©dio genÃ³tipo ambiente j, \\(= 1, ... g; j = 1, ... e\\) sendo g e e o nÃºmero de genÃ³tipos e ambientes, respectivamente; \\(\\mu\\) Ã© mÃ©dia geral; \\(\\alpha_i\\) Ã© o efeito principal genÃ³tipo ; \\(\\beta_j\\) Ã© o efeito principal ambiente j e \\(\\phi_{ij}\\) Ã© o efeito de interaÃ§Ã£o entre genÃ³tipo e o ambiente j. Quando \\(\\phi_{ij}\\) Ã© submetido DecomposiÃ§Ã£o por Valor Singular (SVD), temos o bem conhecido modelo AMMI, visto anteriormente. modelo GGE, o termo \\(\\alpha_i\\) Ã© deletado modelo acima, permitindo que variaÃ§Ã£o explicada por este termo seja absorvida por \\(\\phi_{ij}\\). Em seguida, esta matriz de dados â€“agora centrada ambienteâ€“ Ã© submetida SVD (Yan et al. 2007; Yan Kang 2003). Explicitamente, temos\\[\r\n{\\phi_{ij} =  \\hat y_{ij}} - \\mu - \\beta_j  = \\sum\\limits_{k = 1}^p \\xi_{ik}^*\\eta_{jk}^*\r\n\\]onde \\(\\xi_{ik}^* = \\lambda_k^ \\alpha \\xi_{ik}\\); \\(\\eta_{jk}^* = \\lambda_k^{1-\\alpha}\\eta_{jk}\\) sendo \\(\\lambda_k\\) o k-Ã©simo autovalor da SVD (\\(k = 1, ... p\\)), com \\(p \\le min (e, g)\\); \\(\\alpha\\) Ã© o fator de partiÃ§Ã£o valor singular para o Componente Principal (PC) k (Yan 2002); \\(\\xi_{ik}^*\\) e \\(\\eta_{jk}^*\\) sÃ£o os escores PC k para genÃ³tipo e ambiente j, respectivamente.funÃ§Ã£o gge() pacote metan Ã© usada para ajustar o modelo GGE. De acordo com Yan Kang (2003), funÃ§Ã£o suporta quatro mÃ©todos de centralizaÃ§Ã£o de dados, dois mÃ©todos de escalonamento de dados e trÃªs opÃ§Ãµes para particionamento de valor singular:","code":""},{"path":"interaction.html","id":"data-centering","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.1 Data centering","text":"0 ou none: para dados nÃ£o centralizados;1 ou global: para dados centralizados globalmente (E + G + GE);2 ou environment: (padrÃ£o), para dados centrados ambiente (G + GE);3 ou double: para dados centrados duplamente (GE). Um biplot nÃ£o pode ser produzido sem modelos produzidos sem centralizaÃ§Ã£o.","code":""},{"path":"interaction.html","id":"data-scaling","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.2 Data scaling","text":"0 ou none: (padrÃ£o) para nenhum escalonamento;1 ou sd: Cada valor Ã© dividido pelo desvio padrÃ£o seu ambiente correspondente (coluna). Isso colocarÃ¡ todos os ambientes com aproximadamente o mesmo intervalo de valores.","code":""},{"path":"interaction.html","id":"singular-value-partition","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.3 Singular value partition","text":"1 ou genotype: O valor singular Ã© inteiramente particionado nos autovetores de GenÃ³tipo (\\(\\alpha = 1\\)), TambÃ©m chamado de row metric preserving;2 ou environment: (padrÃ£o) O valor singular Ã© inteiramente particionado nos autovetores de ambiente (\\(\\alpha = 0\\)), tambÃ©m chamado de column metric preserving;3 ou symmetrical: O valor singular Ã© simetricamente dividida nos autovetores de genÃ³tipo e ambiente (\\(\\alpha = 0,5\\)). Esta partiÃ§Ã£o Ã© mais frequentemente usada na anÃ¡lise AMMI.","code":""},{"path":"interaction.html","id":"ajustando-o-modelo-gge","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.4 Ajustando o modelo GGE","text":"Para ajustar o modelo GGE, usaremos os dados em data_ge, que contÃ©m dados rendimento de grÃ£os avaliados em 10 genÃ³tipos conduzidos em 14 ambientes. Primeiro de tudo, vamos criar uma tabela bidirecional para esses dados usando funÃ§Ã£o make_mat(). funÃ§Ã£o gge() ajusta um modelo GGE baseado em uma tabela bidirecional (ge_table em nosso caso) com genÃ³tipos nas linhas e ambientes em colunas, ou em um data.frame contendo pelo menos colunas para genÃ³tipos, ambientes e variÃ¡vel() resposta.\r\nO modelo acima foi ajustado considerando () column metric preserving (onde o valor singular Ã© inteiramente particionado nos autovetores ambiente); (ii) environment centered (o biplot conterÃ¡ uma informaÃ§Ã£o mista de G + GEI); e nenhum mÃ©todo de escalonamento. Para alterar estas configuraÃ§Ãµes padrÃ£o, use os argumentos svp, centering e scaling, respectivamente. Por favor, note que segundo exemplo o argumento table foi definido como TRUE para indicar que os dados de entrada sÃ£o uma tabela bidirecional.","code":"\nge_table = make_mat(data_ge, GEN, ENV, GY)\nprint.data.frame(ge_table, digits = 3)\n#       E1  E10   E11  E12  E13  E14   E2   E3   E4   E5   E6   E7   E8   E9\n# G1  2.37 2.31 1.356 1.34 3.00 1.53 3.04 4.08 3.49 4.17 2.81 1.90 2.27 2.78\n# G10 1.97 1.54 0.899 1.02 1.83 1.86 3.15 4.11 4.27 3.37 2.48 2.24 2.70 3.15\n# G2  2.90 2.30 1.491 1.99 3.03 1.43 3.23 4.57 3.72 3.83 2.54 1.99 2.05 3.36\n# G3  2.89 2.34 1.568 1.76 3.47 2.06 3.61 4.13 4.13 4.13 2.98 2.16 2.85 3.29\n# G4  2.59 2.17 1.370 1.53 2.64 1.86 3.19 3.85 3.30 3.78 2.70 1.98 2.30 3.72\n# G5  2.19 2.14 1.326 1.69 2.57 1.78 3.14 3.74 3.38 3.47 2.43 1.66 2.71 3.30\n# G6  2.30 2.21 1.501 1.39 2.91 1.80 3.29 3.43 3.40 3.57 2.34 1.76 2.54 3.04\n# G7  2.77 2.44 1.364 1.95 3.18 1.94 2.61 4.10 3.02 4.05 2.67 2.55 2.58 3.14\n# G8  2.90 2.57 1.683 2.00 3.52 1.99 3.44 4.11 4.14 4.81 2.91 2.26 2.88 2.83\n# G9  2.33 1.74 1.125 1.41 2.95 1.57 3.09 4.51 3.90 3.93 2.77 1.39 2.49 1.94\n# Usando um data frame\ngge_model <- gge(data_ge, ENV, GEN, GY)"},{"path":"interaction.html","id":"valores-estimados-pelo-modelo-gge","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.5 Valores estimados pelo modelo GGE","text":"","code":"\npredicted <- predict(gge_model, naxis = 2)\nprint(predicted$GY, digits = 3)\n# # A tibble: 10 x 14\n#       E1   E10   E11   E12   E13   E14    E2    E3    E4    E5    E6    E7    E8\n#  * <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1  2.51  2.14 1.35   1.59  2.95  1.76  3.20  4.14  3.78  3.97  2.70  1.95  2.56\n#  2  1.93  1.62 0.988  1.07  2.07  1.66  3.08  3.99  3.78  3.28  2.43  1.69  2.48\n#  3  2.67  2.33 1.47   1.75  3.09  1.82  3.19  4.04  3.59  4.03  2.70  2.08  2.54\n#  4  2.83  2.44 1.56   1.88  3.40  1.83  3.26  4.18  3.73  4.32  2.83  2.10  2.59\n#  5  2.50  2.25 1.39   1.64  2.71  1.83  3.09  3.81  3.32  3.65  2.52  2.11  2.47\n#  6  2.32  2.06 1.27   1.46  2.52  1.78  3.09  3.87  3.46  3.54  2.49  1.99  2.47\n#  7  2.40  2.11 1.31   1.52  2.65  1.78  3.12  3.92  3.52  3.66  2.54  2.00  2.49\n#  8  2.78  2.49 1.56   1.88  3.17  1.87  3.17  3.93  3.40  4.04  2.68  2.21  2.52\n#  9  2.99  2.54 1.64   2.00  3.71  1.84  3.33  4.33  3.89  4.61  2.96  2.11  2.64\n# 10  2.27  1.78 1.15   1.30  2.82  1.64  3.28  4.43  4.28  4.03  2.79  1.65  2.62\n# # ... with 1 more variable: E9 <dbl>"},{"path":"interaction.html","id":"visualizando-o-biplot","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6 Visualizando o Biplot","text":"funÃ§Ã£o genÃ©rica plot() Ã© usada para gerar um biplot usando como entrada o modelo ajustado da classe gge. O tipo de biplot Ã© escolhido pelo argumento type na funÃ§Ã£o. Dez tipos de biplots estÃ£o disponÃ­veis de acordo com Yan Kang (2003).type = 1 Um biplot bÃ¡sico.type = 2 Desempenho mÃ©dio vs.Â estabilidadetype = 3 Que ganhou onde.type = 4 DescriminaÃ§Ã£o vs.Â representatividadetype = 5 Examinar um ambiente.type = 6 Ranquear os ambientes.type = 7 Examinar um genÃ³tipo.type = 8 Ranquear os genÃ³tipos.type = 9 Comparar dois genÃ³tipos.type = 10 RelaÃ§Ã£o entre os ambientes.Neste material, para cada tipo de biplot, dois grÃ¡ficos sÃ£o produzidos. Um com configuraÃ§Ãµes padrÃ£o e outro para mostrar algumas opÃ§Ãµes grÃ¡ficas da funÃ§Ã£o.","code":""},{"path":"interaction.html","id":"biplot-tipo-1-um-biplot-bÃ¡sico","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.1 Biplot tipo 1: Um biplot bÃ¡sico","text":"Esta Ã© configuraÃ§Ã£o padrÃ£o grÃ¡fico da funÃ§Ã£o, portanto, este biplot Ã© produzido apenas chamando plot(model), como mostrado abaixo.\r\n","code":"\np1 <- plot(gge_model)\np2 <- plot(gge_model,\n           col.gen = \"blue\",\n           size.text.env = 2)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-2-desempenho-mÃ©dio-vs.-estabilidade","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.2 Biplot tipo 2: Desempenho mÃ©dio vs.Â estabilidade","text":"Neste biplot, visualizaÃ§Ã£o da mÃ©dia e da estabilidade dos genÃ³tipos Ã© obtida desenhando uma coordenada mÃ©dia de ambiente (AEC) biplot obtido com row metric preserving. Primeiro, um ambiente mÃ©dio, representado pelo pequeno cÃ­rculo, Ã© definido pelas mÃ©dias dos escores PC1 e PC2 dos ambientes. linha que passa pela origem biplot e pelo AEC pode ser chamada de mÃ©dia. projeÃ§Ãµes de marcadores genotÃ­picos nesse eixo deve, portanto, aproximar o rendimento mÃ©dio dos genÃ³tipos. Assim, o G8 foi claramente o genÃ³tipo de maior rendimento, em mÃ©dia.ordenada de AEC Ã© linha que passa pela origem biplot e Ã© perpendicular Ã  abscissa AEC. Portanto, se abscissa AEC representa o G, ordenada AEC deve aproximar o GEI associado cada genÃ³tipo, que Ã© uma medida de variabilidade ou instabilidade dos genÃ³tipos (Yan et al. 2007). Uma projeÃ§Ã£o maior na ordenada AEC, independentemente da direÃ§Ã£o, significa maior instabilidade. Em nosso exemplo, o G3 foi o mais estÃ¡vel e o segundo genÃ³tipo mais produtivo, enquanto o G9 apresentou alta instabilidade.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <-plot(gge_model, type = 2)\np2 <-plot(gge_model,\n          type = 2,\n          col.gen = \"black\",\n          col.env = \"gray\",\n          axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-3-quem-ganhou-onde","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.3 Biplot tipo 3: quem ganhou onde","text":"Neste biplot (obtido com particionamento de valores singulares simÃ©trico) um polÃ­gono Ã© desenhado juntando os genÃ³tipos (G7, G8, G9, G10 e G4) que estÃ£o localizadas mais distante da origem biplot fazendo com que todos os outros genÃ³tipos fiquem contidos polÃ­gono. Os genÃ³tipos vÃ©rtex tÃªm os vetores mais longos, em suas respectivas direÃ§Ãµes, que Ã© uma medida da capacidade de resposta aos ambientes. Estes genÃ³tipos estÃ£o, portanto, entre os genÃ³tipos mais responsivos. Todos os outros genÃ³tipos sÃ£o menos responsivos em suas respectivas direÃ§Ãµes.linhas perpendiculares aos lados polÃ­gono dividem o biplot em setores. Cada setor tem um genÃ³tipo vÃ©rtice. Por exemplo, o setor com o genÃ³tipo-vÃ©rtice G4 pode ser referido como o setor G4. Um ambiente (E9), foi enquadrado neste setor. Como regra geral, o genÃ³tipo vÃ©rtex Ã© o genÃ³tipo de mais alto rendimento em todos os ambientes que compartilham o setor com ele (Yan et al. 2007). Neste caso, G4 teve o maior rendimento em E9, como mostrado na tabela acima.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <-plot(gge_model, type = 3)\np2 <-plot(gge_model,\n          type = 3,\n          size.shape.win = 5,\n          large_label = 6,\n          col.gen = \"black\",\n          col.env = \"gray\",\n          annotation = FALSE,\n          title = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-4-discriminaÃ§Ã£o-vs.-representatividade","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.4 Biplot tipo 4: DiscriminaÃ§Ã£o vs.Â representatividade","text":"","code":"\np1 <-plot(gge_model, type = 4)\np2 <-plot(gge_model,\n          type = 4,\n          plot_theme = theme_gray())+\n  theme(legend.position = \"bottom\")\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-5-examinar-um-ambiente","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.5 Biplot tipo 5: Examinar um ambiente","text":"identificaÃ§Ã£o de genÃ³tipos mais adaptados um ambiente pode ser facilmente alcanÃ§ada utilizando o biplot GGE. Por exemplo, para visualizar o desempenho de diferentes genÃ³tipos em um dado ambiente, por exemplo, E10, simplesmente desenhe uma linha que passa pela origem biplot e o marcador E10. Os genÃ³tipos podem ser classificados de acordo com suas projeÃ§Ãµes eixo E10 com base em seu desempenho neste ambiente, na direÃ§Ã£o apontada pela seta. Em nosso exemplo, E10, o genÃ³tipo de maior rendimento foi o genÃ³tipo G8 e o genÃ³tipo de menor rendimento foi G10. ordem dos genÃ³tipos foi G8 > G7 > G3 > G2 > G4 > G1 > G6 > G5 > G9 > G10.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <-plot(gge_model, type = 5, sel_env = \"E10\")\np2 <-plot(gge_model,\n          type = 5,\n          sel_env = \"E10\",\n          col.gen = \"black\",\n          col.env = \"black\",\n          size.text.env = 10,\n          axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-6-ranquear-os-ambientes","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.6 Biplot tipo 6: ranquear os ambientes","text":"Neste biplot o ambiente â€œidealâ€ Ã© usado como o centro de um conjunto de linhas concÃªntricas que servem para medir distÃ¢ncia entre um ambiente e o ambiente â€œidealâ€. Como o foco principal neste biplot sÃ£o os ambientes, partiÃ§Ã£o de valor singular usada Ã© â€œambienteâ€ (padrÃ£o). Pode ser visto que E13 Ã© o mais prÃ³ximo ambiente ideal e, portanto, Ã© o mais desejÃ¡vel de todos os 14 ambientes. E4 e E9 foram os ambientes de teste menos desejados.\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY)\np1 <- plot(gge_model, type = 6)\np2 <- plot(gge_model,\n           type = 6,\n           col.gen = \"black\",\n           col.env = \"black\",\n           size.text.env = 10,\n           axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-7-examinar-um-genÃ³tipo","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.7 Biplot tipo 7: examinar um genÃ³tipo","text":"Semelhante Ã  visualizaÃ§Ã£o dos desempenhos genotÃ­picos em um determinado ambiente (biplot 5), visualizaÃ§Ã£o da mÃ©dia e da estabilidade dos genÃ³tipos Ã© obtida desenhando-se uma AEC biplot com foco genÃ³tipo, ou row metric preserving (Yan et al. 2007).\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <- plot(gge_model, type = 7, sel_gen = \"G8\")\np2 <- plot(gge_model,\n           type = 7,\n           sel_gen = \"G8\",\n           col.gen = \"black\",\n           col.env = \"black\",\n           size.text.env = 10,\n           axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-8-ranquear-os-genÃ³tipos","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.8 Biplot tipo 8: ranquear os genÃ³tipos","text":"Este biplot compara todos os genÃ³tipos com o genÃ³tipo â€œidealâ€. O genÃ³tipo ideal, representado pelo pequeno cÃ­rculo com uma seta apontando para ele, Ã© definido como tendo o maior rendimento em todos os ambientes. Ou seja, tem o maior rendimento mÃ©dio e Ã© absolutamente estÃ¡vel. Os genÃ³tipos sÃ£o classificados com base em sua distÃ¢ncia genÃ³tipo ideal (Yan et al. 2007). Em nosso exemplo, o G3 e o G8 superaram os outros genÃ³tipos.\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <- plot(gge_model, type = 8)\np2 <- plot(gge_model,\n           type = 8,\n           col.gen = \"black\",\n           col.env = \"gray\",\n           size.text.gen = 6)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-9-comparar-dois-genÃ³tipos","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.9 Biplot tipo 9: comparar dois genÃ³tipos","text":"Para comparar dois genÃ³tipos, por exemplo, G10 e G8, desenhe uma linha de conexÃ£o para conectÃ¡-los e trace uma linha perpendicular que passa pela origem biplot e Ã© perpendicular Ã  linha de conexÃ£o. Vemos que um ambiente, E9, estÃ¡ mesmo lado da linha perpendicular G10, e os outros 13 ambientes estÃ£o outro lado da linha perpendicular, junto com o G8. Isso indica que G10 produziu mais que o G8 E9, mas G8 produziu mais que o G10 nos outros 13 ambientes (Yan et al. 2007).\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <- plot(gge_model, type = 9, sel_gen1 = \"G8\", sel_gen2 = \"G10\")\np2 <- plot(gge_model,\n           type = 9,\n           sel_gen1 = \"G8\",\n           sel_gen2 = \"G10\",\n           col.gen = \"black\",\n           title = FALSE,\n           annotation = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-10-relaÃ§Ã£o-entre-os-ambientes","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.10.6.10 Biplot tipo 10: relaÃ§Ã£o entre os ambientes","text":"","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY)\np1 <-plot(gge_model, type = 10)\np2 <-plot(gge_model,\n          type = 10,\n          col.gen = \"black\",\n          title = FALSE,\n          annotation = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"modelos-mistos-na-avaliaÃ§Ã£o-de-met","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11 Modelos mistos na avaliaÃ§Ã£o de MET","text":"Assumindo \\(\\mathop\\alpha\\nolimits_i\\) e \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) como sendo de efeitos aleatÃ³rios, o modelo em  pode ser convenientemente reescrito utilizando o seguinte modelo linear misto:\r\n\\[ \\label{meq}\r\n{\\boldsymbol{y  = X\\beta  + Zu + \\varepsilon }}\r\n\\]onde y Ã© um vetor \\(n[ = \\sum\\nolimits_{j = 1}^e {(gb)]} \\times 1\\), \\({\\boldsymbol{y}} = {\\rm{ }}{\\left[ {{y_{111}},{\\rm{ }}{y_{112}},{\\rm{ }} \\ldots ,{\\rm{ }}{y_{geb}}} \\right]^\\prime }\\); \\(\\boldsymbol{\\beta}\\) Ã© um vetor \\(eb \\times 1\\) de efeitos fixos, \\({\\boldsymbol{\\beta }} = [\\mathop \\gamma \\nolimits_{11} ,\\mathop \\gamma \\nolimits_{12} ,...,\\mathop \\gamma \\nolimits_{eb} ]'\\); u Ã© um vetor \\(m[ = g + ge] \\times 1\\) de efeitos aleatÃ³rios, \\({\\boldsymbol{u}} = {\\rm{ }}{\\left[ {{\\alpha _1},{\\alpha _2},...,{\\alpha _g},\\mathop {(\\alpha \\tau )}\\nolimits_{11} ,\\mathop {(\\alpha \\tau )}\\nolimits_{12} ,...,\\mathop {(\\alpha \\tau )}\\nolimits_{ge} } \\right]^\\prime }\\); X Ã© uma matriz delineamento de dimensÃ£o \\(n \\times eb\\) relacionando y \\({\\boldsymbol{\\beta }}\\); Z Ã© uma matriz delineamento de dimensÃ£o \\(m\\times n\\) relacionando y \\(\\boldsymbol{u}\\); e \\({\\boldsymbol{\\varepsilon }}\\) Ã© um vetor nÃ—1 de erros aleatÃ³rios, \\({\\boldsymbol{\\varepsilon }} = {\\rm{ }}{\\left[ {{y_{111}},{\\rm{ }}{y_{112}},{\\rm{ }} \\ldots ,{\\rm{ }}{y_{geb}}} \\right]^\\prime }\\)Os vetores aleatÃ³rios \\({\\boldsymbol{\\beta }}\\) e \\(\\boldsymbol{u}\\) sÃ£o assumidos como normais e independentemente distribuÃ­dos com mÃ©dia zero e matrizes de variÃ¢ncia-covariÃ¢ncia G e R, respectivamente, de tal forma que\\[\r\n\\left[ \\begin{array}{l}{\\boldsymbol{u}}\\\\{\\boldsymbol{\\varepsilon }}\\end{array} \\right]\\sim N\\left( {\\left[ \\begin{array}{l}{\\boldsymbol{0}}\\\\{\\boldsymbol{0}}\\end{array} \\right]{\\boldsymbol{,}}\\left[ {\\begin{array}{*{20}{c}}{\\boldsymbol{G}}&{\\boldsymbol{0}}\\\\{\\boldsymbol{0}}&{\\boldsymbol{R}}\\end{array}} \\right]} \\right)\r\n\\]matriz de variÃ¢ncia-covariÃ¢ncia G tem muitas formas possÃ­veis. estrutura mais simples -e opÃ§Ã£o padrÃ£o na maioria dos pacotes estatÃ­sticos para modelos mistos- sÃ£o os componentes de variÃ¢ncia, de modo que\\[\r\n{\\boldsymbol{G}} = \\left[ {\\begin{array}{*{20}{c}}{\\mathop {\\hat \\sigma }\\nolimits_\\alpha ^2 {{\\boldsymbol{}}_g}}&0\\\\0&{\\mathop {\\hat \\sigma }\\nolimits_{\\alpha \\tau }^2 {{\\boldsymbol{}}_{ge}}}\\end{array}} \\right]\r\n\\]e \\({\\boldsymbol{R}} = \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2 {{\\boldsymbol{}}_n}\\), sendo \\(\\mathop {\\hat \\sigma }\\nolimits_\\alpha ^2\\), \\(\\mathop {\\hat \\sigma }\\nolimits_{\\alpha \\tau }^2\\) e \\(\\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2\\) variÃ¢ncias para genÃ³tipo, interaÃ§Ã£o genÃ³tipo-ambiente e erros aleatÃ³rios, respectivamente.Os vetores \\({\\boldsymbol{\\beta }}\\) e \\(\\boldsymbol{u}\\) sÃ£o estimados considerando bem conhecida equaÃ§Ã£o de modelo misto Henderson (1975).\\[ \\label{ad}\r\n\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{\\hat \\beta }}}\\\\{{\\boldsymbol{\\hat u}}}\\end{array}} \\right]{\\boldsymbol{ = }}{\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{\\ - {\\boldsymbol{1}}}}{\\boldsymbol{X}}}&{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{Z}}}\\\\{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{X}}}&{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{Z + }}{{\\boldsymbol{G}}^{ - {\\boldsymbol{1}}}}}\\end{array}} \\right]^ - }\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{y}}}\\\\{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{y}}}\\end{array}} \\right]\r\n\\]Onde o sobescrito \\(^{-1}\\) e \\(^-\\) representam inversas e inversas generalizadas das matrizes, respectivamente. estimativa dos componentes de variÃ¢ncia em \\({\\boldsymbol{\\hat G}}\\) e \\({\\boldsymbol{\\hat R}}\\), pode ser realizada sem maiores problemas utilizando ANOVA  convencional quando os dados sÃ£o balanceados. Quando este pressuposto nÃ£o Ã© cumprido, estimativa baseada em Restricted Maximum Likelihood (REML)  utilizando o algoritmo Expectation-Maximization (Dempster, Laird, Rubin 1977) Ã© mais indicada.Desde dÃ©cada de 1990, os modelos mistos vÃªm ganhando cada vez mais espaÃ§o na avaliaÃ§Ã£o de MET, pois permitem estimativa de parÃ¢metros genÃ©ticos  e ambientais, bem como prediÃ§Ã£o dos valores genotÃ­picos de forma nÃ£o-viciada (Smith, Cullis, Thompson 2005). Da mesma forma, modelos mistos reduzem os ruÃ­dos de anÃ¡lises realizadas com dados desbalanceados e tambÃ©m de variÃ¡veis que nÃ£o assumem aditividade, caracterÃ­sticas frequentemente observadas em MET (Hu 2015). Na avaliaÃ§Ã£o dos dados oriundos de MET Ã© de interesse pesquisador predizer o â€œverdadeiroâ€ rendimento \\(w_{ij}\\) dado os rendimentos observados \\(y_{ij}\\). Quando um fator Ã© considerado fixo, inferÃªncias sÃ£o limitadas apenas aos nÃ­veis testados deste fator e os efeitos sÃ£o estimados por Best Linear Unbiased Estimator, ou BLUE . Para efeitos aleatÃ³rios, onde deseja-se expandir inferÃªncias para uma populaÃ§Ã£o de tratamentos (ou ambientes), prediÃ§Ã£o Ã© realizada por Best Linear Unbiased Predictor, ou BLUP  (Henderson 1975).\r\nEq. ","code":""},{"path":"interaction.html","id":"ajustando-o-modelo","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.1 Ajustando o modelo","text":"funÃ§Ã£o waasb()  pacote metan serÃ¡ utilizada para anÃ¡lise dos dados de nosso exemplo utilizando o modelo misto descrito acima, considerando como aleatÃ³rio os efeitos de genÃ³tipo e da interaÃ§Ã£o genÃ³tipo-vs-ambiente.funÃ§Ã£o get_model_data() pode ser utilizada para extrair importantes informaÃ§Ãµes modelo ajustado com funÃ§Ã£o waasb().Resumo experimentoTeste de razÃ£o de mÃ¡xima verossimilhanÃ§(LRT)O teste LRT  indicou diferenÃ§significativas para os efeitos aleatÃ³rios de genÃ³tipo e interaÃ§Ã£o genÃ³tipo-vs-ambiente. Assim, utilizaÃ§Ã£o de modelos mistos Ã© indicada para prediÃ§Ã£o rendimento em nosso exemplo.Componentes da variÃ¢ncia","code":"\n\nBLUP_model <- waasb(data_ge, ENV, GEN, REP,\n                    resp = c(GY, HM),\n                    verbose = FALSE)\nget_model_data(BLUP_model, \"details\")\n# # A tibble: 14 x 3\n#    Parameters GY                  HM              \n#    <chr>      <chr>               <chr>           \n#  1 Mean       \"2.67\"              \"48.09\"         \n#  2 SE         \"0.05\"              \"0.21\"          \n#  3 SD         \"0.92\"              \"4.37\"          \n#  4 CV         \"34.56\"             \"9.09\"          \n#  5 Min        \"0.67 (G10 in E11)\" \"38 (G2 in E14)\"\n#  6 Max        \"5.09 (G8 in E5)\"   \"58 (G8 in E11)\"\n#  7 MinENV     \"E11 (1.37)\"        \"E14 (41.03)\"   \n#  8 MaxENV     \"E3 (4.06)\"         \"E11 (54.2)\"    \n#  9 MinGEN     \"G10 (2.47) \"       \"G2 (46.66) \"   \n# 10 MaxGEN     \"G8 (3) \"           \"G5 (49.3) \"    \n# 11 wresp      \"50\"                \"50\"            \n# 12 mresp      \"100\"               \"100\"           \n# 13 Ngen       \"10\"                \"10\"            \n# 14 Nenv       \"14\"                \"14\"\nget_model_data(BLUP_model, \"lrt\")\n# # A tibble: 4 x 8\n#   VAR   model    npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>   <int>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 GY    GEN        44  -224.  537. 19.3      1     1.11e- 5\n# 2 GY    GEN:ENV    44  -237.  562. 44.8      1     2.15e-11\n# 3 HM    GEN        44  -867. 1821.  7.86     1     5.07e- 3\n# 4 HM    GEN:ENV    44  -894. 1876. 62.8      1     2.27e-15\nget_model_data(BLUP_model, \"vcomp\")\n# # A tibble: 3 x 3\n#   Group        GY    HM\n#   <chr>     <dbl> <dbl>\n# 1 GEN      0.0280 0.490\n# 2 GEN:ENV  0.0567 2.19 \n# 3 Residual 0.0967 2.84"},{"path":"interaction.html","id":"anÃ¡lise-dos-resÃ­duos-2","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.2 AnÃ¡lise dos resÃ­duos","text":"funÃ§Ã£o autoplot() tambÃ©m pode ser utilizada para investigaÃ§Ã£o dos pressupostos modelo neste exemplo. Dois grÃ¡ficos que sÃ£o gerados mas nÃ£o mostrados por padrÃ£o sÃ£o vistos neste exemplo.","code":"\nplot(BLUP_model)"},{"path":"interaction.html","id":"parÃ¢metros-genÃ©ticos","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.3 ParÃ¢metros genÃ©ticos","text":"AlÃ©m dos componentes de variÃ¢ncia  para os efeitos aleatÃ³rios declarados, alguns importantes parÃ¢metros genÃ©ticos  sÃ£o mostrados. Heribatility Ã© herdabilidade sentido amplo (\\(\\mathop h\\nolimits_g^2\\)) estimada por \\(\\mathop h\\nolimits_g^2 = \\mathop \\sigma \\nolimits_g^2 /\\left( {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 + \\mathop \\sigma \\nolimits_e^2 } \\right)\\) onde \\((\\mathop \\sigma \\nolimits_g^2 )\\) Ã© variÃ¢ncia genotÃ­pica; \\((\\mathop \\sigma \\nolimits_i^2 )\\) Ã© variÃ¢ncia da interaÃ§Ã£o GE; e \\((\\mathop \\sigma \\nolimits_e^2 )\\) Ã© variÃ¢ncia residual. GEIr2 Ã© o coeficiente de determinaÃ§Ã£o efeito da interaÃ§Ã£o GE (\\(\\mathop r\\nolimits_i^2\\) ) estimado por \\(\\mathop r\\nolimits_i^2 = \\mathop \\sigma \\nolimits_i^2 /\\left( {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 + \\mathop \\sigma \\nolimits_e^2 } \\right)\\); Heribatility means Ã© herdabilidade da mÃ©dia assumindo ausÃªncia de valores perdidos \\((\\mathop h\\nolimits_{gm}^2 )\\), estimada por \\(\\mathop h\\nolimits_{gm}^2 = \\mathop \\sigma \\nolimits_g^2 /\\left[ {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 /+ \\mathop \\sigma \\nolimits_e^2 /\\left( {ab} \\right)} \\right]\\), onde e b sÃ£o o nÃºmero de ambientes e blocos, respectivamente; Accuracy Ã© acurÃ¡cia de seleÃ§Ã£o (Ac), estimada por \\(Ac = \\sqrt{\\mathop h\\nolimits_{gm}^2}\\) ; rge Ã© correlaÃ§Ã£o genÃ³tipo-ambiente \\((\\mathop r\\nolimits_{ge})\\) estimada por \\(\\mathop r\\nolimits_{ge} = \\mathop \\sigma \\nolimits_g^2 /\\left({\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2} \\right)\\); CVg Ã© o coeficiente de variaÃ§Ã£o genotÃ­pico, estimado por \\({\\left({\\mathop \\sigma \\nolimits_g^2 /\\mu } \\right)^{0.5}} \\times 100\\), onde \\(\\mu\\) Ã© mÃ©dia geral; CVr Ã© o coeficiente de variaÃ§Ã£o residual, estimado por \\({\\left( {\\mathop \\sigma \\nolimits_e^2 /\\mu} \\right)^{0.5}} \\times 100\\) ; CV ratio Ã© razÃ£o entre o coeficiente de variaÃ§Ã£o genotÃ­pico e residual.","code":"\nget_model_data(BLUP_model, \"genpar\")\n# Class of the model: waasb\n# Variable extracted: genpar\n# # A tibble: 9 x 3\n#   Parameters              GY     HM\n#   <chr>                <dbl>  <dbl>\n# 1 Phenotypic variance  0.181 5.52  \n# 2 Heritability         0.154 0.0887\n# 3 GEIr2                0.313 0.397 \n# 4 h2mg                 0.815 0.686 \n# 5 Accuracy             0.903 0.828 \n# 6 rge                  0.370 0.435 \n# 7 CVg                  6.26  1.46  \n# 8 CVr                 11.6   3.50  \n# 9 CV ratio             0.538 0.415"},{"path":"interaction.html","id":"mÃ©dias-preditas","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.4 MÃ©dias preditas","text":"Imprimindo os BLUPs preditos para genÃ³tiposblupGEN mostra mÃ©dia predita para os genÃ³tipos testados. BLUPg Ã© o efeito genotÃ­pico \\((\\hat{g}_{})\\) estimado por \\(\\hat{g}_{} = h_g^2(\\bar{y}_{.}-\\bar{y}_{..})\\) onde \\(h_g^2\\) Ã© o efeito shrinkage para genÃ³tipo, estimado por \\(\\mathop h\\nolimits_g^2 = (\\mathop {\\hat \\sigma }\\nolimits_i^2 + E\\mathop {\\hat \\sigma }\\nolimits_g^2 )/(\\mathop {\\hat \\sigma }\\nolimits_i^2 + \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2 + E\\mathop {\\hat \\sigma }\\nolimits_g^2 ).\\). Predicted Ã© mÃ©dia predita por \\(\\hat{g}_{}+\\mu\\) onde \\(\\mu\\) Ã© mÃ©dia geral. LL e UL sÃ£o os limites inferiore e superior, respectivamente estimado por \\((\\hat{g}_{}+\\mu)\\pm{CI}\\). \\(CI\\) Ã© o intervalo de confianÃ§para prediÃ§Ã£o BLUP, assumindo uma dada probabilidade de erro, onde \\(CI = t\\times\\sqrt{((1-Ac)\\times{\\mathop \\sigma \\nolimits_g^2)}}\\) onde \\(t\\) Ã© o valor t-Student para um teste bilateral uma dada probabilidade de erro; \\(Ac\\) Ã© acurÃ¡cia de seleÃ§ao; e \\(\\mathop \\sigma \\nolimits_g^2\\) Ã© variÃ¢ncia genotÃ­pica.plotando os BLUPs preditos para genÃ³tipos\r\nFigure 14.4: MÃ©dias preditas para genÃ³tipos considerando um modelo misto\r\nImprimindo os BLUPs estimados para combinaÃ§Ãµes genÃ³tipo x ambiente (primeiras 10 entradas)saÃ­da acima os BLUPs  estimados para combinaÃ§Ãµes genÃ³tipo x ambiente. BLUPg Ã© o efeito genotÃ­pico, descrito acima; BLUPge Ã© o efeito genotÃ­pico genÃ³tipo ambiente j \\((\\hat{g}_{ij})\\) estimado por \\(\\hat{g}_{ij} = h_g^2(\\bar{y}_{.}-\\bar{y}_{..})+h_{ge}^2(y_{ij}-\\bar{y}_{.}-\\bar{y}_{.j}+\\bar{y}_{..})\\), onde \\(h_{ge}^2\\) Ã© o efeito shrinkage para interaÃ§Ã£o GE, estimado por \\(\\mathop h\\nolimits_{ge}^2 = \\mathop {\\hat \\sigma }\\nolimits_i^2 /(\\mathop {\\hat \\sigma }\\nolimits_i^2 + \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2)\\); BLUPg+ge Ã© \\(BLUP_g+BLUP_{ge}\\); Predicted Ã© mÃ©dia predita (\\(\\hat{y}_{ij}\\)) estimada por \\(\\hat{y}_{ij} = \\bar{y}_{.j}+BLUP_{g+ge}\\).Para obter os valores acima para cada variÃ¡vel modelo basta utilizar funÃ§Ã£o get_model_data(), por exemplo:get_model_data(BLUP_model, \"blupg\") para mÃ©dias preditas de genÃ³tipos;get_model_data(BLUP_model, \"blupge\") para mÃ©dias preditas de genÃ³tipos em ambientes;","code":"\n\nprint(BLUP_model$GY$BLUPgen)\n# # A tibble: 10 x 6\n#     Rank GEN     BLUPg Predicted    LL    UL\n#    <dbl> <fct>   <dbl>     <dbl> <dbl> <dbl>\n#  1     1 G8     0.269       2.94  2.78  3.11\n#  2     2 G3     0.229       2.90  2.74  3.07\n#  3     3 G2     0.0570      2.73  2.57  2.90\n#  4     4 G7     0.0543      2.73  2.56  2.89\n#  5     5 G4    -0.0264      2.65  2.48  2.81\n#  6     6 G1    -0.0575      2.62  2.45  2.78\n#  7     7 G5    -0.112       2.56  2.40  2.73\n#  8     8 G6    -0.114       2.56  2.39  2.73\n#  9     9 G9    -0.134       2.54  2.37  2.71\n# 10    10 G10   -0.166       2.51  2.34  2.67\np1 <-plot_blup(BLUP_model)\np2 <-plot_blup(BLUP_model,\n               col.shape  =  c(\"gray20\", \"gray80\"),\n               y.lab = \"GenÃ³tipos\",\n               x.lab = \"Rendimento de grÃ£os predito\") + coord_flip()\n\narrange_ggplot(p1, p2, labels = c(\"p1\", \"p2\"))\n\nprint(BLUP_model$GY$BLUPint)\n# # A tibble: 420 x 7\n#    ENV   GEN   REP     BLUPg  BLUPge `BLUPg+ge` Predicted\n#    <fct> <fct> <fct>   <dbl>   <dbl>      <dbl>     <dbl>\n#  1 E1    G1    1     -0.0575 -0.0621    -0.120       2.46\n#  2 E1    G1    2     -0.0575 -0.0621    -0.120       2.44\n#  3 E1    G1    3     -0.0575 -0.0621    -0.120       2.31\n#  4 E1    G2    1      0.0570  0.207      0.264       2.84\n#  5 E1    G2    2      0.0570  0.207      0.264       2.82\n#  6 E1    G2    3      0.0570  0.207      0.264       2.69\n#  7 E1    G3    1      0.229   0.0885     0.318       2.89\n#  8 E1    G3    2      0.229   0.0885     0.318       2.87\n#  9 E1    G3    3      0.229   0.0885     0.318       2.75\n# 10 E1    G4    1     -0.0264  0.0601     0.0337      2.61\n# # ... with 410 more rows"},{"path":"interaction.html","id":"Ã­ndices-de-estabilidade-baseados-em-blup","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.5 Ãndices de estabilidade baseados em BLUP","text":"","code":""},{"path":"interaction.html","id":"wsb","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.5.1 O Ã­ndice WAASB","text":"Recentemente, (T. Olivoto, LÃºcio, Da silva, Marchioro, et al. 2019) propuseram um Ã­ndice de estabilidade, WAASB (Weighted Average Absolute Scores), que combina caracterÃ­sticas modelo AMMI e BLUP. O Ã­ndice Ã© baseado na mÃ©dia ponderada dos escores absolutos obtidos pela decomposiÃ§Ã£o por valores singulares da matriz BLUP dos efeitos da interaÃ§Ã£o em um modelo de efeito misto, conforme seguite equaÃ§Ã£o.\\[\r\n        WAASB_i  =\r\n        \\sum_{k = 1}^{p} |IPCA_{ik} \\times EP_k|/ \\sum_{k = 1}^{p}EP_k\r\n\\]onde \\(WAASB_i\\) Ã© mÃ©dia ponderada dos escores absolutos genÃ³tipo ; \\(IPCA_{ik}\\) Ã© o escore genÃ³tipo k-esimo IPCA; e \\(EP_k\\) Ã© variÃ¢ncia explicada pelo k IPCA para \\(k = 1,2, .., p\\), sendo \\(p = min (g-1; e-1)\\). O genÃ³tipo mais estÃ¡vel Ã© aquele com o menor valor de WAASB (T. Olivoto, LÃºcio, Da silva, Marchioro, et al. 2019).Devido aplicaÃ§Ã£o da tÃ©nica de decomposiÃ§Ã£o por valores singulares, Ã© possÃ­vel confecÃ§Ã£o de biplots semelhantes ao mÃ©todo AMMI, considerando agora, um modelo de efeito misto. Para isto, funÃ§Ã£o plot_scores  Ã© utilizada utilizando como argumento de o modelo ajustado BLUP_model. T. Olivoto, LÃºcio, Da silva, Marchioro, et al. (2019) e T. Olivoto, LÃºcio, Da silva, Sari, et al. (2019) propuseram utilizaÃ§Ã£o Ã­ndice WAASB na ordenada biplot AMMI1, substituindo os valores IPCA1 pelos valores WAASB. Este biplot Ã© criado utilizando type = 3 na funÃ§Ã£o.\r\nFigure 14.5: Biplot AMMI2 gerado pelo pacote metan\r\nOs quadrantes propostos nesta interpretaÃ§Ã£o representam quatro classificaÃ§Ãµes propostas por T. Olivoto, LÃºcio, Da silva, Marchioro, et al. (2019) em relaÃ§Ã£o Ã  interpretaÃ§Ã£o conjunta da produtividade e estabilidade. Os genÃ³tipos ou ambientes incluÃ­dos quadrante podem ser considerados genÃ³tipos instÃ¡veis â€“ou ambientes com alta capacidade de discriminaÃ§Ã£o, mas com produtividade abaixo da mÃ©dia geral. quadrante II estÃ£o incluÃ­dos os genÃ³tipos instÃ¡veis, embora com produtividade acima da mÃ©dia geral. Os ambientes incluÃ­dos neste quadrante merecem atenÃ§Ã£o especial, pois, alÃ©m de fornecerem altas magnitudes da variÃ¡vel resposta, apresentam boa capacidade de discriminaÃ§Ã£o. Os genÃ³tipos dentro quadrante III tÃªm baixa produtividade, mas podem ser considerados estÃ¡veis devido aos valores mais baixos WAASB. Quanto menor esse valor, mais estÃ¡vel o genÃ³tipo pode ser considerado. Os ambientes incluÃ­dos neste quadrante podem ser considerados pouco produtivos e com baixa capacidade de discriminaÃ§Ã£o. Os genÃ³tipos dentro quadrante IV sÃ£o altamente produtivos e estÃ¡veis.","code":"\np5 = plot_scores(BLUP_model, type = 3)\np6 = plot_scores(BLUP_model, type = 3) +\n                 theme_gray() +\n                 theme(legend.position = c(0.1, 0.9),\n                       legend.background = element_rect(fill = NA))\n\narrange_ggplot(p5, p6, labels = c(\"p5\",\"p6\"))"},{"path":"interaction.html","id":"o-Ã­ndice-waasby","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.5.2 O Ã­ndice WAASBY","text":"Um novo Ã­ndice de superioridade que considera tanto estabilidade quanto produtividade para classificaÃ§Ã£o dos genÃ³tipos tambÃ©m foi proposto por T. Olivoto, LÃºcio, Da silva, Marchioro, et al. (2019), considerando seguinte equaÃ§Ã£o.\\[\r\nWAASB{Y_i} = \\frac{{\\left( {r{G_i} \\times {\\theta _Y}} \\right) + \\left( {r{W_i} \\times {\\theta _S}} \\right)}}{{{\\theta _Y} + {\\theta _S}}}\r\n\\]onde \\(WAASBY_i\\) Ã© o Ã­ndice de superioridade para o genÃ³tipo que pondera entre desempenho e estabilidade; \\(rG_i\\) e \\(rW_i\\) sÃ£o os valores escalonados (0-100) para GY e WAASB, respectivamente; \\(\\theta_Y\\) e \\(\\theta_S\\) sÃ£o os pesos para GY e WAASB, respectivamente.Este Ã­ndice permite atribuir pesos para estabilidade e produtividade na classificaÃ§Ã£o dos genÃ³tipos. Para isto, o argumento wresp da funÃ§Ã£o waasb()  Ã© usado. Por exemplo, se o objetivo Ã© selecionar genÃ³tipos com alto rendimento (independentemente de sua estabilidade), entÃ£o o wresp = 100 deve ser usado. Nesse caso, classificaÃ§Ã£o Ã­ndice WAASBY corresponderÃ¡ perfeitamente Ã  classificaÃ§Ã£o da variÃ¡vel de resposta. Por outro lado, visando selecionar genÃ³tipos altamente estÃ¡veis (independentemente da produtividade), entÃ£o o wresp = 0 deve ser usado. Nesse caso, classificaÃ§Ã£o Ã­ndice WAASBY corresponderÃ¡ perfeitamente Ã  classificaÃ§Ã£o Ã­ndice WAASB. Qualquer valor entre 0 e 100 pode ser usado argumento wresp para ponderar entre o desempenho mÃ©dio e estabilidade. Em nosso exemplo, o Ã­ndice WAASBY foi calculado considerando wresp = 50 (padrÃ£o da funÃ§Ã£o), o que significa pesos iguais para desempenho e estabilidade mÃ©dios. Para plotar os valores de WAASBY, o cÃ³digo seguir Ã© usado.O reescalonamento da variÃ¡vel resposta e Ã­ndice WAASB Ã© necessÃ¡rio para que eles sejam diretamente comparÃ¡veis. Por padrÃ£o, variÃ¡vel resposta Ã© reescalonada de forma que o valor mÃ¡ximo (genÃ³tipo com maior mÃ©dia) seja 100 e o valor mÃ­nimo (genÃ³tipo com menor mÃ©dia) seja 0. Digamos que para uma determinada variÃ¡vel, menores valores sÃ£o desejados, entÃ£o o argumento mresp = 100 (padrÃ£o) deve ser ajustado para mresp = 0.Par obter o Ã­ndice WAASBY para mÃºltiplas variÃ¡veis, por exemplo, basta utilizar funÃ§Ã£o get_model_data(), como mostrado abaixo.","code":"\np1 <- plot_waasby(BLUP_model)\np2 <- plot_waasby(BLUP_model,\n                  col.shape = c(\"black\", \"gray\")) +\n  coord_flip()\nwaas(data_ge2, ENV, GEN, REP,\n     resp = c(PH, ED, TKW, NKR),\n     wresp = rep(65, 4)) %>%\n get_model_data(what = \"WAASY\")\n# variable PH \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  7.719  2.5728 127.913 4.25e-07          .           .\n#   REP(ENV)   8  0.161  0.0201   0.897 5.22e-01          .           .\n#        GEN  12  1.865  0.1554   6.929 6.89e-09          .           .\n#    GEN:ENV  36  5.397  0.1499   6.686 5.01e-14          .           .\n#        PC1  14  4.466  0.3190  14.230 0.00e+00       82.8        82.8\n#        PC2  12  0.653  0.0545   2.430 8.40e-03       12.1        94.9\n#        PC3  10  0.277  0.0277   1.240 2.76e-01        5.1         100\n#  Residuals  96  2.153  0.0224      NA       NA          .           .\n#      Total 191 22.692  0.1188      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable ED \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  306.0  101.99  43.386 2.70e-05          .           .\n#   REP(ENV)   8   18.8    2.35   0.906 5.15e-01          .           .\n#        GEN  12  212.9   17.74   6.838 8.95e-09          .           .\n#    GEN:ENV  36  398.2   11.06   4.263 7.60e-09          .           .\n#        PC1  14  212.2   15.16   5.840 0.00e+00       53.3        53.3\n#        PC2  12  134.7   11.23   4.330 0.00e+00       33.8        87.1\n#        PC3  10   51.3    5.13   1.980 4.38e-02       12.9         100\n#  Residuals  96  249.1    2.59      NA       NA          .           .\n#      Total 191 1583.2    8.29      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable TKW \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  37013   12338   11.13 3.16e-03          .           .\n#   REP(ENV)   8   8869    1109    1.21 3.03e-01          .           .\n#        GEN  12  44633    3719    4.05 4.41e-05          .           .\n#    GEN:ENV  36 164572    4571    4.98 1.73e-10          .           .\n#        PC1  14 104276    7448    8.11 0.00e+00       63.4        63.4\n#        PC2  12  33361    2780    3.03 1.20e-03       20.3        83.6\n#        PC3  10  26935    2694    2.93 3.00e-03       16.4         100\n#  Residuals  96  88171     918      NA       NA          .           .\n#      Total 191 507829    2659      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable NKR \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  237.0   79.01  15.843 0.000997          .           .\n#   REP(ENV)   8   39.9    4.99   0.635 0.746348          .           .\n#        GEN  12  227.8   18.99   2.418 0.008726          .           .\n#    GEN:ENV  36  602.7   16.74   2.132 0.001839          .           .\n#        PC1  14  337.4   24.10   3.070 0.000600         56          56\n#        PC2  12  192.2   16.02   2.040 0.028500       31.9        87.9\n#        PC3  10   73.1    7.31   0.930 0.509500       12.1         100\n#  Residuals  96  753.7    7.85      NA       NA          .           .\n#      Total 191 2463.8   12.90      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!\n# Class of the model: waas\n# Variable extracted: WAASY\n# # A tibble: 13 x 5\n#    gen      PH    ED   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     73.6 78.1   84.5  42.7\n#  2 H10    22.0 18.8   34.8  56.3\n#  3 H11    42.5 39.2   56.6  56.3\n#  4 H12    27.5 51.5   41.0  35  \n#  5 H13    49.2 60.2   70.7  25.1\n#  6 H2     66.3 54.9   62.5  51.6\n#  7 H3     59.4 57.9   52.1  48.5\n#  8 H4     68.6 59.4   59.8  98.7\n#  9 H5     85.3 61.6   69.9  75.2\n# 10 H6     67.5 95.7   90.3  34.5\n# 11 H7     41.4 62.8   55.8  42.6\n# 12 H8     11.6 32.2   17.3  29.2\n# 13 H9     45.5  7.93   0    48.4"},{"path":"interaction.html","id":"diferentes-cenÃ¡rios-para-a-estimativa-do-waasby","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.5.3 Diferentes cenÃ¡rios para a estimativa do WAASBY","text":"exemplo seguir, aplicaremos funÃ§Ã£o wsmp()  (weighting stability mean performance) ao modelo previamente ajustado BLUP_model visando planejar diferentes cenÃ¡rios de estimaÃ§Ã£o WAASBY alterando os pesos atribuÃ­dos Ã  estabilidade e ao desempenho mÃ©dio. O nÃºmero de cenÃ¡rios Ã© definido pelo argumento increment. Por padrÃ£o, vinte e um cenÃ¡rios diferentes sÃ£o simulados. Neste caso, o Ã­ndice de superioridade WAASBY Ã© calculado considerando os seguintes pesos: estabilidade = 100; desempenho mÃ©dio = 0. Em outras palavras, apenas estabilidade Ã© considerada para classificaÃ§Ã£o dos genÃ³tipos. Na prÃ³xima iteraÃ§Ã£o, os pesos se tornam 95/5 (uma vez que increment = 5). terceiro cenÃ¡rio, os pesos se tornam 90/10, e assim por diante atÃ© esses pesos se tornarem 0/100. Na Ãºltima iteraÃ§Ã£o, classificaÃ§Ã£o genÃ³tipo para o WAASBY corresponde perfeitamente Ã s classificaÃ§Ãµes da variÃ¡vel resposta.funÃ§Ã£o genÃ©rica plot()  Ã© entÃ£o usada para plotar o objeto. Dois heatmaps sÃ£o criados. O primeiro tipo mostra classificaÃ§Ã£o genÃ³tipo dependendo nÃºmero de eixos de componentes principais usados para estimar o Ã­ndice WAASB. Um dendrograma baseado na distÃ¢ncia euclidiana Ã© usado para agrupar classificaÃ§Ã£o dos genÃ³tipos. O segundo tipo mostra classificaÃ§Ã£o genÃ³tipo dependendo da relaÃ§Ã£o WAASB/GY. classificaÃ§Ãµes obtidas cenÃ¡rio 100/0 consideram exclusivamente estabilidade para o ranking de genÃ³tipos. Por outro lado, o cenÃ¡rio 0/100 considera exclusivamente produtividade para o ranking de genÃ³tipos.","code":"\nscenarios <- wsmp(BLUP_model)\np1 <- plot(scenarios)\np2 <- plot(scenarios, type = 2)\narrange_ggplot(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"interaction.html","id":"performance-e-mÃ©dia-harmÃ´nica-dos-valores-genotÃ­picos","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.11.5.4 Performance e mÃ©dia harmÃ´nica dos valores genotÃ­picos","text":"funÃ§Ã£o Resende_indexes() computa os Ã­ndices MÃ©dia HarmÃ´nica dos Valores GenotÃ­picos (MHVG), Performance Relativa dos Valores GenotÃ­picos (PRVG) e MÃ©dia HarmÃ´nica da Performance Relativa dos Valores Genotipicos (MHPRVG), conforme descrito em Colombari Filho et al. (2013). Estes Ã­ndices sÃ£o calculados para cada genÃ³tipo de acordo com seguintes equaÃ§Ãµes.\\[\r\nMHVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {\\frac{1}{{G{v_{ij}}}}}\r\n\\]\\[\r\nPRVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {G{v_{ij}}/{\\mu_j}}\r\n\\]\\[\r\nMHPRVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {\\frac{1}{{G{v_{ij}}/{\\mu_j}}}}\r\n\\]onde e Ã© o nÃºmero de ambientes incluÃ­dos na anÃ¡lise, \\(Gv_{ij}\\) Ã© o valor genotÃ­pico (BLUP) para o genÃ³tipo ambiente j.","code":"\nIndexes <- Resende_indexes(BLUP_model)\nprint(Indexes$GY)\n# # A tibble: 10 x 10\n#    GEN       Y  HMGV HMGV_R  RPGV RPGV_Y RPGV_R HMRPGV HMRPGV_Y HMRPGV_R\n#    <fct> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>    <dbl>    <dbl>\n#  1 G1     2.60  2.32      6 0.969   2.59      6  0.967     2.59        6\n#  2 G10    2.47  2.11     10 0.913   2.44     10  0.896     2.40       10\n#  3 G2     2.74  2.47      4 1.03    2.74      4  1.02      2.73        4\n#  4 G3     2.96  2.68      2 1.11    2.96      2  1.10      2.95        2\n#  5 G4     2.64  2.39      5 0.990   2.65      5  0.988     2.64        5\n#  6 G5     2.54  2.30      8 0.954   2.55      7  0.952     2.55        8\n#  7 G6     2.53  2.30      7 0.954   2.55      8  0.952     2.55        7\n#  8 G7     2.74  2.52      3 1.04    2.77      3  1.03      2.76        3\n#  9 G8     3.00  2.74      1 1.13    3.01      1  1.12      3.00        1\n# 10 G9     2.51  2.18      9 0.926   2.48      9  0.917     2.45        9"},{"path":"interaction.html","id":"ammi-ou-blup-decisÃ£o-em-cada-caso","chapter":"CapÃ­tulo 14 InteraÃ§Ã£o genÃ³tipo-vs-ambiente","heading":"14.12 AMMI ou BLUP? DecisÃ£o em cada caso!","text":"Vimos anteriormente que um membro da famÃ­lia de modelos AMMI (AMMI2) Ã© o mais preciso na prediÃ§Ã£o da variÃ¡vel resposta em nosso exemplo. ApÃ³s analizar-mos os mesmos dados utilizando modelos mistos, nos surge seguinte questÃ£o. Qual Ã© o mÃ©todo mais preciso para predizer variÃ¡vel resposta em nosso exemplo? resposta esta pergunda serÃ¡ dada agora. O pacote metan tambÃ©m conta com uma funÃ§Ã£o para cross-validation considerando o modelo misto acima. idÃ©ia Ã© simples. Relizaremos uma validaÃ§Ã£o cruzada semelhante Ã quela realizada modelo AMMI e compararemos o RMSPD dos valores preditos utilizando BLUP com aqueles obtidos pelo modelo AMMI.funÃ§Ã£o cv_blup() realiza uma validaÃ§Ã£o cruzada para experimentos com repetiÃ§Ã£o usando modelos mistos. Por padrÃ£o, blocos completos sÃ£o selecionados aleatoriamente em cada ambiente. O procedimento para calcular o RMSPD Ã© idÃªntico ao apresentado na anÃ¡lise AMMI.Para unir os resultados obtidos pelas funÃ§Ãµes cv_ammif() e cv_blup() funÃ§Ã£o bind_cv(). Usando o argumento bind = \"means\" Ã© possÃ­vel concatenar os valores mÃ©dios obtidos por cada modelo.   Um grÃ¡fico boxplot tambÃ©m pode ser obtido utilizando o argumento bind = \"boot\" (padrÃ£o).","code":"\nvalida_blup <- cv_blup(data_ge, ENV, GEN, REP, GY, nboot = 20)\nval_means <- bind_cv(AMMIF, valida_blup, bind = \"means\")\nval_means$RMSPD\n# # A tibble: 11 x 6\n#    MODEL        mean     sd      se  Q2.5 Q97.5\n#    <fct>       <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#  1 BLUP_g_RCBD 0.398 0.0226 0.00506 0.368 0.440\n#  2 AMMI2       0.411 0.0242 0.00171 0.369 0.460\n#  3 AMMI4       0.416 0.0234 0.00166 0.373 0.462\n#  4 AMMI3       0.416 0.0215 0.00152 0.377 0.457\n#  5 AMMI5       0.422 0.0232 0.00164 0.377 0.464\n#  6 AMMI6       0.422 0.0219 0.00155 0.381 0.462\n#  7 AMMI7       0.425 0.0199 0.00141 0.381 0.458\n#  8 AMMI8       0.427 0.0204 0.00145 0.387 0.462\n#  9 AMMIF       0.429 0.0213 0.00150 0.390 0.469\n# 10 AMMI1       0.430 0.0251 0.00178 0.384 0.479\n# 11 AMMI0       0.430 0.0283 0.00200 0.370 0.485\nval_plot <- bind_cv(AMMIF, valida_blup)\np1 <-plot(val_plot)\np2 <-plot(val_plot,\n          width.boxplot = 0.6,\n          col.boxplot = \"cyan\")\narrange_ggplot(p1, p2)"},{"path":"resposta-dos-exercícios.html","id":"resposta-dos-exercÃ­cios","chapter":"A Resposta dos exercÃ­cios","heading":"A Resposta dos exercÃ­cios","text":"","code":""},{"path":"resposta-dos-exercícios.html","id":"exerc1","chapter":"A Resposta dos exercÃ­cios","heading":"A.1 ExercÃ­cio 1","text":"O resultado foi o mesmo, pois embora se tenha invertido o valor dos nÃºmeros, segundo exemplo se declarou qual argumento o numero pertencial.Pois o argumento (x > 10) faz com que ocorra um erro e funÃ§Ã£o nÃ£o seja executada.O argumento â€˜elevaâ€™ nÃ£o estÃ¡ correto. Ele deve ser ou â€˜quadradoâ€™ ou â€˜cuboâ€™.","code":"\nF2(2, 3)\n# [1] 8\nF2(y = 3, x =2)\n# [1] 8\nF3(20)\n# Error in F3(20): O argumento x = 20 Ã© invÃ¡lido. 'x' precisa ser maior que 10\nelevar(12, eleva = \"cubico\")\n# Error in elevar(12, eleva = \"cubico\"): O argumento eleva = cubico deve ser ou 'quadrado' ou 'cubo'\nmega = function(jogos, numeros = 6){\n  if(!numeros %in% c(6:15)){\n    stop(\"O numero deve ser entre 6 e 15\")\n  }\n  result = list()\nfor(i in 1:jogos){\nresult[[i]] = sort(\n  sample(1:60, size = numeros, replace = FALSE)\n  )\n}\n  return(do.call(rbind, result))\n}\n\n# 4 jogos\nmega(5, 10)\n#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n# [1,]    2   10   20   25   28   32   37   47   57    58\n# [2,]    1    3    9   13   18   29   34   36   47    50\n# [3,]    1    9   14   16   21   25   31   47   50    53\n# [4,]    8    9   11   15   24   26   29   56   57    60\n# [5,]    3    5   21   22   24   27   51   52   55    60"},{"path":"resposta-dos-exercícios.html","id":"exerc2","chapter":"A Resposta dos exercÃ­cios","heading":"A.2 ExercÃ­cio 2","text":"","code":"\nmaize %>%\n  mutate(MGRA_kg = MGRA / 1000) %>%\n  select(AMB, HIB, REP, MGRA_kg) %>%\n  top_n(5, MGRA_kg)"},{"path":"resposta-dos-exercícios.html","id":"exerc3","chapter":"A Resposta dos exercÃ­cios","heading":"A.3 ExercÃ­cio 3","text":"","code":"\nmaize %>%\n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA)) %>%\n  mutate(Rank = rank(MGRA_mean)) %>%\n  arrange(Rank)\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc4","chapter":"A Resposta dos exercÃ­cios","heading":"A.4 ExercÃ­cio 4","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH, colour = AMB, size = APLA)) +\ngeom_point()\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc5","chapter":"A Resposta dos exercÃ­cios","heading":"A.5 ExercÃ­cio 5","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH, colour = GEN)) +\n     geom_point() +\n     facet_wrap(~AMB)+\n     my_theme()\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc6","chapter":"A Resposta dos exercÃ­cios","heading":"A.6 ExercÃ­cio 6","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH)) +\n     geom_point(aes(colour = AMB))+\n     geom_smooth(method = \"lm\", se = F)+\n     my_theme()+\n     labs(x = \"Rendimento de grÃ£os\", y = \"Peso hectolitro\")\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc7","chapter":"A Resposta dos exercÃ­cios","heading":"A.7 ExercÃ­cio 7","text":"","code":"\nmeans = qualitativo %>% \n  group_by(HIBRIDO) %>% \n  summarise(RG = mean(RG)) %>%\n  mutate(letras = \"a\")\nggplot(means, aes(x = HIBRIDO, y = RG)) +\n  geom_bar(stat = \"identity\", col = \"black\", fill = \"orange\", width = 0.5)+\n  scale_y_continuous(expand = expand_scale(c(0, .1)))+\n  geom_text(aes(label = letras), hjust = -1, size = 3.5)+\n  geom_hline(yintercept = mean(means$RG), linetype = \"dashed\")+\n  coord_flip()"},{"path":"resposta-dos-exercícios.html","id":"exerc8","chapter":"A Resposta dos exercÃ­cios","heading":"A.8 ExercÃ­cio 8","text":"","code":"\nplot_lines(quantitativo, x = DOSEN, y = RG, fit = 2, col = F)"},{"path":"resposta-dos-exercícios.html","id":"exerc9","chapter":"A Resposta dos exercÃ­cios","heading":"A.9 ExercÃ­cio 9","text":"","code":"\nwith(quantitativo, dbc(DOSEN, BLOCO, RG, quali = FALSE))"},{"path":"resposta-dos-exercícios.html","id":"exerc10","chapter":"A Resposta dos exercÃ­cios","heading":"A.10 ExercÃ­cio 10","text":"","code":"\nres = tibble(Convencional = residuals(convencional),\n             Transformado = residuals(transform),\n             Generalizado = residuals(general, type = \"deviance\"))\nshapiro.test(res$Convencional)\nshapiro.test(res$Transformado)\nshapiro.test(res$Generalizado)"},{"path":"resposta-dos-exercícios.html","id":"exerc11","chapter":"A Resposta dos exercÃ­cios","heading":"A.11 ExercÃ­cio 11","text":"","code":"\nplot_factbars(FAT1_CI,\n              HIBRIDO,\n              FONTEN,\n              resp = RG,\n              palette = \"Greys\")"},{"path":"resposta-dos-exercícios.html","id":"exerc12","chapter":"A Resposta dos exercÃ­cios","heading":"A.12 ExercÃ­cio 12","text":"","code":"\nNUPEC_1 <- \n  FAT2_CI %>%\n  filter(HIBRIDO == \"NUPEC_1\")\nggplot(NUPEC_1, aes(x = DOSEN, y = RG)) +\ngeom_point()+\nstat_smooth(method = \"lm\", formula = as.formula(\"y ~ poly(x, 2)\")) +\ngeom_vline(xintercept = 50, linetype = \"dashed\", col = \"gray\") +\ngeom_vline(xintercept = 48, col = \"gray\")"},{"path":"resposta-dos-exercícios.html","id":"exerc13","chapter":"A Resposta dos exercÃ­cios","heading":"A.13 ExercÃ­cio 13","text":"","code":"\ncovar_mat = maize %>%\n  split_factors(ENV, keep_factors = TRUE) %>%\n  covcor_design(gen = GEN,\n                rep = REP,\n                resp = c(PH, EH, NKE, TKW),\n                type = \"rcov\")"},{"path":"tabela-de-distribuições.html","id":"tabela-de-distribuiÃ§Ãµes","chapter":"Tabela de distribuiÃ§Ãµes","heading":"Tabela de distribuiÃ§Ãµes","text":"Tabela 1. Limite unilateral da cauda direita da distribuiÃ§Ã£o F de Fisher-Snedecor, 0,01 de probabilidade de erro\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 2. Limite unilateral da cauda direita da distribuiÃ§Ã£o F de Fisher-Snedecor, 0,05 de probabilidade de erro\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 3. Valores da estatÃ­stica q para teste de Tukey\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 4. Valores crÃ­ticos da distribuiÃ§Ã£o t de Student bicaudal em diferentes probabilidades\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 5. Valores da distribuiÃ§Ã£o normal padrÃ£o. Primeira decimal de Z nas linhas e segunda decimal de Z nas colunas\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 6. Valores crÃ­ticos (funÃ§Ã£o inversa) em relaÃ§Ã£o cauda esquerda da distribuiÃ§Ã£o Chi-quadrado\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 7. Valores crÃ­ticos (funÃ§Ã£o inversa) em relaÃ§Ã£o cauda direita da distribuiÃ§Ã£o Chi-quadrado\r\n","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Altman, N., M. Krzywinski. 2017. â€œInterpreting P values.â€ Nature Methods 14 (3): 213â€“14. https://doi.org/10.1038/nmeth.4210.Anderson, T. W. 2003. introduction multivariate statistical analysis. 3rd ed. Wiley-Interscience.Annicchiarico, P. 1992. â€œCultivar adaptation recommendation alfalfa trials Northern Italy.â€ Journal Genetics Breeding 46: 269â€“78.Baker, Monya. 2016. â€œStatisticians issue warning misuse P values.â€ Nature 531 (7593): 151â€“51. https://doi.org/10.1038/nature.2016.19503.Bartlett, M. S. 1947. â€œThe Use Transformations.â€ Biometrics 3 (1): 39â€“52. https://doi.org/10.2307/3001536.Bates, D. M., D. G. Watts. 1988. Nonlinear Regression Analysis Applications. 2nd ed. Wiley Series Probability Statistics. Hoboken, NJ, USA: John Wiley & Sons, Inc. https://doi.org/10.1002/9780470316757.Blalock, H M. 1963. â€œCorrelated {Independent} {Variables}: {} {Problem} {Multicollinearity}.â€ Social Forces 42 (2): 233â€“37. https://doi.org/10.1093/sf/42.2.233.Blanca, M. J., R. AlarcÃ³n, J. Arnau, R. Bono, R. Bendayan. 2017. â€œNon-normal data: ANOVA still valid option?â€ Psicothema 29 (4): 552â€“57. https://doi.org/10.7334/psicothema2016.383.Box, G. E. P., D. R. Cox. 1964. â€œAn Analysis Transformations.â€ Journal Royal Statistical Society. Series B (Methodological) 211-252: 211â€“52. https://doi.org/10.2307/2984418.Breslow, N. E., D. G. Clayton. 1993. â€œApproximate Inference Generalized Linear Mixed Models.â€ Journal American Statistical Association 88 (421): 9â€“25. https://doi.org/10.2307/2290687.Casella, George. 2008. Statistical Design. Springer Texts Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-75965-4.Charrad, Malika, Nadia Ghazzali, VÃ©ronique Boiteau, Azam Niknafs. 2014. â€œ<b>NbClust<\/b> : <>R<\/> Package Determining Relevant Number Clusters Data Set.â€ Journal Statistical Software 61 (6): 1â€“36. https://doi.org/10.18637/jss.v061.i06.Chawla, D. S. 2017. â€œBig names statistics want shake much-maligned P value.â€ Nature 548 (7665): 16â€“17. https://doi.org/10.1038/nature.2017.22375.Cochran, W. G. 1940. â€œThe Analysis Variance Experimental Errors Follow Poisson Binomial Laws.â€ Annals Mathematical Statistics 11 (3): 335â€“47. https://doi.org/10.1214/aoms/1177731871.Colombari Filho, J. M., M. D. V. Resende, O. P. Morais, . P. Castro, Ã‰. P. GuimarÃ£es, J. . Pereira, M. M. Utumi, F. Breseghello. 2013. â€œUpland rice breeding Brazil: simultaneous genotypic evaluation stability, adaptability grain yield.â€ Euphytica 192 (1): 117â€“29. https://doi.org/10.1007/s10681-013-0922-2.Dempster, . P., N. M. Laird, D. B. Rubin. 1977. â€œMaximum likelihood incomplete data via EM algorithm.â€ Journal Royal Statistical Society, Series B 39 (1): 1â€“38. http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4884.Draper, Norman R, Harry Smith. 1998. Applied regression analysis. John Wiley & Sons. https://books.google.com.br/books?hl=pt-BR{\\&}lr={\\&}id=uSReBAAAQBAJ{\\&}oi=fnd{\\&}pg=PT12{\\&}ots=P9bwDDSasT{\\&}sig=4RbKqw-ZEgE40xtra-EsI9dLDPk.Eberhart, S. ., W. . Russell. 1966. â€œStability parameters comparing Varieties.â€ Crop Science 6 (1): 36â€“40. https://doi.org/10.2135/cropsci1966.0011183X000600010011x.Eisenhart, C. 1947. â€œThe assumptions underlying analysis variance.â€ Biometrics 3 (1): 1â€“21. http://www.ncbi.nlm.nih.gov/pubmed/20240414.Farshadfar, E. 2008. â€œIncorporation AMMI stability value grain yield single non-parametric index (GSI) bread wheat.â€ Pakistan Journal Biological Sciences 11 (14): 1791â€“6. http://www.ncbi.nlm.nih.gov/pubmed/18817218.Ferreira, D. F. 2009. Estatistica Basica. ViÃ§osa, MG.: UFV.Ferreira, E. B., P. P. Cavalcanti, D. . Nogueira. 2018. â€œExpDes: Experimental Designs.â€ https://cran.r-project.org/web/packages/ExpDes/index.html.Field, ., J. Miles, Z. Field. 2012. Discovering Statistics Using R. SAGE Publications Ltd. https://us.sagepub.com/en-us/sam/discovering-statistics-using-r/book236067.Fisher, R. . 1925. Statistical methods research workers. 11th ed. Edinburgh: Oliver; Boyd.â€”â€”â€”. 1935. design experiments. Edinburgh: Oliver; Boyd.Fisher, R. ., W. . Mackenzie. 1923. â€œStudies crop variation. II. manurial response different potato varieties.â€ Journal Agricultural Science 13 (03): 311â€“20. https://doi.org/10.1017/S0021859600003592.Friedman, M. 1937. â€œThe Use Ranks Avoid Assumption Normality Implicit Analysis Variance.â€ Journal American Statistical Association 32 (200): 675â€“701. https://doi.org/10.2307/2279372.Gabriel, K. R. 1971. â€œThe Biplot Graphic Display Matrices Application Principal Component Analysis.â€ Biometrika 58 (3): 453â€“67. https://doi.org/10.2307/2334381.Galton, Francis. 1888. â€œCo-relations measurement, chiefly anthropometric data.â€ Proceedings Royal Society London 45 (273-279): 135â€“45. http://rspl.royalsocietypublishing.org/content/45/273-279/135.full.pdf.Gauch, H. G. 1988. â€œModel Selection Validation Yield Trials Interaction.â€ Biometrics 44 (3): 705â€“15. https://doi.org/10.2307/2531585.Gollob, H. F. 1968. â€œA statistical model combines features factor analytic analysis variance techniques.â€ Psychometrika 33 (1): 73â€“115. https://doi.org/10.1007/BF02289676.Graham, Michael H. 2003. â€œConfronting Multicollinearity Ecological Multiple Regression.â€ Ecology 84 (11): 2809â€“15. https://doi.org/10.1890/02-3114.Halkidi, Maria, Yannis Batistakis, Michalis Vazirgiannis. 2001. â€œOn Clustering Validation Techniques.â€ Journal Intelligent Information Systems 17 (2/3): 107â€“45. https://doi.org/10.1023/:1012801612483.Hartigan, J. ., M. . Wong. 1979. â€œAlgorithm 136: K-Means Clustering Algorithm.â€ Applied Statistics 28 (1): 100â€“108. https://doi.org/10.2307/2346830.Henderson, C. R. 1949. â€œEstimation changes herd environment.â€ Journal Dairy Science 32: 706.â€”â€”â€”. 1950. â€œEstimation genetic parameters.â€ Annals Mathematical Statistics 21: 309â€“10.â€”â€”â€”. 1953. â€œEstimation Variance Covariance Components.â€ Biometrics 9 (2): 226â€“52. https://doi.org/10.2307/3001853.â€”â€”â€”. 1975. â€œBest Linear Unbiased Estimation Prediction Selection Model.â€ Biometrics 31 (2): 423â€“47. https://doi.org/10.2307/2529430.Hoerl, Arthur E, Robert W Kennard. 1976. â€œRidge regression iterative estimation biasing parameter.â€ Communications Statistics - Theory Methods 5 (1): 77â€“88. https://doi.org/10.1080/03610927608827333.Hoerl, Arthur E., Robert W. Kennard. 1970. â€œRidge Regression: Biased Estimation Nonorthogonal Problems.â€ Technometrics 12 (1): 55â€“67. https://doi.org/10.1080/00401706.1970.10488634.Hu, X. 2015. â€œA comprehensive comparison ANOVA BLUP valuate location-specific genotype effects rape cultivar trials random locations.â€ Field Crops Research 179: 144â€“49. https://doi.org/10.1016/j.fcr.2015.04.023.Hubert, Lawrence, Phipps Arabie. 1985. â€œComparing partitions.â€ Journal Classification 2 (1): 193â€“218. https://doi.org/10.1007/BF01908075.Kaiser, Henry F. 1961. â€œA Note Guttmanâ€™s Lower Bound Number Common Factors.â€ British Journal Statistical Psychology 14 (1): 1â€“2. https://doi.org/10.1111/j.2044-8317.1961.tb00061.x.Koopman, B. O. 1936. â€œOn distributions admitting sufficient statistic.â€ Transactions American Mathematical Society 39 (3): 399â€“409. https://doi.org/10.1090/S0002-9947-1936-1501854-3.Kozak, M., H.-P. Piepho. 2017. â€œWhatâ€™s normal anyway? Residual plots telling significance tests checking ANOVA assumptions.â€ Journal Agronomy Crop Science 204: 86â€“98. https://doi.org/10.1111/jac.12220.Kruskal, W. H., W. . Wallis. 1952. â€œUse Ranks One-Criterion Variance Analysis.â€ Journal American Statistical Association 47 (260): 583â€“621. https://doi.org/10.2307/2280779.Krzanowski, W. J., Y. T. Lai. 1988. â€œA Criterion Determining Number Groups Data Set Using Sum--Squares Clustering.â€ Biometrics 44 (1): 23â€“34. https://doi.org/10.2307/2531893.Krzywinski, M., N. Altman. 2013. â€œSignificance, P values t-tests.â€ Nature Methods 10 (11): 1041â€“2. https://doi.org/10.1038/nmeth.2698.Kutner, Michael H., Chris Nachtsheim, John Neter, William Li. 2005. Applied linear statistical models.Langsrud, Ã˜. 2003. â€œANOVA unbalanced data: Use Type II instead Type III sums squares.â€ Statistics Computing 13 (2): 163â€“67. https://doi.org/10.1023/:1023260610025.Laurent, R., P. Turk. 2013. â€œThe Effects Misconceptions Properties Friedmanâ€™s Test.â€ Communications Statistics - Simulation Computation 42 (7): 1596â€“1615. https://doi.org/10.1080/03610918.2012.671874.Lin, C. S., M. R. Binns. 1988. â€œA superiority measure cultivar performance cultivar x location data.â€ Canadian Journal Plant Science 68 (1): 193â€“98. https://doi.org/10.4141/cjps88-018.Lucio, Alessandro Dal Col, Luis F Nunes, Francisco Rego. 2016. â€œNonlinear regression plot size estimate green beans production.â€ Horticultura Brasileira 34 (4): 507â€“13. https://doi.org/10.1590/s0102-053620160409.LÃºcio, Alessandro Dal Col, Luis Filipe Nunes, Francisco Rego. 2015. â€œNonlinear models describe production fruit Cucurbita pepo Capiscum annuum.â€ Scientia Horticulturae 193: 286â€“93. https://doi.org/10.1016/j.scienta.2015.07.021.LÃºcio, Alessandro D., Daniel Santos, Tiago Olivoto. 2017. â€œVariability Experimental Desing Trials Cucurbita pepo Capsicum annuum.â€ Journal Agricultural Science 9 (11): 58â€“75. https://doi.org/10.5539/jas.v9n11p58.Mayer, ., B. Nagengast, J. Fletcher, R. Steyer. 2014. â€œAnalyzing average conditional effects multigroup multilevel structural equation models.â€ Frontiers Psychology 5 (304): 1â€“16. https://doi.org/10.3389/fpsyg.2014.00304.Miller, G. ., J. P. Chapman. 2001. â€œMisunderstanding analysis covariance.â€ Journal Abnormal Psychology 110 (1): 40â€“48. http://www.ncbi.nlm.nih.gov/pubmed/11261398.Milligan, Glenn W., Martha C. Cooper. 1985. â€œAn examination procedures determining number clusters data set.â€ Psychometrika 50 (2): 159â€“79. https://doi.org/10.1007/BF02294245.Mojena, R. 1977. â€œHierarchical grouping methods stopping rules: evaluation.â€ Computer Journal 20 (4): 359â€“63. https://doi.org/10.1093/comjnl/20.4.359.Mora, F., L. M. Goncalves, C. . Scapim, E. N. Martins, M. F. P. S. Machado. 2008. â€œGeneralized lineal models analysis binary data propagation experiments Brazilian orchids.â€ Brazilian Archives Biology Technology 51 (5): 963â€“70. https://doi.org/10.1590/S1516-89132008000500013.Murakami, D. M., C. D. Cruz. 2004. â€œProposal methodologies environment stratification analysis genotype adaptability.â€ Crop Breeding Applied Biotechnology 4 (1): 7â€“11. http://www.sbmp.org.br/cbab/siscbab/uploads/c8128f42-aefe-cdf5.pdf.Nelder, J. ., R. W. M. Wedderburn. 1972. â€œGeneralized Linear Models.â€ Journal Royal Statistical Society. Series (General) 135 (3): 370â€“84. https://doi.org/10.2307/2344614.Niles, Henry E. 1922. â€œCorrelation, Causation Wrightâ€™s Theory \"Path Coefficients\".â€ Genetics 7 (3): 258â€“73. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1200533/.Nuzzo, Regina. 2014. â€œScientific method: Statistical errors.â€ Nature 506 (7487): 150â€“52. https://doi.org/10.1038/506150a.Olivoto, Tiago, Alessandro Dalâ€™Col LÃºcio. 2020. â€œmetan: R package multiâ€environment trial analysis.â€ Edited Simon Jarman. Methods Ecology Evolution 11 (6): 783â€“89. https://doi.org/10.1111/2041-210X.13384.Olivoto, Tiago, Maicon Nardino, Ivan Ivan Ricardo Carvalho, Diego Nicolau Follmann, MaurÃ­cio Ferrari, Alan JÃºnior de Pelegrin, VinÃ­cius Jardel Szareski, AntÃ´nio Costa de Oliveira, Braulio Otomar Caron, Velcio Queiroz de Souza. 2017. â€œOptimal sample size data arrangement method estimating correlation matrices lesser collinearity: statistical focus maize breeding.â€ African Journal Agricultural Research 12 (2): 93â€“103. https://doi.org/10.5897/AJAR2016.11799.Olivoto, T., . D. C LÃºcio, J. . G. Da silva, V. S. Marchioro, V. Q. de Souza, E. Jost. 2019. â€œMean performance stability multi-environment trials : Combining features AMMI BLUP techniques.â€ Agronomy Journal. https://doi.org/10.2134/agronj2019.03.0220.Olivoto, T., . D. C LÃºcio, J. . G. Da silva, B. G. Sari, M. . Diel. 2019. â€œMean performance stability multi-environment trials II: Selection based multiple traits.â€ Agronomy Journal. https://doi.org/10.2134/agronj2019.03.0221.Olivoto, T., . D. C LÃºcio, V. Q. Souza, M. Nardino, M. . Diel, B. G. Sari, D .K. Krysczun, D. Meira, C. Meier. 2018. â€œConfidence interval width Pearsonâ€™s correlation coefficient: Gaussian-independent estimator based sample size strength association.â€ Agronomy Journal 110 (1): 1â€“8. https://doi.org/10.2134/agronj2017.09.0566.Olivoto, T., M. Nardino, . R. Carvalho, D. N. Follmann, M. Ferrari, V. J. Szareski, . J. de Pelegrin, V. Q. de Souza. 2017. â€œREML/BLUP sequential path analysis estimating genotypic values interrelationships among simple maize grain yield-related traits.â€ Genetics Molecular Research 16 (1): gmr16019525. https://doi.org/10.4238/gmr16019525.Olivoto, T., V. Q. Souza, M. Nardino, . R. Carvalho, M. Ferrari, . J. Pelegrin, V. J. Szareski, D. Schmidt. 2017. â€œMulticollinearity path analysis: simple method reduce effects.â€ Agronomy Journal 109 (1): 131â€“42. https://doi.org/10.2134/agronj2016.04.0196.Patterson, H. D., R. Thompson. 1971. â€œRecovery Inter-Block Information Block Sizes Unequal.â€ Biometrika 58 (3): 545â€“54. https://doi.org/10.2307/2334389.Pearson, K. 1920. â€œNotes History Correlation.â€ Biometrika 13 (1): 25â€“45. https://doi.org/10.2307/2331722.Piepho, H. P., R. N. Edmondson. 2018. â€œA tutorial statistical analysis factorial experiments qualitative quantitative treatment factor levels.â€ Journal Agronomy Crop Science 204 (5): 429â€“55. https://doi.org/10.1111/jac.12267.Purchase, J. L., H. Hatting, C. S. van Deventer. 2000. â€œGenotype Ã— environment interaction winter wheat ( Triticum aestivum L.) South Africa: II. Stability analysis yield performance.â€ South African Journal Plant Soil 17 (3): 101â€“7. https://doi.org/10.1080/02571862.2000.10634878.Rencher, Alvin C., G. Bruce. Schaalje. 2008. Linear models statistics. John Wiley & Sons.Rodrigues-Soares, J. P, G. F. . Jesus, E. L. T. GonÃ§alves, K.TN. Moraes, E. C. Chagas, F. C. M. Chaves, M. . . Belo, Adolfo JatobÃ¡, J. L. P. MouriÃ±o, M. L. Martins. 2018. â€œInduced aerocystitis hemato-immunological parameters Nile tilapia fed supplemented diet essential oil Lippia alba.â€ Brazilian Journal Veterinary Research Animal Science 55 (1): 1â€“12. https://doi.org/10.11606/issn.1678-4456.bjvras.2018.136717.Rutherford, . 2001. Introducing ANOVA ANCOVA : GLM approach. London: SAGE.Sari, B. G. 2018. â€œParametros biologicos da producao de tomateiro via modelo logistico.â€ PhD thesis, Universidade Federal de Santa Maria.Scheiner, S. M., J. Gurevitch. 2001. Design analysis ecological experiments. 2nd ed. New York: Oxford University Press.Schenider, P. R., P. S. P. Schenider, C. . M. Souza. 2009. Analise de regressao aplicada engenharia florestal. Santa Maria: FACOS, UFSM.Scott, . J., M. J. Symons. 1971. â€œClustering Methods Based Likelihood Ratio Criteria.â€ Biometrics 27 (2): 387â€“97. https://doi.org/10.2307/2529003.Seber, G. . F., C. J. Wild. 2003. Nonlinear regression. John Wiley & Sons, Inc. https://www.wiley.com/en-us/Nonlinear+Regression-p-9780471471356.Senoglu, Birdal, Moti L. Tiku. 2001. â€œAnalysis variance experimental design nonnormal error distributions.â€ Communications Statistics - Theory Methods 30 (7): 1335â€“52. https://doi.org/10.1081/STA-100104748.Silverman, B. W. 1998. Density Estimation Statistics Data Analysis. New York: Routledge. https://doi.org/10.1201/9781315140919.Smith, . B., B. R. Cullis, R. Thompson. 2005. â€œThe analysis crop cultivar breeding evaluation trials: overview current mixed model approaches.â€ Journal Agricultural Science 143 (06): 449â€“62. https://doi.org/10.1017/S0021859605005587.Snedecor, G. W., W. G. Cochran. 1967. Statistical methods. 6th ed. Ames: Iowa State University Press.Sneller, C. H., L. Kilgore-Norquest, D. Dombek. 1997. â€œRepeatability Yield Stability Statistics Soybean.â€ Crop Science 37 (2): 383â€“90. https://doi.org/10.2135/cropsci1997.0011183X003700020013x.Stevens, James (James Paul). 2009. Applied multivariate statistics social sciences. Routledge.Stroup, W. W. 2013. Generalized linear mixed models : modern concepts, methods applications. Boca Raton,FL.: CRC Press.â€”â€”â€”. 2015. â€œRethinking Analysis Non-Normal Data Plant Soil Science.â€ Agronomy Journal 107 (2): 811â€“27. https://doi.org/10.2134/agronj2013.0342.Suzuki, R., H. Shimodaira. 2006. â€œPvclust: R package assessing uncertainty hierarchical clustering.â€ Bioinformatics 22 (12): 1540â€“2. https://doi.org/10.1093/bioinformatics/btl117.Wickham, Hadley. 2009. Ggplot2 : elegant graphics data analysis. Springer.Wilkinson, L. 2005. grammar graphics. Springer.Wolfinger, R., M. Oâ€™connell. 1993. â€œGeneralized linear mixed models pseudo-likelihood approach.â€ Journal Statistical Computation Simulation 48 (3-4): 233â€“43. https://doi.org/10.1080/00949659308811554.Wright, Sewall. 1921. â€œCorrelation causation.â€ Journal Agricultural Research 20 (7): 557â€“85. http://www.ssc.wisc.edu/soc/class/soc952/Wright/Wright{\\_}Correlation Causation.pdf.â€”â€”â€”. 1923. â€œThe Theory Path Coefficients Reply Nilesâ€™s Criticism.â€ Genetics 8 (3): 239â€“55. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1200747/.Yan, Weikai. 2002. â€œSingular-Value Partitioning Biplot Analysis Multienvironment Trial Data.â€ Agronomy Journal 94 (5): 990â€“96. https://doi.org/10.2134/agronj2002.0990.Yan, Weikai., Manjit S. Kang. 2003. GGE biplot analysis : graphical tool breeders, geneticists, agronomists. CRC Press.Yan, W., M. S. Kang, B. Ma, S. Woods, P. L. Cornelius. 2007. â€œGGE Biplot vs. AMMI Analysis Genotype--Environment Data.â€ Crop Science 47 (2): 641â€“53. https://doi.org/10.2135/cropsci2006.06.0374.Yates, F. 1940. â€œThe recovery inter-block information balanced incomplete block designs.â€ Annals Eugenics 10 (1): 317â€“25. https://doi.org/10.1111/j.1469-1809.1940.tb02257.x.Zali, H., E. Farshadfar, S. H. Sabaghpour, R. Karimizadeh. 2012. â€œEvaluation genotype Ã— environment interaction chickpea using measures stability AMMI model.â€ Annals Biological Research 3 (7): 3126â€“36. http://eprints.icrisat.ac./7173/.Zoz, T, F Steiner, Zoz, D. D. Castagnara, T. W. Witt, M. D. Zanotto, D. L. Auld. 2018. â€œEffect row spacing plant density grain yield yield components Crambe abyssinica Hochst.â€ Semina: Ciencias Agrarias 39 (1): 393â€“402. https://doi.org/10.5433/1679-0359.2018v39n1p393.","code":""}]
