[{"path":"index.html","id":"prefácio","chapter":"Prefácio","heading":"Prefácio","text":"Atualmente, na área das Ciências Agrárias, identifica-se o uso de diversos sofwares para análise estatística de dados originados em coletas de experimentos. Esta miscelânea de sofwares pode confundir o pesquisador momento de escolher qual é o software que será adotado para suas análises estatísticas, já que existem aqueles que devem ser adquiridas licenças para uso e nem todos disponibilizam opções de todos os métodos de análise estatística de dados.Dentre esses o Software R destaca-se por ser uma linguagem de programação de código aberto open source basicamente destinado para computação estatística e gráficos. Com proposta de organização de um curso e capacitação de acadêmicos e professores envolvidos em Pós-Graduação na Área de Ciências Agrárias, os Drs. Bruno Giacomini Sari e Tiago Olivoto propuseram-se elaborar um documento onde oferecem uma excelente apresentação e introdução ao ambiente R, bem como diversas aplicações de abordagens estatísticas em experimentos agrícolas.Em sua segunda edição ampliada e atualizada apresentam-se variações nos tipos de tratamentos (qualitativos e quantitativos), variações nos desdobramentos das interações e variações nas formas da casualização de experimentos bifatoriais. Uma breve abordagem ao uso de modelos lineares generalizados é apresentada. Técnicas biométricas voltadas ao melhoramento genético vegetal como análise conjunta de experimentos, análise de estabilidade e associações entre variáveis ou grupo de variáveis são também abordadas. Todos os exemplos são reproduzíveis. expectativa é de que este documento seja útil para aqueles pesquisadores que desejam utilizar este ambiente de programação para realização de suas análises estatísticasParabenizamos os autores pela iniciativa e qualidade material oferecido.Alessandro Dal’Col LúcioProfessor Titular, Setor de Experimentação VegetalDepartamento de FitotecniaCentro de Ciências RuraisUniversidade Federal de Santa Maria","code":""},{"path":"index.html","id":"por-que-eu-deveria-ler-este-e-book","chapter":"Prefácio","heading":"Por que eu deveria ler este e-Book","text":"Com uma disponibilidade cada vez maior de bons softwares estatísticos, escolha por um único programa se torna uma tarefa difícil, até mesmo para alguém com vasta experiência na área de análise de dados de experimentos agronômicos. O ambiente de programação R é, também, um poderoso software estatístico. Assim, inúmeras são fontes com informações relacionadas análise de dados, criação de gráficos, etc.grande maioria dos blogs12345678 relacionados ao software R estão na língua Inglesa e mesmo que nos tempos atuais esta não seja uma questão limitante, materiais de qualidade em língua Portuguesa são muito bem-vindos. Por exemplo, R-br9 é lista Brasileira oficial de discussão programa R e tem o propósito de permitir troca de informações entre os usuários de R (em português) e contém inúmeras dicas/discussões sobre mais diversas áreas de estudo.Esta material, voltado para análise de dados de experimentos agronômicos, apresenta teoria e aplicação software R dos procedimentos mais utilizados na análise de experimentos agronômicos. Assim, ela pode servir de referência para aqueles que querem realizar suas análises R, principalmente para os que ainda possuem pouca ou nenhuma experiência com este ambiente de programação.","code":""},{"path":"index.html","id":"estrutura","chapter":"Prefácio","heading":"Estrutura","text":"Este material contém 14 capítulos divididos em 3 principais partes. Na parte (Capítulos 1 5 ) o ambiente R é apresentado. O Capítulo 1, apresenta uma breve introdução sobre os softwares R e RStudio, mostrando como instalar e carregar os pacotes necessários, além de mostrar ao leitor como criar seu primeiro script. O Capítulo 2 apresenta os tipos de objetos. Capítulo 3, principais operações matemáticas são mostradas. Capítulo 4 é mostrado como loops podem ser úteis para repetir um determinado código diversas vezes. Capítulo 5 é mostrado os dados podem ser armazenados em objetos com diferentes classes.parte II (Capítulos 6 9) é voltada para organização, manipulação e apresentação gráfica de dados. Capítulo 6 é mostrado diversos formatos de dados podem ser carregados ambiente R. O Capítulo 7 trata da manimulação dos dados, tais como adição, seleção, resumo e combinação de variáveis. O Capítulo 8 trata da apresentação dos dados utilizando diversos tipos de gráficos, tais como barra, histogramas e gráficos de dispersão. O Capítulo 9 é voltado para exportação dos dados, tanto numérico quanto gráficos.parte III (Capítulos 10 14) é voltada para análise dos dados. O Capítulo 10 trata da análise de dados experimentais, incluindo estatística básica, análise descritiva, análise de experimentos uni- e bi-fatoriais considerando os principais delineamentos, transformações de dados, análise de covariância bem como uma breve abordagem ao uso de modelos lineares generalizados. O Capítulo 11 é voltado exclusivamente para análise de regressão linear e não linear. O Capítulo 12 trata da associação entre variáveis tais como correlação linear, correlação parcial e análise de trilha. Capítulo 13 análise multivariada de dados é apresentada. Por fim –mas não menos importante– Capítulo 14 são apresentados diversos modelos para análise de ensaios multi-ambientes, com ênfase na aplicação dos métodos AMMI10, BLUP11 e GGE12.","code":""},{"path":"index.html","id":"conjunto-de-dados","chapter":"Prefácio","heading":"Conjunto de dados","text":"Exemplos reproduzíveis são muito importante para uma curva de aprendizado satisfatória ambiente de programação R. Os leitores podem interagir com os exemplos deste material ao lê-lo. Por exemplo, é possível, utilizando Ctrl+C, copiar uma programação, colar em seu ambiente de trabalho utilizando Ctrl+V e saber imediatamente o que acontece se certos parâmetros/argumentos de um modelo/análise forem alterados. Todos os dados utilizados estão disponíveis repositório digital deste e-book. Os dados são carregados ambiente R utilizando função import() pacote rio.","code":""},{"path":"sobre-os-autores.html","id":"sobre-os-autores","chapter":"Sobre os autores","heading":"Sobre os autores","text":"","code":""},{"path":"sobre-os-autores.html","id":"tiago-olivoto","chapter":"Sobre os autores","heading":"Tiago Olivoto","text":"Tiago Olivoto é Técnico Agrícola pela Escola Estadual de Educação Básica Viadutos (2008), Engenheiro agrônomo pela Universidade Oeste de Santa Catarina (2014), Mestre em Agronomia: Agricultura e Ambiente pela Universidade Federal de Santa Maria (2017) e Doutor em Agronomia com ênfase em Melhoramento Genético Vegetal e Experimentação Agrícola pela Universidade Federal de Santa Maria (2020). Tem experiência profissional como Técnico Agrícola (2008-2011), consultor técnico de vendas (2012-2013), na administração pública e gestão de pessoas (2014-2015), atuando como Secretário Municipal da Agricultura e Meio Ambiente município de Cacique Doble-RS. Foi professor (bolsista) Instituto Federal de Educação, Ciência e Tecnologia Rio Grande Sul, edital nº 271, de 17 de julho de 2014, atuando na ação Bolsa-Formação Programa Nacional de Acesso ao Ensino Técnico e Emprego (PRONATEC), na Unidade Remota de Cacique Doble. Atualmente é Professor Ensino Superior Instituto de Desenvolvimento Educacional Alto Uruguai (IDEAU), sendo membro Núcleo Docente Estruturante curso de Agronomia. É membro atuante da International Biometric Society (IBS), American Society Agronomy (ASA), Crop Science Society America (CSSA) e da Soil Science Society America (SSSA). É integrante da comissão de Jovens Pesquisadores da Região Brasileira da Sociedade Internacional de Biometria, RBras, (JP-RBras) representando os estados RS, SC e PR. Atua também como revisor ad hoc em revistas científicas nacionais e internacionais, sendo membro Conselho Editorial da revista Genetics Molecular Research. Exerce atividades de pesquisa relacionadas ao planejamento, condução e avaliação de experimentos com culturas anuais, com ênfase desenvolvimento e aperfeiçoamento de métodos estatístico-experimentais para avaliação de ensaios multi-ambientes em melhoramento genético de plantas. Em seu currículo, os termos mais frequentes na contextualização da produção científica são: análise de ensaios multi-ambientes, índices multivariados, intervalo de confiança para correlação, planejamento de experimentos, seleção indireta, interação genótipo-vs-ambiente, modelos mistos e parâmetros genéticos. Tem experiência com os softwares Gênes, GEA-R, R, SAS e SPSS. Desenvolveu o pacote para sofware R metan https://tiagoolivoto.github.io/metan/, voltado para checagem, manipulação, análise e apresentação de dados de ensaios multi-ambientes.Sua mais recente pesquisa publicada em uma série de dois artigos diz respeito ao desenvolvimento de um novo índice de estabilidade para análise de ensaios multi-ambientes, bem comoo desenvolvimento de um índice multivariado, para seleção de genótipos baseado na estabilidade ou performance e estabilidade quando diversas variáveis são analizadas.Como um usuário ativo de R, Tiago desenvolveu o pacote metan, acrônimo para multi-environment trial analysis. O código fonte pode ser encontrado em sua página GitHub.ContatosE-mail: tiagoolivoto@gmail.com | Curriculo Lattes | GitHub | ORCID | Research Gate | ResearcherID","code":""},{"path":"sobre-os-autores.html","id":"bruno-giacomini-sari","chapter":"Sobre os autores","heading":"Bruno Giacomini Sari","text":"Possui graduação (2012), mestrado (2015) e doutorado (2018) em Agronomia pela Universidade Federal de Santa Maria - UFSM. Atualmente realiza estágio pós doutoral junto ao Programa de Pós Graduação em Agronomia da UFSM. Tem experiência na área de estatística, com enfase em experimentação agrícola, atuando nos seguintes temas: probabilidade, amostragem, planejamento experimental, análise de regressão linear e não linear.ContatosE-mail: brunosari@hotmail.com | Curriculo Lattes | ORCID | Research Gate","code":""},{"path":"detalhes-importantes.html","id":"detalhes-importantes","chapter":"Detalhes importantes","heading":"Detalhes importantes","text":"Ambiente de criaçãoEste e-book foi escrito em RMarkdown, usando o pacote bookdown e hospedado na web em GitHub. versão pdf deste material pode ser baixada aqui.Código fonteO código fonte deste e-book pode ser encontrado neste repositório GitHub. Para informar qualquer problema, por favor, crie um pull request.LicençaEste trabalho está licenciado com uma Licença Creative Commons - Atribuição-NãoComercial-CompartilhaIgual 4.0 Internacional. O resumo legível da licença afirma que você tem o direito de:Compartilhar — copiar e redistribuir o material em qualquer suporte ou formatoCompartilhar — copiar e redistribuir o material em qualquer suporte ou formatoAdaptar — remixar, transformar, e criar partir materialAdaptar — remixar, transformar, e criar partir materialAtribuição — Você deve dar o crédito apropriado, prover um link para licença e indicar se mudanças foram feitas. Você deve fazê-lo em qualquer circunstância razoável, mas de nenhuma maneira que sugira que o licenciante apoia você ou o seu uso.Atribuição — Você deve dar o crédito apropriado, prover um link para licença e indicar se mudanças foram feitas. Você deve fazê-lo em qualquer circunstância razoável, mas de nenhuma maneira que sugira que o licenciante apoia você ou o seu uso.De acordo com os termos seguintes\r\nNão Comercial — Você não pode usar o material para fins comerciais.\r\nCompartilhaIgual — Se você remixar, transformar, ou criar partir material, tem de distribuir suas contribuições sob mesma licença que o original.\r\nSem restrições adicionais — Você não pode aplicar termos jurídicos ou medidas de caráter tecnológico que restrinjam legalmente outros de fazerem algo que licença permita.\r\nDe acordo com os termos seguintesNão Comercial — Você não pode usar o material para fins comerciais.Não Comercial — Você não pode usar o material para fins comerciais.CompartilhaIgual — Se você remixar, transformar, ou criar partir material, tem de distribuir suas contribuições sob mesma licença que o original.CompartilhaIgual — Se você remixar, transformar, ou criar partir material, tem de distribuir suas contribuições sob mesma licença que o original.Sem restrições adicionais — Você não pode aplicar termos jurídicos ou medidas de caráter tecnológico que restrinjam legalmente outros de fazerem algo que licença permita.Sem restrições adicionais — Você não pode aplicar termos jurídicos ou medidas de caráter tecnológico que restrinjam legalmente outros de fazerem algo que licença permita.","code":""},{"path":"intro.html","id":"intro","chapter":"Capítulo 1 Introdução ao ambiente R","heading":"Capítulo 1 Introdução ao ambiente R","text":"","code":""},{"path":"intro.html","id":"o-software-r","chapter":"Capítulo 1 Introdução ao ambiente R","heading":"1.1 O software R","text":"O artigo R: Language Data Analysis Graphics13 marca o início de uma nova era processamento e análise de dados: o desenvolvimento software R. O R é uma linguagem e ambiente estatístico que traz muitas vantagens para o usuário, embora elas não sejam tão obvias inicialmente: () o R é um Software Livre (livre sentido de liberdade) distribuído sob Licença Pública Geral14, podendo ser livremente copiado, distribuído, e instalado em diversos computadores livremente. Isso contrasta com softwares comerciais, que têm licenças altamente restritivas, que não permitem que cópias sejam distribuídas ou instaladas em mais de um computador sem devida licença (que obviamente é paga!); (ii) grande maioria dos Softwares livres são grátis, e o R não é uma exceção; (iii) os códigos-fontes R estão disponíveis para os usuários, e atualmente são gerenciados por um grupo chamado R Development Core Team15. vantagem de ter o código aberto é que falhas podem ser detectadas e rapidamente corrigidas. Este sistema de revisão depende da participação dos usuários. Em contraste, em muitos pacotes comerciais, falhas não são corrigidas até o lançamento da próxima versão, o que pode levar vários anos; (iv) o R fornece um interface de entrada por linha de comando (ELC).software R, todos os comandos são digitados e o mouse é pouco usado. Pode parecer antigo, pouco amigável ou até pobre em recursos visuais, mas isso faz com que nos deparemos com o melhor recurso R: sua flexibilidade. Para usuários familiarizados, linguagem R se torna clara e simples. Com poucos comandos, funções poderosas podem ser criadas e o usuário é sempre consciente que foi pedido através da ELC (Meus dados, minhas análises!). Isso contrasta com pacotes que possuem uma interface amigável (Windows-based), mas escondem dinâmica dos cálculos e, potencialmente, os seus erros. Finalmente, o R fornece uma ampla variedade de procedimentos estatísticos básicos ou que exigem grande esforço computacional (modelagem linear e não linear, testes estatísticos clássicos, análise de séries temporais, classificação, agrupamento, etc.) e recursos gráficos elegantes. Um dos pontos fortes de R é facilidade com que gráficos de qualidade podem ser produzidos, incluindo símbolos matemáticos e fórmulas, quando necessário. O software R está disponível em uma ampla variedade de plataformas UNIX e sistemas similares (incluindo FreeBSD e Linux), Windows e MacOS.","code":""},{"path":"intro.html","id":"o-software-rstudio","chapter":"Capítulo 1 Introdução ao ambiente R","heading":"1.2 O software RStudio","text":"Quem ja é usuário de softwares por linhas de comando, como o SAS, provavelmente não notou nenhuma grande diferença até aqui. Toda análise se resume à seguinte sequência dados > códigos > saída. experiência usuário com o R, entanto, pode ser mais atrativa utilizando o RStudio16. O Rstudio é um produto de código aberto disponível publicamente em 28/02/2011 que está disponível gratuitamente. Ele é um ambiente de desenvolvimento integrado para R que inclui () janelas de edição de texto partir das quais o código pode ser enviado para o console e/ou salvo sistema operacional, (ii) listas de objetos em sua área de trabalho, (iii) histórico infinito dos comandos facilmente pesquisável com capacidade de inserir, partir histórico, um comando console novamente; (iv); interface com o sistema operacional para acesso arquivos; (v) janela de ajuda com botões de voltar e avançar; (vi) download de pacotes. Apesar de todas estas capacidades, o RStudio é muito fácil de utilizar.Nesta seção serão abordados alguns aspectos básicos para que o usuário R possa desenvolver suas análises. Será dado enfoque para áreas básicas da interface, cujo conhecimento é necessário para que um usuário inicante possa realizar sua primeira análisee. figura abaixo mostra principais janelas Rstudio, incluindo o script, o console, “área de trabalho” e o output para gráficos.Interface RStudioAntes de iniciar análises, recomenda-se escolher um diretório  onde devem estar os inputs (dados) e para onde serão enviados os outputs (gráficos, arquivos .txt, .xlsx, etc) . Para selecionar o diretório, basta seguir o caminho Session > Set Working Directory > Choosing diretory ou utilizar teclas de atalho Ctrl+Shift+H.Selecionando um diretórioPosteriormente, um R Script é iniciado conforme figura abaixo ou pelas teclas de atalho Ctrl+Shift+N. canto inferior direito, além de servir como output para gráficos (Plots), também é o local onde os pacotes utilizados nas análises (Packages) são instalados e maiores informações sobre funções podem ser encontradas (Help).","code":""},{"path":"intro.html","id":"meu-primeiro-script","chapter":"Capítulo 1 Introdução ao ambiente R","heading":"1.3 Meu primeiro script","text":"Se você realizou o download software R pela primeira vez e achou um tanto quanto “estranho” o pequeno tamanho arquivo (~80 Mb), provalemente deve ter se perguntado como um software estatistico tão poderoso pode ser comprimido em um arquivo tão pequeno (pequeno em comparação com os +20 GB SAS). resposta é simples, somente os pacote básicos R são baixados com o arquivo de instalação. Na medida em que o usuário necessita realizar uma análise específica, instalação de um pacote que contém uma função específica para realizar tal análise é necessária.instalação dos pacotes pode ser realizada conforme figura abaixo, ou através da função install.packages(). Após instalação pacote, o usuário deve utilizar funções require() ou library()  para que o pacote seja carregado e suas funções possam ser utilizadas. Quando o usuário tenta utilizar uma função pertencente um determinado pacote e este pacote não está instalado (ou carregado), um erro é exibido.Instalando pacotesAjuda para função nls()Como exemplo inicial, vamos tentar selecionar variável Sepal.Length conjunto de dados base iris utilizando função abaixo. Cuidado, spoilers pacote dplyr! O primero passo é criar um novo script, seguindo os seguintes passos: File > New File > R script ou utilizando o atalho Ctrl+Shift+N. Posteriormente, o seguinte código é digitado e executado ao se selecionar linha código e clicar botão run ou utilizando o atalho Ctrl+Enter).Neste caso, um erro é exibido pois o pacote dplyr não está instalado ou carregado. Caso ele já estiver instalado, mensagem de erro acima é superada ao carregar o pacote antes de executar função.Caso o pacote dplyr não esteja instalado, maneira mais fácil de instalá-lo é instalando coleção de pacotes tidyverse17 seguindo os passos da Figura 3 ou utilizando seguinte função.O tidyverse é uma coleção de pacotes R projetados para ciência de dados, contendo, dentre outros, seguintes pacotes que serão utilizados neste material.ggplot2, para criação de gráficos.dplyr, para manipulação de dados.tidyr, para organização de dados.tibble, para criação de tibbles.","code":"\nSL <- select(iris, Sepal.Length)\n# Error in select(iris, Sepal.Length): não foi possível encontrar a função \"select\"\nlibrary(dplyr)\nSL <- select(iris, Sepal.Length)\ninstall.packages(\"tidyverse\", dependencies = TRUE)\nlibrary(tidyverse)"},{"path":"intro.html","id":"pacotes-necessários","chapter":"Capítulo 1 Introdução ao ambiente R","heading":"1.4 Pacotes necessários","text":"Praticamente todas rotinas realizadas R são baseadas em bibliotecas de códigos. Com manipulaçao de pacotes não seria diferente. O pacote pacman18 é uma ferramenta de gerenciamento de pacotes R que combina funcionalidade das funções relacionadas à biblioteca base em funções intuitivamente nomeadas, reduzindo o código para executar simultaneamente várias ações. Uma das funções mais úteis pacote pacman é p_load() (package load). Esta função checa se um pacote está instalado e em caso afirmativo, carrega-o como se função library() tivesse sido usada. Caso o pacote não esteja instalado, ela primeiro o instala-rá antes de carregá-lo. Vamos, agora, instalar (para quem está utilizando o R pela primeira vez) ou carregar (para quem já tem instalado) os pacotes utilizados neste material.Os pacotes disponibilizados software R estão em constante atualização. Utilizando função packageStatus() e summary(packageStatus()) é possível verificar se os pacotes estão atualizados. Outra forma é ir em Packages > Update que se encontra canto inferior direito conforme demonstrado na Figura 5.Verificando se há atualizações disponíveisPor fim, para citar os pacotes , recomenda-se verificar referência através da função citation().   Para o pacote metan, por exemplo, referência oficial é encontrada artigo que descreve o pacote (Olivoto Lúcio 2020)","code":"\nif (!require(\"pacman\")){\ninstall.packages(\"pacman\")\n}\nlibrary(pacman)\np_load(hnp, asbio, car, DT, dplyr, devtools, emmeans, effects, multcomp,\n       lme4, rio, ExpDes.pt, qqplotr, manipulate, metan, MASS, olsrr,\n       tidyverse, tibble, agricolae, psych, corrplot, pvclust, factoextra, ggfortify)\ncitation(\"metan\")\n# \n# Please, support this project by citing it in your publications!\n# \n#   Olivoto, T., and Lúcio, A.D. (2020). metan: an R package for\n#   multi-environment trial analysis. Methods Ecol Evol. 11:783-789\n#   doi:10.1111/2041-210X.13384\n# \n# A BibTeX entry for LaTeX users is\n# \n#   @Article{Olivoto2020,\n#     author = {Tiago Olivoto and Alessandro Dal'Col L{'{u}}cio},\n#     title = {metan: an R package for multi-environment trial analysis},\n#     journal = {Methods in Ecology and Evolution},\n#     volume = {11},\n#     number = {6},\n#     pages = {783-789},\n#     year = {2020},\n#     doi = {10.1111/2041-210X.13384},\n#     url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13384},\n#     eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13384},\n#   }"},{"path":"objects.html","id":"objects","chapter":"Capítulo 2 Tipos de objetos","heading":"Capítulo 2 Tipos de objetos","text":"Os tipos de objeto mais utilizados na linguagem R são: () vetores, (ii) matrizes, (iii) data frames, (iv) listas e (v) funções. Um enfoque maior será dado aos vetores, matrizes e data frames, pois estes são amplamen\r\nte utilizados, inclusive nas análises mais simples.","code":""},{"path":"objects.html","id":"vetores","chapter":"Capítulo 2 Tipos de objetos","heading":"2.1 Vetores","text":"Existem quatro tipos principais de vetores: lógico, inteiro, duplo e caractere (que contém cadeias de caracteres). Vetores coletivamente inteiros e duplos são conhecidos como vetores numéricos. Cada um dos quatro tipos primários possui uma sintaxe especial para criar um valor individual, um escalar.Vetores lógicos podem ser escritos por extenso (TRUE ou FASLSE) ou abreviados (T ou F).Vetores lógicos podem ser escritos por extenso (TRUE ou FASLSE) ou abreviados (T ou F).Vetores duplos podem ser especificadas em formato decimal (0.1234), científico (1.23e4).Vetores duplos podem ser especificadas em formato decimal (0.1234), científico (1.23e4).Vetores inteiros são escritos de forma semelhante aos duplos, mas devem ser seguidos por L (1234L, 1e4L ou 0xcafeL) e não podem conter valores fracionados.Vetores inteiros são escritos de forma semelhante aos duplos, mas devem ser seguidos por L (1234L, 1e4L ou 0xcafeL) e não podem conter valores fracionados.Caracteres são cercadas por \" (\"dia\") ou ’ ('noite').Caracteres são cercadas por \" (\"dia\") ou ’ ('noite').função c()   combina valores que formam vetores19. Abaixo, é demonstrado como vetores podem ser criados utilizando c(). Note que o código é dado entre parênteses (...) para que o valor seja armazenado ambiente ao mesmo tempo em que é impresso console.Os vetores foram armazenados em x1, x2 e x3 e ficaram armazenados como valores na área de trabalho como valores (values). Para que os valores sejam mostrados basta digitar console onde os vetores foram armazenados.Vetores também podem ser criados utilizando funções rep() e seq(), conforme mostrado abaixo.  função c() também pode ser combinada com funções rep() e seq() para criar vetores mais complexos, como mostrado abaixo.Utilizando colchtes [] é possível selecionar um (ou um conjunto) de elementos de um vetor. Por exemplo:Em adição ao uso de [], funções first(), last() e nth(),  são utilizadas para selecionar o primeiro, o último e o -ésimo elemento de um vetor. principal vantagem é que você pode fornecer um vetor secundário opcional que define ordem e fornecer um valor padrão ser usado quando entrada menor que o esperado.","code":"\n(x1 <- c(1)) # Escalar \n# [1] 1\n(x2 <- c(1,2)) # Vetor\n# [1] 1 2\n(x3 <- c(1,2,3)) # Vetor\n# [1] 1 2 3\n(x3.1 <- c(\"um\",\"dois\",\"três\")) # Vetor com caracteres\n# [1] \"um\"   \"dois\" \"três\"\nrep(5, 10)\n#  [1] 5 5 5 5 5 5 5 5 5 5\nseq(1, 5)\n# [1] 1 2 3 4 5\nseq(1, 5, by = 0.5)\n# [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nseq(2, 20, by = 2)\n#  [1]  2  4  6  8 10 12 14 16 18 20\n(x4 <- c(rep(1:4, each = 4)))\n#  [1] 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4\n(x5 <- seq(1:5))\n# [1] 1 2 3 4 5\n(x6 <- c(rep(seq(1:5), each = 2)))\n#  [1] 1 1 2 2 3 3 4 4 5 5\n(x7 <- x6[1]) # Seleciona o primeiro elemento do vetor \n# [1] 1\n(x8 <- x6[4]) #  Seleciona o quarto elemento do vetor \n# [1] 2\n(x9 <- x6[c(1, 4, 8)]) # Seleciona o primeiro, o quarto e o oitavo elemento\n# [1] 1 2 4\n(x10 <- x6[1:4]) # armazena uma sequência de elementos (primeiro ao quarto)\n# [1] 1 1 2 2\n(x <- 1:10)\n#  [1]  1  2  3  4  5  6  7  8  9 10\nx <- runif(100, 0, 100)\nfirst(x)\n# [1] 30.99486\nlast(x)\n# [1] 9.459087\nnth(x, 23)\n# [1] 13.93334"},{"path":"objects.html","id":"matrizes","chapter":"Capítulo 2 Tipos de objetos","heading":"2.2 Matrizes","text":"matrizes  são um conjunto de valores (ou variáveis) dispostos em linhas e colunas, e que formam um corpo delimitado por [ ]. matrizes são geralmente representadas genericamente por \\({{\\boldsymbol{}}_{{\\boldsymbol{MxN}}}}\\), onde M e N represetam os números de linhas e colunas da matriz, respectivamente. matrizes podem ser facilmente construídas utilizando função matrix(). Alternativamente, funções cbind() e rbind() também podem ser utilizadas. primeira função adiciona colunas matrizes, enquanto que segunda adiciona linhas. Veremos mais tarde que estas funções podem ser combinadas com outras funções para construção de data frames .funções cbind() e rbind()   podem ser utilizadas conjuntamente. Não queremos confundir sua cabeça, mas se lição anterior foi entendida, próxima se torna fácil.Com função matrix()  podemos ter o mesmo resultado que o obtido com o uso das funções cbind() e rbind(). Porém, para utilizar função matrix(), alguns argumentos devem ser declarados. Na função matrix(data = NA, nrow = 1, ncol = 1, byrow = FALSE,dimnames = NULL), os argumentos que devemos inicialmente conhecer são o nrow, ncol e byrow. O primeiro indica o número de linhas da matriz, o segundo número de colunas e o terceiro indica como matriz é preenchida. Por default, byrow é FALSE, indicando que matrizes são preenchidas por colunas. Se TRUE, o preenchimento ocorre por linhas.Para selecionar elementos, linhas e colunas da matriz com [ ] utiliza-se um sistema de coordenadas:","code":"\n## Usando cbind()\nx10 <- cbind(1,2,3,4,5) # ou x10 = cbind(1:5), 5 colunas com 1 elemento cada\nx11 <- cbind(c(1,2,3,4,5)) # ou x11 = cbind(c(1:5)), 1 coluna com 5 elementos cada \nx12 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10)) # 2 colunas de 5 elementos\nx12.1 <- cbind(x11,c(6:10))\nx13 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10),c(11,12,13,14,15)) # 3 colunas de 5 elementos\nx13.1 <- cbind(x12.1,c(11,12,13,14,15))\n## Usando rbind()\nx14 <- rbind(1,2,3,4,5) # x14 = x11, 5 linhas com 1 elemento cada \nx15 <- rbind(c(1,2,3,4,5)) # x15 = x10, 1 linha com 5 elementos cada\nx16 <- rbind(c(1,2,3,4,5),c(6,7,8,9,10)) # 2 linhas com 5 elementos cada\nx16.1 <- rbind(x15,c(6,7,8,9,10))\nx17 <- rbind(c(1,6),c(2,7),c(3,8),c(4,9),c(5,10)) # x16 = x12\n## Usando rbind() e cbind()\nx18 <- cbind(c(1,2,3,4,5),c(6,7,8,9,10), rbind(11, 12, 13, 14, 15))  \nx18\n#      [,1] [,2] [,3]\n# [1,]    1    6   11\n# [2,]    2    7   12\n# [3,]    3    8   13\n# [4,]    4    9   14\n# [5,]    5   10   15\n## Usando matrix\nx19 <- matrix(1:15, nrow = 5, ncol = 3)\nx19\n#      [,1] [,2] [,3]\n# [1,]    1    6   11\n# [2,]    2    7   12\n# [3,]    3    8   13\n# [4,]    4    9   14\n# [5,]    5   10   15\nx20 <- matrix(1:15, nrow = 5, ncol = 3, byrow = TRUE)\nx20\n#      [,1] [,2] [,3]\n# [1,]    1    2    3\n# [2,]    4    5    6\n# [3,]    7    8    9\n# [4,]   10   11   12\n# [5,]   13   14   15\nx19[2, 3] # seleciona o elemento que está na linha 2 e coluna 3\n# [1] 12\nx19[, 2] # \",\" indica que todas as linhas serão selecionadas na coluna 2\n# [1]  6  7  8  9 10\nx19[1, ] # \",\" indica que todas as colunas serão selecionadas na linha 1\n# [1]  1  6 11"},{"path":"objects.html","id":"data-frame","chapter":"Capítulo 2 Tipos de objetos","heading":"2.3 Data Frame","text":"função data.frame()  cria estruturas cujas colunas podem ser valores numéricos ou caracteres. É uma estrutura muito utilizada em funções software R.Em x22 simulamos como muitos experimentos são organizados momento de tabulação dos dados (fatores nas colunas e variáveis nas linhas).","code":"\nx22 <- data.frame(\n      expand.grid(Ambiente = c(\"A1\", \"A2\"),\n                  Genotipo = c(\"G1\", \"G2\", \"G3\"),\n                  Rep = c(\"I\", \"II\", \"III\")),\n                  Y = rnorm(18, 50, 15))\nstr(x22)\n# 'data.frame': 18 obs. of  4 variables:\n#  $ Ambiente: Factor w/ 2 levels \"A1\",\"A2\": 1 2 1 2 1 2 1 2 1 2 ...\n#  $ Genotipo: Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 1 1 2 2 3 3 1 1 2 2 ...\n#  $ Rep     : Factor w/ 3 levels \"I\",\"II\",\"III\": 1 1 1 1 1 1 2 2 2 2 ...\n#  $ Y       : num  37.1 75.7 49 43.2 37.5 ..."},{"path":"objects.html","id":"tibbles","chapter":"Capítulo 2 Tipos de objetos","heading":"2.4 Tibbles","text":"Um tibble, ou tbl_df, é uma versão moderna data.frame. Tibbles são datas frames que não alteram nomes ou tipos de variáveis, possuindo um método print() aprimorado, que facilita o uso com grandes conjuntos de dados contendo objetos complexos. Você pode forçar um objeto de classe data.frame um de classe tibble utilizando as_tibble()  ou criar um partir de vetores individuais com tibble() . função tibble(), diferente de data.frame() permite que você se refira às variáveis que você acabou de criar. É possível, também, que um tibble tenha nomes de colunas que não sejam nomes de variáveis R válidos. Por exemplo, elas podem não começar com uma letra ou podem conter caracteres incomuns como um espaço. Para se referir essas variáveis, você precisa cercá-las com `. Neste documento, estrutura de dados padrão ser utilizada será tibble.","code":"\n# Convertendo um dataframe a um tibble\ntbl_x22 <- as_tibble(x22)\n# Tentando criar um dataframe\ndata.frame(x = 1:5,\n           y = 1,\n           z = x ^ 2 + y)\n# Criando um tibble\ntibble(x = 1:5,\n       y = x ^ 2,\n       `soma x + y` = x + y)\n# # A tibble: 5 x 3\n#       x     y `soma x + y`\n#   <int> <dbl>        <dbl>\n# 1     1     1            2\n# 2     2     4            6\n# 3     3     9           12\n# 4     4    16           20\n# 5     5    25           30"},{"path":"objects.html","id":"lista","chapter":"Capítulo 2 Tipos de objetos","heading":"2.5 Lista","text":"exemplo abaixo, será armazenado em uma lista  dois data-frames e uma matriz. Posteriomente, será selecionado matriz que está armazenada na lista:","code":"\nx23 <- list(x19, x22)\nx24 <- x23[[1]]"},{"path":"objects.html","id":"funções","chapter":"Capítulo 2 Tipos de objetos","heading":"2.6 Funções","text":"funções  são base da linguagem R. Através de argumentos que são indicados em funtion(), uma expressão (ou série de expressões) é resolvida e um valor (ou um conjunto de valores) é retornado. Quando uma função é armazenada ambiente de trabalho, basta digitar o nome como o qual aquela função foi gravada. Os argumentos podem ser inseridos na ordem em que aparecem na função, sem especificar qual argumento aquele valor pertence. caso em que inserção dos argumentos é diferente da ordem em que aparecem na função, é preciso identificar qual argumento aquele valor pertente. Note que é possível combinar valores numéricos e texto como argumentos e/ou resultados de funções.Exercício 1O resultado da função F2(2, 3) foi o mesmo da F2(y = 3, x =2)? Por quê?Por quê ocorreu um erro quando função F3(20) foi rodada?O que tem de errado na execução da função elevar(12, eleva = \"cubico\")?Crie uma função chamada mega que retorna os números serem apostados em jogo da Mega Sena, tendo como argumentos jogos, que define quantos jogos e numeros, que define quantos numeros serão escolhidos em cada aposta (6-15). Para cada jogo ordene os números em ordem crescente.Resposta","code":"\nF1 <- function(x){ # x é o argumento da função\n  a = 2 * x + 1\n  return(a) # retorna a\n}\n\nF2 <- function(x, y){ # dois argumentos na função\n  a = 2 * x + 1\n  b = y\n  c = a + b\n  return(c) # retorna c\n}\n\nF3 <- function(x){\n  if(x > 10){\n    stop(\"O argumento x = \", x, \" é inválido. 'x' precisa ser maior que 10\")\n  }\n  a = ifelse(x<= 5, 2 * x + 1, 3 * x + 1)\n  return(a)\n}\n\nelevar <- function(x, eleva = \"quadrado\"){\n  if(!eleva %in% c(\"quadrado\", \"cubo\")){\n    stop(\"O argumento eleva = \",eleva, \" deve ser ou 'quadrado' ou 'cubo'\")\n  }\n  if(eleva == \"quadrado\"){\n  valor <- ifelse(x^2 >= 1000,\n                 paste(\"O resultado (\",x^2,\") tem mais que 3 dígitos\"),\n                 paste(\"O resultado (\",x^2,\") tem menos que 3 dígitos\"))\n  }\n  if(eleva == \"cubo\"){\n  valor <- ifelse(x^3 >= 1000,\n                 paste(\"O resultado (\",x^3,\") tem mais que 3 dígitos\"),\n                 paste(\"O resultado (\",x^3,\") tem menos que 3 dígitos\"))\n  }\n                 \n  return(valor)\n}\nF1(2)\nF2(2, 3)\nF2(y = 3, x =2)\nF3(1)\nF3(6)\nF3(20)\nelevar(12)\nelevar(12, eleva = \"cubico\")"},{"path":"objects.html","id":"identificando-as-classes-de-objetos","chapter":"Capítulo 2 Tipos de objetos","heading":"2.7 Identificando as classes de objetos","text":"Conforme visto anteriormente, é possível construir várias classes de objetos em linguagem R. Veremos mais adiante que muitas funções exigem classes específicas como argumento, e por isso conhecê-los é muito importante. Funções genéricas como class()  ou .objeto são importantes para identificar qual tipo de classe tal objeto pertence.Algumas funções permitem forçar objetos uma classe específica, como por exemplo, transformar um objeto de classe data.frame em um objeto de classe matrix. ","code":"\nclass(x19)\n# [1] \"matrix\" \"array\"\nclass(x22)\n# [1] \"data.frame\"\nclass(x24)\n# [1] \"matrix\" \"array\"\nis.matrix(x19)\n# [1] TRUE\nx22 <- as.matrix(x22)\nx19 <- as.data.frame(x19)"},{"path":"math.html","id":"math","chapter":"Capítulo 3 Operações matemáticas","heading":"Capítulo 3 Operações matemáticas","text":"operações matemáticas utilizam símbolos que são padrão em outros softwares estatísticos.Para multiplicação de matrizes utiliza-se %*% ao envéz de *. Note diferença exemplo abaixo.O resultado da multiplicação da matriz x e y é dado por:\\[\r\n    \\left( {\\begin{array}{*{20}{c}}{1 \\cdot 5 + 3 \\cdot 6}&{1 \\cdot 7 + 3 \\cdot 8}\\\\{2 \\cdot 5 + 4 \\cdot 6}&{2 \\cdot 7 + 4 \\cdot 8}\\end{array}} \\right) = \\left( {\\begin{array}{*{20}{c}}{23}&{31}\\\\{34}&{46}\\end{array}} \\right)\r\n\\]função t()  é utilizada para transposição de matrizes e solve()  para inversão de matrizes. Vamos resolver o seguinte sistema de equações retirado livro de (Rencher Schaalje 2008) utilizando estes operadores.\\[\r\n    \\begin{array}{l}{x_1} + 2{x_2} = 4\\\\{x_1} - {x_2} = 1\\\\{x_1} + {x_2} = 3\\end{array}\r\n\\]Matricialmente o sistema acima é dado por:\\[\\left[ {\\begin{array}{*{20}{c}}{\\begin{array}{*{20}{l}}1\\\\1\\\\1\\end{array}}&{\\begin{array}{*{20}{c}}2\\\\{ - 1}\\\\1\\end{array}}\\end{array}} \\right]\\left[ {\\begin{array}{*{20}{c}}{{x_1}}\\\\{{x_2}}\\end{array}} \\right] = \\left[ {\\begin{array}{*{20}{c}}4\\\\1\\\\3\\end{array}} \\right]\\]Esse sistema de equações é representado por \\({\\bf{AX}} = {\\bf{c}}\\) e tem como solução \\({\\bf{X = }}{{\\bf{}}^{{\\bf{ - 1}}}}{\\bf{c}}\\)Considere um equação linear múltipla cuja variável dependente é Y e variáveis independentes são X1 e X2 (dados obtidos em (Kutner et al. 2005)). O sistema de equações é representado matricialmente por \\[\r\n\\boldsymbol{X'X{\\beta = X'Y}}\r\n\\]que tem como solução:\\[\r\n\\boldsymbol{\\hat\\beta = X'X^{-1}X'Y}\r\n\\]O modelo ajustado é dado por \\(\\hat Y = - 234.698704 + 1.074137{X_1} + 20.547108{X_2} + \\varepsilon\\). Combinando algumas funções vistas até agora, vamos criar um vetor de dados chamado PRED com os valores estimados pelo modelo acima. Em adição, um vetor de resíduos (RESID) será criado.funções det()  para calcular o determinante de uma matriz. Já função eigen()  retorna uma lista com os autovalores e autovetores da matriz. função names()  indica o que contém objeto av, e usando $ é possivel selecionar os autovalores ou os autovetores.função diag()  extrai diagonal de uma matriz ou cria uma matriz onde diagonal são os elementos declarados. Os próximos comandos extraem diagonal de XX e criam uma matriz identidade, com 5 linhas e 5 colunas.","code":"\n1 + 1 # Soma\n2 - 1 # Subtração\n2 * 2 # Multiplicação\n1 + 2 * 2 ^ 2 # Primeiro eleva ao quadrado, depois multiplica e depois soma\n((1 + 2) * 2) ^ 2 # Primeiro soma e depois multiplica depois eleva ao quadrado\nsqrt(4) # Raiz quadrada\n4^2 # Potência\nlog(10) # Por default, o logarítimo é de base e (logarítimo natural)\nlog(100, 10) # Logarítimo de base 10\nexp(100) # exponencial\n(x <- matrix(1:4, ncol = 2))\n#      [,1] [,2]\n# [1,]    1    3\n# [2,]    2    4\n(y <- matrix(5:8, ncol = 2))\n#      [,1] [,2]\n# [1,]    5    7\n# [2,]    6    8\nx * y # Errado\n#      [,1] [,2]\n# [1,]    5   21\n# [2,]   12   32\nx %*% y # Certo\n#      [,1] [,2]\n# [1,]   23   31\n# [2,]   34   46\n\n(A <- matrix(c(1, 1, 1, 2, -1, 1), nrow = 3, ncol = 2))\n#      [,1] [,2]\n# [1,]    1    2\n# [2,]    1   -1\n# [3,]    1    1\nA1 <- MASS::ginv(A) # Obtém a inversa generalizada de A\nc <- c(4, 1, 3) # Vetor C\nX <- A1 %*% c\nX\n#      [,1]\n# [1,]    2\n# [2,]    1\nX0 <- rep(1, each = 10)\nX1 <- c(68.5, 45.2, 91.3, 47.8, 46.9, 66.1, 49.5, 52, 48.9, 38.4)\nX2 <- c(16.7, 16.8, 18.2, 16.3, 17.3, 18.2, 15.9, 17.2, 16.6, 16)\nY <- c(174.4,  164.4, 244.2, 154.6, 181.6, 207.5, 152.8, 163.2, 145.4, 137.2)\nX <- matrix(c(X0, X1, X2), nrow = 10, ncol = 3)\nX\n#       [,1] [,2] [,3]\n#  [1,]    1 68.5 16.7\n#  [2,]    1 45.2 16.8\n#  [3,]    1 91.3 18.2\n#  [4,]    1 47.8 16.3\n#  [5,]    1 46.9 17.3\n#  [6,]    1 66.1 18.2\n#  [7,]    1 49.5 15.9\n#  [8,]    1 52.0 17.2\n#  [9,]    1 48.9 16.6\n# [10,]    1 38.4 16.0\nB <- (solve(t(X) %*% X)) %*%  t(X) %*% Y\nB\n#             [,1]\n# [1,] -234.698704\n# [2,]    1.074137\n# [3,]   20.547108\nPRED <- B[1] + B[2] * X1 + B[3] * X2\nRESID <- Y - PRED\nmat <- matrix(c(0, 2, 4, 3, 5, 0, 2, 4, 4), nrow = 3)\ndetXX <- det(mat)\nav <- eigen(mat)\nnames(av)\n# [1] \"values\"  \"vectors\"\nav$values # Extrai os autovalores\n# [1]  8  2 -1\nav$vectors # Extrai os autovetores\n#           [,1]       [,2]       [,3]\n# [1,] 0.4082483 -0.3333333  0.7715167\n# [2,] 0.8164966 -0.6666667  0.1543033\n# [3,] 0.4082483  0.6666667 -0.6172134\ndiag(mat)\n# [1] 0 5 4\ndiag(x = 1, nrow = 4, ncol = 4)\n#      [,1] [,2] [,3] [,4]\n# [1,]    1    0    0    0\n# [2,]    0    1    0    0\n# [3,]    0    0    1    0\n# [4,]    0    0    0    1"},{"path":"loops.html","id":"loops","chapter":"Capítulo 4 Loops","heading":"Capítulo 4 Loops","text":"Reescrever um código muitas vezes por necessidade de repetir um determinado procedimento seria bastante trabalhoso, além de precisarmos de mais tempo para isso. Por isso, o R tem algumas funções que fazem essas repetições para nós. Isso é muito comum e pode ser facilmente implementado pela função (), () e repeat().\r\nfunção () repete o código indicado dentro de {} n vezes, sendo n o comprimento da sequência dentro dos parênteses.função () (que significa enquanto) repete o código dentro de {} enquanto alguma condição verdadeira.Note que os dois últimos exemplos apresentam o mesmo resultado: o R vai retornar uma sequência sendo = 1:5, onde cada número será o resultado da multiplicação \\(\\times 2\\). caso (), precisamos mudar o valor de para que sequência continue até que condição (<= 5) verdadeira. Em adição, precisamos declarar variável (= 1) antes para que o R possa testar condição expressa dentro dos parênteses. caso (), sequência progride sem precisarmos fazer isso manualmente.último exemplo, utilizando repeat(), o R repetirá o código dentro de {} sem condições. Com isso, precisamos utilizar combinação das funções () e break() para informar ao programa quando o código deve parar de ser repetido.Os Loops são importantes em estudos que utilizam reamostragens para realizar análises estatísticas. Através de reamostragnes é possível construir intervalos de confiança e testar hipóteses não paramétricas. Como exemplo, vamos demonstrar o teorema central limite20. Para isto, criamos uma função (teor_lim())  que tem 4 argumentos: n o tamanho da amostra ser considerada, namostra, min, max são os parâmetros da distribuição uniforme (veja ?runif). Para confecção dos dendrogramas, os pacotes ggplot2 e cowplot serão utilizados. Veja seção 1.6 para maiores informações sobre confecção de gráficos com estes pacotes. \r\nFigure 4.1: Demonstração teorema central limite\r\n","code":"\nj <- 5\nfor (i in 1:j){\n  k <- i * 2\n  print(k)\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\ni <- 1\nwhile (i <= 5){\n  print(i * 2)\n  i <- i + 1\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\ni <- 1\nrepeat {\n  print(i * 2)\n  i <- i + 1\n  if(i > 5){\n    break()\n  }\n}\n# [1] 2\n# [1] 4\n# [1] 6\n# [1] 8\n# [1] 10\nlibrary(ggplot2)\nlibrary(cowplot)\nteor_lim <- function(n, namostra, min, max){\n  set.seed(100)\n  media <- data.frame(matrix(ncol = 1, nrow = n))\n  names(media) = \"value\"\n  for(j in 1:n){\n    media[j, 1] = mean(runif(namostra, min, max))\n  }\n  return(media)\n}\n\nteorema_limite <- list(\n  n20 <- teor_lim(n = 20, namostra = 100, min = 0, max = 10),\n  n200 <- teor_lim(n = 200, namostra = 100, min = 0, max = 10),\n  n2000 <- teor_lim(n = 2000, namostra = 100, min = 0, max = 10),\n  n20000 <- teor_lim(n = 20000, namostra = 100, min = 0, max = 10)\n)\n\np <- lapply(teorema_limite, function(d){\n  ggplot(data = d, aes(x = value))+\n    geom_histogram(bins = 50,\n                   col = \"black\",\n                   size = 0.3,\n                   aes(fill = ..count..))+\n    theme(legend.position = \"none\")+\n    labs(x = \"Média\", y = \"Contagem\")\n  \n})\nplot_grid(plotlist = p,\n          labels = names(teorema_limite),\n          vjust = 2.5,\n          hjust = c(-1.7, -1.5, -1, -1))"},{"path":"dataframe.html","id":"dataframe","chapter":"Capítulo 5 Construindo uma tabela","heading":"Capítulo 5 Construindo uma tabela","text":"Com o que foi visto até agora, é possível construir uma tabela para armazenar dados ou resultados gerados em uma análise. São inumeras formas de fazer isso, porém vamos mostrar apenas uma (devido ao pouco tempo). Utilizaremos funções matrix()  e data.frame()  para tabela, e função names() para dar nome colunas. construção de tabelas é muito util para armazenar resultados na área de trabalho.Vimos anteriormente, entanto, que utilização de tibbles  é recomendada. O mesmo conjunto de dados pode ser criado mais “elegantemente” com função abaixo","code":"\nEx.gen <- as.data.frame(matrix(0, ncol = 3, nrow = 20))\nnames(Ex.gen) = c(\"G1\",\"G2\",\"G3\")\nYeld1 <- rnorm(20,10,1) \n# gera 20 valores de uma distribuição normal com média 10 e desvio padrão 1\nYeld2 <- rnorm(20,40,3)\n# gera 20 valores de uma distribuição normal com média 40 e desvio padrão 3\nYeld3 <- rnorm(20,25,2.5)\n# gera 20 valores de uma distribuição normal com média 25 e desvio padrão 2,5\nEx.gen$G1 <- Yeld1\nEx.gen$G2 <- Yeld2\nEx.gen$G3 <- Yeld3\nEx.gen2 <- tibble(G1 = rnorm(20,10,1),\n                  G2 = rnorm(20,40,3),\n                  G3 = rnorm(20,25,2.5))"},{"path":"entrada.html","id":"entrada","chapter":"Capítulo 6 Entrada de dados","heading":"Capítulo 6 Entrada de dados","text":"entrada de dados pode ser feita de várias maneiras, e em vários formatos. Porém daremos destaque formas mais comuns. forma mais simples (e mais trabalhosa) é digitar os dados diretamente console, utilizando para isso função scan().  Para importar dados digitados em extensão .txt utiliza-se função read.table().  Por fim, para carregar arquivos em extensão .xlsx maneira mais simples é utilizar o Import Dataset que encontra-se na área de trabalho.função scan() é muito trabalhosa e pouco utilizada. Caso o usuário queira carregar dados salvos em extensão .txt (bloco de notas), usando função read.table(), deve-se ter o cuidado de mover o arquivo para o diretório previamente indicado. Como colunas são identificadas por espaços, é importante que o usuário não utilize nomes compostos cabeçalho ou corpo da tabela. Quando isso ocorre, o R identifica o erro console através da mensagem Error read.table(\"Dados_1.txt\", header = TRUE) : columns column names.forma mais comum pesquisador digitar seus dados é através de planilhas eletrônicas Excel. Para carregar esses dados, basta ir em Import Dataset na área de trabalho. O passo passo está descrito abaixo: Importando dados de planilhas eletrônicas Excel - Passo 1Importando dados de planilhas eletrônicas Excel - Passo 2Importando dados de planilhas eletrônicas Excel - Passo 3Também é possível carregar dados já extistentes dentro software R. Geralmente, os pacotes contém dados que são utilizados como exemplo de aplicação das suas funções.","code":"\ndata <- scan() #Enter\n1: 10\n2: 20\n3: 30\n4: 40\n5: # Duplo enter\ndata\ndados <- read.table(\"data/Dados_1.txt\", header = TRUE)\n# Argumento header = TRUE indica a existência de cabeçalho\ndados <- read.table(\"data/Dados_2.txt\", header = TRUE)\n# Argumento header = TRUE indica a existência de cabeçalho\ndados\n#           Trat Rep      Sta     num    peso\n# 1 bcco_alb_imp   1 1132.789 0.00000 0.00000\n# 2 bcco_alb_imp   2 1199.039 0.06250 0.61875\n# 3 bcco_alb_imp   3 1265.189 0.06250 0.61875\n# 4 bcco_alb_imp   4 1337.789 0.16525 1.48125\nhead(iris) # head() limita os valores que serão impressos no console\n#   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n# 1          5.1         3.5          1.4         0.2  setosa\n# 2          4.9         3.0          1.4         0.2  setosa\n# 3          4.7         3.2          1.3         0.2  setosa\n# 4          4.6         3.1          1.5         0.2  setosa\n# 5          5.0         3.6          1.4         0.2  setosa\n# 6          5.4         3.9          1.7         0.4  setosa"},{"path":"manipula.html","id":"manipula","chapter":"Capítulo 7 Manipulação de dados","heading":"Capítulo 7 Manipulação de dados","text":"Após seus dados estarem carregados ambiente R, eles provavelmente necessitarão de alguma manimulação antes de serem utilizados em uma determinada análise. Esta manipulação pode envolver operações como exclusão de colunas, ordenamento de linhas com base em valores, criação de covariáveis (que serão resultado de operações com uma ou mais variáveis existentes), dentre muitas outras possibilidades. Felizmente, o pacote dplyr  permite que esta manimulação seja relativamente fácil, lógica (ponto de vista de digitação de códigos) e rápida, pois ele integra linguagem C++ em suas funções.O pacote dplyr é uma gramática de manipulação de dados. Nos rerferimos à gramática aqui porque ele fornece funções nomeadas como verbos simples, relacionados às tarefas mais comuns de manipulação de dados, para ajudá-lo traduzir seus pensamentos em código. Este será o pacote utilizado para manipulação dos dados decorrer deste material. De fato, maioria dos dados em R podem ser manipulados utilizando os seguintes “verbos”.filter() para selecionar linhas com base em seus valores.arrange() para reordenar linhas.select() e rename() para selecionar variáveis com base em seus nomes.mutate() e transmute() para adicionar novas variáveis que são funções de variáveis existentes.summarise() para resumir vários valores para um único valor.sample_n() e sample_frac() para obter amostras aleatórias.Anteriomente mencionamos que manipulação dos dados com o pacote dplyr é lógica ponto de vista da implementação código. Isto só é possivel devido utilização operador %>% (forward-pipe operator), importado pacote magrittr. Basicamente, este operador capta o argumento resultante de uma função à sua esquerda e passa como input à função à sua direita. Não é nossso objetivo aqui discutir os benefícios da utilização deste operador, mas uma pequena demonstração (com spoilers das funções pacote dplyr) será apresentada. Considere seguintes (e simples) operações. Crie um data frame com 100 linhas com variáveis x e y contendo valores aleatórios. Adicione uma terceira variáveis (z) que será uma função da multiplicação de x e y, selecione apenas os valores de z menores que 10 e extraia raiz quadrada destes valores. Finalmente, compute média e armazene object mean_sqrt.Criando o conjunto de dadosUtilizando funções bases R (código massivo)Utilizando funções bases R (código mais limpo)Utilizando o operdor %>%utilização operador %>% parece não trazer tantos benefícios neste exemplo, visto que objetivo aqui foi demonstrar como ele permite uma implementação lógica das operações realizadas, captando saída da função diretamente à esquerda (acima neste caso) e passando para próxima função. Em operações mais complexas, entanto, o %>% se mostrará muito mais útil.O pacote metan fornece funções úteis para manipulação de dados. Duas principais categorias de funções serão utilizadas neste material:Utilitários para lidar com linhas e colunasadd_cols(): adiciona uma ou mais colunas um conjunto de dados existente. Se colunas .ou .especificadas não existirem, colunas serão anexadas final dos dados. Retorna um conjunto de dados com todas colunas originais em .data mais colunas declaradas em .... Em add_cols(), colunas em .data estão disponíveis para expressões. Portanto, é possível adicionar uma coluna com base nos dados existentes.\r\nadd_rows(): adiciona uma ou mais linhas um conjunto de dados existente. Se não houver linhas especificadas .ou ., linhas serão anexadas final dos dados. Retorna um conjunto de dados com todas linhas originais em .data mais linhas declaradas em ....add_prefix() e add_suffix(): adicionam prefixos e sufixos, respectivamente, nos nomes das variáveis selecionadas.\r\nall_pairs(): obtém todos os pares possíveis entre os níveis de um fator.\r\ncolnames_to_lower(): converte todos os nomes de coluna para minúsculas.\r\ncolnames_to_upper(): converte todos os nomes de coluna para maiúsculas.\r\ncolnames_to_title(): converte todos os nomes de coluna em maiúsculas.\r\ncolumn_exists(): verifica se existe uma coluna em um conjunto de dados. Retorne um valor lógico.\r\ncolumns_to_first(): move colunas para primeiras posições em .data.\r\ncolumns_to_last(): move colunas para últimas posições em .data.\r\nconcatenate(): concatena colunas de um conjunto de dados.\r\nget_levels(): obtém os níveis de um fator.\r\nget_level_size(): obtém o tamanho de cada nível de um fator.\r\nremove_cols(): remove uma ou mais colunas de um conjunto de dados.\r\nremove_rows(): remove uma ou mais linhas de um conjunto de dados.\r\nreorder_cols(): reordena colunas em um conjunto de dados.\r\nselect_cols(): seleciona uma ou mais colunas de um conjunto de dados.\r\nselect_first_col(): seleciona primeira variável, possivelmente com um deslocamento.\r\nselect_last_col(): seleciona última variável, possivelmente com um deslocamento.\r\nselect_numeric_cols(): selecione todas colunas numéricas de um conjunto de dados.\r\nselect_non_numeric_cols(): seleciona todas colunas não numéricas de um conjunto de dados.\r\nselect_rows(): seleciona uma ou mais linhas de um conjunto de dados.\r\nUtilitários para lidar com números e stringsall_lower_case(): converte todas sequências não numéricas de um conjunto de dados para minúsculas (“Env” para “env”). all_upper_case(): converte todas sequências não numéricas de um conjunto de dados para maiúsculas (por exemplo, “Env” para “ENV”).all_title_case(): converta todas sequências não numéricas de um conjunto de dados em maiúsculas e minúsculas (por exemplo, “ENV” para “Env”).extract_number(): extrai o(s) número(s) de uma sequência de caracteres.extract_string(): Extrai todas letras de uma sequência de caracteres, ignorando maiúsculas e minúsculas.find_text_in_num(): encontra caracteres de texto em uma sequência numérica e retorna o índice de linha.has_text_in_num(): inspeciona colunas procurando por texto na sequência numérica e retorna um aviso se algum texto encontrado.remove_strings(): remove todas strings de uma variável.replace_number(): substitui os números por uma substituição.replace_string(): substitui todas strings por uma substituição, ignorando caixa.round_cols(): Arredonda uma coluna selecionada ou um conjunto de dados inteiro para números significativos.tidy_strings(): arruma seqüências de caracteres, colunas não numéricas ou quaisquer colunas selecionadas em um conjunto de dados colocando todas palavras em maiúsculas, substituindo qualquer espaço, tabulação e caracteres de pontuação por '_' e colocando '_' entre letras maiúsculas e minúsculas. Suponha que str = c(\"Env1\", \"env 1\", \"env.1\") (que por definição deve representar um nível único nos ensaios de melhoramento de plantas, por exemplo, ambiente 1) seja submetido tidy_strings(str): o resultado será então c(\"ENV_1\", \"ENV_1\", \"ENV_1\").O conjunto de dados maize será utilizado como exemplo para operações de manipulação de dados. Este arquivo em formato .xlsx está hospedado em https://github.com/TiagoOlivoto/e-bookr/tree/master/data e pode ser carregado ambiente R com função import() pacote rio.","code":"\nlibrary(tidyverse)\nlibrary(metan)\nset.seed(1)\ndata <- tibble(x = runif(100, 0, 10),\n               y = runif(100, 0, 10))\ndata$z <- data$x * data$y\ndf <- subset(data, z < 10)\ndf <- df[, 3]\nsqr_dat <- sqrt(df$z)\nmean_sqrt <- mean(sqr_dat)\nmean_sqrt\n# [1] 1.977507\ndata$z <- data$x * data$y\nmean_sqrt <- mean(sqrt(subset(data, z < 10)$z))\nmean_sqrt\n# [1] 1.977507\nmean_sqrt <- \n  data %>% \n  mutate(z = x * y) %>%\n  filter(z < 10) %>%\n  pull(z) %>%\n  sqrt() %>%\n  mean()\nmean_sqrt\n# [1] 1.977507\nlibrary(rio)\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nmaize <- import(url,\n                sheet = \"maize\", \n                setclass = \"tibble\")\ninspect(maize)\n# # A tibble: 10 x 9\n#    Variable Class     Missing Levels Valid_n   Min Median    Max Outlier\n#    <chr>    <chr>     <chr>   <chr>    <int> <dbl>  <dbl>  <dbl>   <dbl>\n#  1 AMB      character No      0          780  NA    NA     NA         NA\n#  2 HIB      character No      0          780  NA    NA     NA         NA\n#  3 REP      character No      0          780  NA    NA     NA         NA\n#  4 APLA     numeric   No      -          780   1     2.52   3.3        4\n#  5 AIES     numeric   No      -          780   0.5   1.38   2.39       1\n#  6 CESP     numeric   No      -          780   0.8  15.4   20.4       16\n#  7 DIES     numeric   No      -          780  36.4  50.0   59.7        1\n#  8 MGRA     numeric   No      -          780  58.5 174.   291.         0\n#  9 MMG      numeric   No      -          780 123.  344.   546.         6\n# 10 NGRA     numeric   No      -          780 147   517    903          9"},{"path":"manipula.html","id":"trabalhando-com-linhas-e-colunas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1 Trabalhando com linhas e colunas","text":"","code":""},{"path":"manipula.html","id":"selecionar-colunas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.1 Selecionar colunas","text":"função select_cols() pode ser usada para selecionar colunas de um conjunto de dados.colunas numéricas podem ser selecionadas rapidamente usando função select_numeric_cols(). colunas não numéricas são selecionadas com select_non_numeric_cols().Podemos selecionar primeira ou última coluna rapidamente com select_first_col() e select_last_col(), respectivamente.Select helpers podem ser usados para selecionar variáveis que correspondem uma expressão. Isso significa que podemos usar uma função para selecionar variáveis em vez de digitar seus próprios nomes. O metan reexporta os tidy select helpers e implementa os próprios select helpers com base em operações com prefixos e sufixos (different_var(), intersect_var() e union_var()), tamanho dos nomes das variáveis (width_of(), width_gength_than() e width_less_than()) e tipo de letra (lower_case_only(), upper_case_only() e title_case_only()).Selecionando variáveis que começam com um prefixo.Se quisermos selecionar variáveis que começam com “C”, podemos usar:mas se quisermos selecionar aqueles que não começam com “C”, basta adicionar “-” logo antes de starts_with()Selecionando variáveis que terminam com um sufixo.Da mesma forma, se quisermos selecionar variáveis que terminam com “S”, podemos usar:Selecionando variáveis que começam com um prefixo E terminam com um sufixo “S”Agora, se quisermos selecionar variáveis que começam com “” e terminam com “S”, ou seja, interseção entre letra inicial “” e letra final “S”, podemos:Selecionando variáveis que começam com um prefixo OU terminam com um sufixo.Também podemos obter união entre letra inicial “” e letra final “S”, ou seja, variáveis que começam com “” ou terminam com “S”.Selecionando variáveis que começam com um prefixo E NÃO terminam com um sufixo.Também podemos obter diferença entre letra inicial “” e letra final “S”, ou seja, variáveis que começam com “C” e não terminam com “D”.Selecionando variáveis que contêm uma string literal.Se variáveis conjunto de dados tiverem um padrão com diferenças entre um grupo de variáveis, podemos usar o código seguir para selecionar variáveis com um padrão. Primeiro, iremos alterar os nomes das variáveis PH, EH, EP e EL incluindo _PLANT para indicar que são variáveis relacionadas à planta. Em seguida, selecionaremos essas variáveis com função contains().Selecionando variáveis que correspondem uma expressão regular.Seleções mais sofisticadas podem ser feitas usando matches(). Supondo que gostaríamos de selecionar variáveis que começam com “” e tem segunda letra entre “” e “M”, usaríamos algo como:Selecionando última ou primeira variável, possivelmente com um deslocamento.Podemos selecionar n-ésima primeira ou última coluna com select_last_col() ou select_first_col().Selecione variáveis com um comprimento de nome específico (quatro letras)Selecione variáveis com largura menor que n.Selecione variáveis com largura maior que * n *.Selecione variáveis por tipo de letraVamos criar um conjunto de dados com nomes de colunas bagunçados.","code":"\nselect_cols(maize, AMB, HIB)\n# # A tibble: 780 x 2\n#    AMB   HIB  \n#    <chr> <chr>\n#  1 A1    H1   \n#  2 A1    H1   \n#  3 A1    H1   \n#  4 A1    H1   \n#  5 A1    H1   \n#  6 A1    H1   \n#  7 A1    H1   \n#  8 A1    H1   \n#  9 A1    H1   \n# 10 A1    H1   \n# # ... with 770 more rows\nselect_numeric_cols(maize)\n# # A tibble: 780 x 7\n#     APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1  2.45  2.39  16.9  52.1 228.   375.   609\n#  2  2.5   1.43  14.4  50.7 187.   437.   427\n#  3  2.69  1.52  16.5  54.7 230.   464.   497\n#  4  2.8   1.64  16.8  52.0 213.   408.   523\n#  5  2.62  1.55  15.9  51.6 224.   406.   551\n#  6  2.12  1.8   15    51.4 203.   383.   529\n#  7  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8  2.97  1.84  15    53.4 204.   387.   528\n#  9  3.1   1.78  13.6  50.8 187.   348.   538\n# 10  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nselect_non_numeric_cols(maize)\n# # A tibble: 780 x 3\n#    AMB   HIB   REP  \n#    <chr> <chr> <chr>\n#  1 A1    H1    I    \n#  2 A1    H1    I    \n#  3 A1    H1    I    \n#  4 A1    H1    I    \n#  5 A1    H1    I    \n#  6 A1    H1    II   \n#  7 A1    H1    II   \n#  8 A1    H1    II   \n#  9 A1    H1    II   \n# 10 A1    H1    II   \n# # ... with 770 more rows\nselect_first_col(maize)\n# # A tibble: 780 x 1\n#    AMB  \n#    <chr>\n#  1 A1   \n#  2 A1   \n#  3 A1   \n#  4 A1   \n#  5 A1   \n#  6 A1   \n#  7 A1   \n#  8 A1   \n#  9 A1   \n# 10 A1   \n# # ... with 770 more rows\nselect_last_col(maize)\n# # A tibble: 780 x 1\n#     NGRA\n#    <dbl>\n#  1   609\n#  2   427\n#  3   497\n#  4   523\n#  5   551\n#  6   529\n#  7   294\n#  8   528\n#  9   538\n# 10   582\n# # ... with 770 more rows\nselect_cols(maize, starts_with(\"C\"))\n# # A tibble: 780 x 1\n#     CESP\n#    <dbl>\n#  1  16.9\n#  2  14.4\n#  3  16.5\n#  4  16.8\n#  5  15.9\n#  6  15  \n#  7  10.9\n#  8  15  \n#  9  13.6\n# 10  16.3\n# # ... with 770 more rows\nselect_cols(maize, -starts_with(\"C\"))\n# # A tibble: 780 x 9\n#    AMB   HIB   REP    APLA  AIES  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  52.0 213.   408.   523\n#  5 A1    H1    I      2.62  1.55  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   53.9 250.   430.   582\n# # ... with 770 more rows\nselect_cols(maize, ends_with(\"S\"))\n# # A tibble: 780 x 2\n#     AIES  DIES\n#    <dbl> <dbl>\n#  1  2.39  52.1\n#  2  1.43  50.7\n#  3  1.52  54.7\n#  4  1.64  52.0\n#  5  1.55  51.6\n#  6  1.8   51.4\n#  7  1.78  41.9\n#  8  1.84  53.4\n#  9  1.78  50.8\n# 10  1.6   53.9\n# # ... with 770 more rows\nselect_cols(maize, intersect_var(\"A\", \"S\"))\n# # A tibble: 780 x 1\n#     AIES\n#    <dbl>\n#  1  2.39\n#  2  1.43\n#  3  1.52\n#  4  1.64\n#  5  1.55\n#  6  1.8 \n#  7  1.78\n#  8  1.84\n#  9  1.78\n# 10  1.6 \n# # ... with 770 more rows\nselect_cols(maize, union_var(\"A\", \"S\"))\n# # A tibble: 780 x 4\n#    AMB    APLA  AIES  DIES\n#    <chr> <dbl> <dbl> <dbl>\n#  1 A1     2.45  2.39  52.1\n#  2 A1     2.5   1.43  50.7\n#  3 A1     2.69  1.52  54.7\n#  4 A1     2.8   1.64  52.0\n#  5 A1     2.62  1.55  51.6\n#  6 A1     2.12  1.8   51.4\n#  7 A1     3.15  1.78  41.9\n#  8 A1     2.97  1.84  53.4\n#  9 A1     3.1   1.78  50.8\n# 10 A1     3.02  1.6   53.9\n# # ... with 770 more rows\nselect_cols(maize, difference_var(\"A\", \"S\"))\n# # A tibble: 780 x 2\n#    AMB    APLA\n#    <chr> <dbl>\n#  1 A1     2.45\n#  2 A1     2.5 \n#  3 A1     2.69\n#  4 A1     2.8 \n#  5 A1     2.62\n#  6 A1     2.12\n#  7 A1     3.15\n#  8 A1     2.97\n#  9 A1     3.1 \n# 10 A1     3.02\n# # ... with 770 more rows\ndata_vars <- \n  maize %>%\n  rename(APLA_PLANT = APLA,\n         AIES_PLANT = AIES)\nnames(data_vars)\n#  [1] \"AMB\"        \"HIB\"        \"REP\"        \"APLA_PLANT\" \"AIES_PLANT\"\n#  [6] \"CESP\"       \"DIES\"       \"MGRA\"       \"MMG\"        \"NGRA\"\n\nselect_cols(data_vars, contains(\"PLANT\"))\n# # A tibble: 780 x 2\n#    APLA_PLANT AIES_PLANT\n#         <dbl>      <dbl>\n#  1       2.45       2.39\n#  2       2.5        1.43\n#  3       2.69       1.52\n#  4       2.8        1.64\n#  5       2.62       1.55\n#  6       2.12       1.8 \n#  7       3.15       1.78\n#  8       2.97       1.84\n#  9       3.1        1.78\n# 10       3.02       1.6 \n# # ... with 770 more rows\nselect_cols(maize, matches(\"^A[A-M]\"))\n# # A tibble: 780 x 2\n#    AMB    AIES\n#    <chr> <dbl>\n#  1 A1     2.39\n#  2 A1     1.43\n#  3 A1     1.52\n#  4 A1     1.64\n#  5 A1     1.55\n#  6 A1     1.8 \n#  7 A1     1.78\n#  8 A1     1.84\n#  9 A1     1.78\n# 10 A1     1.6 \n# # ... with 770 more rows\nselect_first_col(data_vars)\n# # A tibble: 780 x 1\n#    AMB  \n#    <chr>\n#  1 A1   \n#  2 A1   \n#  3 A1   \n#  4 A1   \n#  5 A1   \n#  6 A1   \n#  7 A1   \n#  8 A1   \n#  9 A1   \n# 10 A1   \n# # ... with 770 more rows\nselect_last_col(data_vars)\n# # A tibble: 780 x 1\n#     NGRA\n#    <dbl>\n#  1   609\n#  2   427\n#  3   497\n#  4   523\n#  5   551\n#  6   529\n#  7   294\n#  8   528\n#  9   538\n# 10   582\n# # ... with 770 more rows\nselect_cols(data_vars, width_of(4))\n# # A tibble: 780 x 4\n#     CESP  DIES  MGRA  NGRA\n#    <dbl> <dbl> <dbl> <dbl>\n#  1  16.9  52.1 228.    609\n#  2  14.4  50.7 187.    427\n#  3  16.5  54.7 230.    497\n#  4  16.8  52.0 213.    523\n#  5  15.9  51.6 224.    551\n#  6  15    51.4 203.    529\n#  7  10.9  41.9  75.2   294\n#  8  15    53.4 204.    528\n#  9  13.6  50.8 187.    538\n# 10  16.3  53.9 250.    582\n# # ... with 770 more rows\nselect_cols(data_vars, width_less_than(4))\n# # A tibble: 780 x 4\n#    AMB   HIB   REP     MMG\n#    <chr> <chr> <chr> <dbl>\n#  1 A1    H1    I      375.\n#  2 A1    H1    I      437.\n#  3 A1    H1    I      464.\n#  4 A1    H1    I      408.\n#  5 A1    H1    I      406.\n#  6 A1    H1    II     383.\n#  7 A1    H1    II     256.\n#  8 A1    H1    II     387.\n#  9 A1    H1    II     348.\n# 10 A1    H1    II     430.\n# # ... with 770 more rows\nselect_cols(data_vars, width_greater_than(3))\n# # A tibble: 780 x 6\n#    APLA_PLANT AIES_PLANT  CESP  DIES  MGRA  NGRA\n#         <dbl>      <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1       2.45       2.39  16.9  52.1 228.    609\n#  2       2.5        1.43  14.4  50.7 187.    427\n#  3       2.69       1.52  16.5  54.7 230.    497\n#  4       2.8        1.64  16.8  52.0 213.    523\n#  5       2.62       1.55  15.9  51.6 224.    551\n#  6       2.12       1.8   15    51.4 203.    529\n#  7       3.15       1.78  10.9  41.9  75.2   294\n#  8       2.97       1.84  15    53.4 204.    528\n#  9       3.1        1.78  13.6  50.8 187.    538\n# 10       3.02       1.6   16.3  53.9 250.    582\n# # ... with 770 more rows\ndf <- head(maize, 3)\ncolnames(df) <- c (\"Amg\", \"hib\", \"Rep\", \"APLA\", \"AIES\", \"CESp\", \"dies\", \"Mgra\", \"mmG\", \"ngra\")\nselect_cols(df, lower_case_only())\n# # A tibble: 3 x 3\n#   hib    dies  ngra\n#   <chr> <dbl> <dbl>\n# 1 H1     52.1   609\n# 2 H1     50.7   427\n# 3 H1     54.7   497\nselect_cols(df, upper_case_only())\n# # A tibble: 3 x 2\n#    APLA  AIES\n#   <dbl> <dbl>\n# 1  2.45  2.39\n# 2  2.5   1.43\n# 3  2.69  1.52\nselect_cols(df, title_case_only())\n# # A tibble: 3 x 3\n#   Amg   Rep    Mgra\n#   <chr> <chr> <dbl>\n# 1 A1    I      228.\n# 2 A1    I      187.\n# 3 A1    I      230."},{"path":"manipula.html","id":"remover-linhas-ou-colunas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.2 Remover linhas ou colunas","text":"Podemos usar remove_cols() e remove_rows() para remover colunas e linhas, respectivamente.funções remove_rows_na() e remove_rows_na() são usados para remover linhas e colunas com valores NA, respectivamente. ","code":"\nremove_cols(maize, AMB, HIB)\n# # A tibble: 780 x 8\n#    REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 II     2.12  1.8   15    51.4 203.   383.   529\n#  7 II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 II     2.97  1.84  15    53.4 204.   387.   528\n#  9 II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\ndata_with_na <- maize\ndata_with_na[c (1, 5, 10), c (3:5, 9:10)] <- NA\nremove_cols_na(data_with_na)\n# Warning: Column(s) REP, APLA, AIES, MMG, NGRA with NA values deleted.\n# # A tibble: 780 x 5\n#    AMB   HIB    CESP  DIES  MGRA\n#    <chr> <chr> <dbl> <dbl> <dbl>\n#  1 A1    H1     16.9  52.1 228. \n#  2 A1    H1     14.4  50.7 187. \n#  3 A1    H1     16.5  54.7 230. \n#  4 A1    H1     16.8  52.0 213. \n#  5 A1    H1     15.9  51.6 224. \n#  6 A1    H1     15    51.4 203. \n#  7 A1    H1     10.9  41.9  75.2\n#  8 A1    H1     15    53.4 204. \n#  9 A1    H1     13.6  50.8 187. \n# 10 A1    H1     16.3  53.9 250. \n# # ... with 770 more rows\nremove_rows_na(data_with_na)\n# Warning: Row(s) 1, 5, 10 with NA values deleted.\n# # A tibble: 777 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  2 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  3 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  4 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  5 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  6 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  7 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n#  8 A1    H1    III    2.69  1.52  15.6  49.5 195.   369.   529\n#  9 A1    H1    III    2.6   1.68  14.3  48.9 172.   344.   500\n# 10 A1    H1    III    2.82  1.52  18.4  54.3 255.   371.   689\n# # ... with 767 more rows"},{"path":"manipula.html","id":"ordenar-linhas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.3 Ordenar linhas","text":"função arrange()  é utilizada para ordenar linhas de um tibble (ou data.frames) com base em uma expressão envolvendo suas variáveis. Considerando funções que vimos até aqui, vamos computar média para MGRA, criar uma nova variável chamada Rank, qual corresponde ao ranqueamento dos híbridos para variável em questão e ordenar variável Rank em ordem crescente, onde o híbrido com maior média ficará na primeira linha.Exercício 3Considerando o exemplo anterior, ordene variável Rank em ordem decrescente.RespostaAo combinar função group_by() com arrange() é possível realizar o ordenamento para cada nível de um determinado fator. exemplo abaixo, variável APLA é ordenada de maneira crescente para cada híbrido.","code":"\nmaize %>%\n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA)) %>%\n  mutate(Rank = rank(MGRA_mean)) %>%\n  arrange(-Rank)\n# # A tibble: 13 x 3\n#    HIB   MGRA_mean  Rank\n#    <chr>     <dbl> <dbl>\n#  1 H6         188.    13\n#  2 H2         187.    12\n#  3 H4         184.    11\n#  4 H1         184.    10\n#  5 H5         184.     9\n#  6 H13        180.     8\n#  7 H7         171.     7\n#  8 H3         169.     6\n#  9 H11        167.     5\n# 10 H10        164.     4\n# 11 H8         160.     3\n# 12 H12        157.     2\n# 13 H9         153.     1\n\nmaize %>%\n  group_by(HIB) %>%\n  arrange(APLA, .by_group = TRUE)\n# # A tibble: 780 x 10\n# # Groups:   HIB [13]\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A3    H1    II     1.93  0.93  13    50.0 120.   276.   433\n#  2 A3    H1    I      2     1.05  19.9  53.3 253.   444.   570\n#  3 A3    H1    I      2.07  1.05  13.2  47.9 110.   293.   377\n#  4 A3    H1    II     2.08  0.94  12    47.6 103.   334.   309\n#  5 A3    H1    III    2.1   0.97  17.5  50.8 222.   423.   524\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A3    H1    I      2.12  1.03  18.5  52.0 214.   382.   560\n#  8 A3    H1    III    2.12  0.96  15    56.8 174.   339.   512\n#  9 A3    H1    I      2.13  1.05  11.6  47.0  89.5  300.   298\n# 10 A4    H1    I      2.13  1.1   12.8  47.6 144.   280.   516\n# # ... with 770 more rows"},{"path":"manipula.html","id":"selecionar-top-n-linhas-baseado-em-valor","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.4 Selecionar top n linhas baseado em valor","text":"função top_n() é usada para selecionar linhas superiores ou inferiores em cada grupo.","code":"\n# seleciona as duas linhas com o maior valor de MGRA\ntop_n(maize, 2, MGRA)\n# # A tibble: 2 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    I      2.92  1.64  18    56.0  289.  393.   734\n# 2 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n\n# seleciona as duas linhas com o menor valor de MGRA\ntop_n(maize, 2, -MGRA)\n# # A tibble: 2 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H9    III    2.72  1.54  11    42.8  58.5  295.   198\n# 2 A2    H8    I      1.92  0.63  12.1  39.7  59.5  243.   245\n\n# Maior produtividade em cada ambiente\nmaize %>%\n  group_by(AMB) %>%\n  top_n(1, MGRA)\n# # A tibble: 4 x 10\n# # Groups:   AMB [4]\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n# 2 A2    H6    III    3.18  1.62  19.2  53.0  270.  382.   708\n# 3 A3    H1    I      2     1.05  19.9  53.3  253.  444.   570\n# 4 A4    H10   I      2.65  1.47  14    50.3  287.  275.   493"},{"path":"manipula.html","id":"adicionar-novas-variáveis","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.5 Adicionar novas variáveis","text":"função mutate()  é utilizada quando se deseja adicionar novas variáveis conjunto de dados. Estas variáveis são funções de variáveis existentes. Como exemplo, vamos criar uma nova variável chamada PRE_2 conjunto de dados maize, qual será razão entre AIES e APLA. Note que função adiciona nova variável após última variável origina e mantém todas demais. Digamos que queríamos adicionar nova variável criada após variável REP, seguinte abordagem com o pacote dplyr deve ser usada.Com função add_cols(), o mesmo resultado pode ser obtido com:Exercício 2Crie uma variável chamada MGRA_kg qual será o resultado em quilogramas da massa de grãos.Crie uma variável chamada MGRA_kg qual será o resultado em quilogramas da massa de grãos.Selecione somente colunas HIB, AMB, REP e MGRA_Kg.Selecione somente colunas HIB, AMB, REP e MGRA_Kg.Selecione somente cinco linhas com maior valor de MGRA_Kg.Selecione somente cinco linhas com maior valor de MGRA_Kg.Resposta","code":"\n\nmaize %>% \n  mutate(PRE_2 = AIES/APLA) %>%\n  select(AMB, HIB, REP, PRE_2, everything())\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   PRE_2  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     0.976  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     0.572  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     0.565  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     0.586  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     0.592  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    0.849  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    0.565  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    0.620  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    0.574  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    0.530  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\n\nadd_cols(maize,\n         PRE_2 = AIES/APLA,\n         .after = REP)\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   PRE_2  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     0.976  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     0.572  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     0.565  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     0.586  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     0.592  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    0.849  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    0.565  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    0.620  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    0.574  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    0.530  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"concatenar-colunas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.6 Concatenar colunas","text":"função concatenate() pode ser usada para concatenar várias colunas de um conjunto de dados. concatenate() retorna um quadro de dados com todas colunas originais em .data mais variável concatenada, após última coluna, nomeada como new_var. Para escolher posição da nova variável, use o argumento .ou., como seguir.concatenate() também pode ser utilizada dentro de add_cols() para mutar um conjunto de dados. Para isso, é preciso utilizar o argumento pull = TRUE para que o valor concatenado seja exibido como um vetor. Note que agora o argumento .é utilizado dentro da função add_cols().","code":"\nconcatenate(maize, AMB:REP, .after = REP)\n# # A tibble: 780 x 11\n#    AMB   HIB   REP   new_var   APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I     A1_H1_I   2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I     A1_H1_I   2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I     A1_H1_I   2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I     A1_H1_I   2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    I     A1_H1_I   2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II    A1_H1_II  2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II    A1_H1_II  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II    A1_H1_II  2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II    A1_H1_II  3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II    A1_H1_II  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nmaize %>% \n  add_cols(AMB_REP =  concatenate(., AMB:REP, pull = TRUE),\n           .after = REP) %>% \n  head()\n# # A tibble: 6 x 11\n#   AMB   HIB   REP   AMB_REP   APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I     A1_H1_I   2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I     A1_H1_I   2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I     A1_H1_I   2.69  1.52  16.5  54.7  230.  464.   497\n# 4 A1    H1    I     A1_H1_I   2.8   1.64  16.8  52.0  213.  408.   523\n# 5 A1    H1    I     A1_H1_I   2.62  1.55  15.9  51.6  224.  406.   551\n# 6 A1    H1    II    A1_H1_II  2.12  1.8   15    51.4  203.  383.   529"},{"path":"manipula.html","id":"formatar-nomes-de-coluna","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.7 Formatar nomes de coluna","text":"funções colnames_to_lower(), colnames_to_upper() e colnames_to_title() podem ser usados para converter nomes de colunas em maiúsculas, minúsculas ou em formato de título, respectivamente .  ","code":"\ncolnames_to_lower(maize) %>% head()\n# # A tibble: 6 x 10\n#   amb   hib   rep    apla  aies  cesp  dies  mgra   mmg  ngra\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# 4 A1    H1    I      2.8   1.64  16.8  52.0  213.  408.   523\n# 5 A1    H1    I      2.62  1.55  15.9  51.6  224.  406.   551\n# 6 A1    H1    II     2.12  1.8   15    51.4  203.  383.   529\ncolnames_to_upper(maize) %>% head()\n# # A tibble: 6 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# 4 A1    H1    I      2.8   1.64  16.8  52.0  213.  408.   523\n# 5 A1    H1    I      2.62  1.55  15.9  51.6  224.  406.   551\n# 6 A1    H1    II     2.12  1.8   15    51.4  203.  383.   529\ncolnames_to_title(maize) %>% head()\n# # A tibble: 6 x 10\n#   Amb   Hib   Rep    Apla  Aies  Cesp  Dies  Mgra   Mmg  Ngra\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# 4 A1    H1    I      2.8   1.64  16.8  52.0  213.  408.   523\n# 5 A1    H1    I      2.62  1.55  15.9  51.6  224.  406.   551\n# 6 A1    H1    II     2.12  1.8   15    51.4  203.  383.   529"},{"path":"manipula.html","id":"reordenando-colunas","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.8 Reordenando colunas","text":"função reorder_cols() pode ser usada para reordenar colunas de um quadro de dados.É possível colocar colunas primeiro e último lugar rapidamente com columns_to_first() e columns_to_last(), respectivamente.","code":"\nreorder_cols(data_vars, contains(\"PLANT\"), .before = AMB) %>% head()\n# # A tibble: 6 x 10\n#   APLA_PLANT AIES_PLANT AMB   HIB   REP    CESP  DIES  MGRA   MMG  NGRA\n#        <dbl>      <dbl> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1       2.45       2.39 A1    H1    I      16.9  52.1  228.  375.   609\n# 2       2.5        1.43 A1    H1    I      14.4  50.7  187.  437.   427\n# 3       2.69       1.52 A1    H1    I      16.5  54.7  230.  464.   497\n# 4       2.8        1.64 A1    H1    I      16.8  52.0  213.  408.   523\n# 5       2.62       1.55 A1    H1    I      15.9  51.6  224.  406.   551\n# 6       2.12       1.8  A1    H1    II     15    51.4  203.  383.   529\nreorder_cols(data_vars, AMB, HIB, .after = REP) %>% head()\n# # A tibble: 6 x 10\n#   REP   AMB   HIB   APLA_PLANT AIES_PLANT  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr>      <dbl>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 I     A1    H1          2.45       2.39  16.9  52.1  228.  375.   609\n# 2 I     A1    H1          2.5        1.43  14.4  50.7  187.  437.   427\n# 3 I     A1    H1          2.69       1.52  16.5  54.7  230.  464.   497\n# 4 I     A1    H1          2.8        1.64  16.8  52.0  213.  408.   523\n# 5 I     A1    H1          2.62       1.55  15.9  51.6  224.  406.   551\n# 6 II    A1    H1          2.12       1.8   15    51.4  203.  383.   529\ncolumn_to_first(maize, CESP, DIES) %>% head()\n# # A tibble: 6 x 10\n#    CESP  DIES AMB   HIB   REP    APLA  AIES  MGRA   MMG  NGRA\n#   <dbl> <dbl> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1  16.9  52.1 A1    H1    I      2.45  2.39  228.  375.   609\n# 2  14.4  50.7 A1    H1    I      2.5   1.43  187.  437.   427\n# 3  16.5  54.7 A1    H1    I      2.69  1.52  230.  464.   497\n# 4  16.8  52.0 A1    H1    I      2.8   1.64  213.  408.   523\n# 5  15.9  51.6 A1    H1    I      2.62  1.55  224.  406.   551\n# 6  15    51.4 A1    H1    II     2.12  1.8   203.  383.   529"},{"path":"manipula.html","id":"obtendo-níveis-de-fatores","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.9 Obtendo níveis de fatores","text":"Para obter os níveis e o tamanho dos níveis de um fator, funções get_levels() e get_level_size() pode ser usado.\r\n","code":"\nget_levels(maize, AMB)\n# [1] \"A1\" \"A2\" \"A3\" \"A4\"\nget_level_size(maize, AMB)\n# # A tibble: 4 x 10\n#   AMB     HIB   REP  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n# * <chr> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n# 1 A1      195   195   195   195   195   195   195   195   195\n# 2 A2      195   195   195   195   195   195   195   195   195\n# 3 A3      195   195   195   195   195   195   195   195   195\n# 4 A4      195   195   195   195   195   195   195   195   195"},{"path":"manipula.html","id":"selecionar-linhas-com-base-em-seus-valores","chapter":"Capítulo 7 Manipulação de dados","heading":"7.1.10 Selecionar linhas com base em seus valores","text":"Utilizando função filter()  é possivel filtrar linhas de um conjunto de dados com base valor de suas variáveis. primeiro exemplo, selecionaremos linhas onde o valor da variável MGRA é maior que 280.segundo exemplo, selecionaremos apenas linhas onde MGRA é maior que 220 OU APLA é menor que 1.3 OU o NGRA é maior que 820.último exemplo, selecionaremos apenas linhas onde MGRA é maior que é maior que 220 E APLA é menor que 2.Isto é aproximadamente equivalente ao seguinte código R base.","code":"\nmaize %>% \n  filter(MGRA > 280)\n# # A tibble: 4 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    I      2.92  1.64  18    56.0  289.  393.   734\n# 2 A1    H10   I      2.92  1.61  20.3  55.4  283.  441.   641\n# 3 A1    H13   II     2.47  1.28  15.3  53.0  291.  417.   698\n# 4 A4    H10   I      2.65  1.47  14    50.3  287.  275.   493\nmaize %>% \n  filter(MGRA > 280 | APLA < 1.3 | NGRA > 820)\n# # A tibble: 10 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H6    I      2.92  1.64  18    56.0 289.   393.   734\n#  2 A1    H10   I      2.92  1.61  20.3  55.4 283.   441.   641\n#  3 A1    H13   II     2.47  1.28  15.3  53.0 291.   417.   698\n#  4 A2    H8    II     1.03  0.69  10.8  44.8  94.8  277.   342\n#  5 A2    H10   III    1.09  0.92  15    47.6 166.   299.   555\n#  6 A3    H10   I      1.04  0.71  14.8  45.5 112.   265.   423\n#  7 A3    H11   I      1     0.65  14.5  43.6 120.   210.   571\n#  8 A4    H8    I      2.65  1.67  18    50   277.   251.   903\n#  9 A4    H8    I      2.95  1.7   18.6  52.9 249.   302.   824\n# 10 A4    H10   I      2.65  1.47  14    50.3 287.   275.   493\nmaize %>% \n  filter(MGRA > 220 & APLA < 2)\n# # A tibble: 1 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H6    II     1.97  1.63  17.1  54.7  230.  375.   614\nmaize[maize$MGRA > 220 & maize$APLA < 2, ]"},{"path":"manipula.html","id":"trabalhando-com-números-e-seqüências-de-caracteres","chapter":"Capítulo 7 Manipulação de dados","heading":"7.2 Trabalhando com números e seqüências de caracteres","text":"","code":""},{"path":"manipula.html","id":"arredondando","chapter":"Capítulo 7 Manipulação de dados","heading":"7.2.1 Arredondando","text":"função round_cols() arredonda uma coluna selecionada ou um quadro de dados inteiro para o número especificado de casas decimais (padrão 0). Se nenhuma variável informada, todas variáveis numéricas serão arredondadas.Como alternativa, selecione variáveis para arredondar.","code":"\nround_cols(maize)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 214.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nround_cols(maize, MGRA, MMG, digits = 1)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375    609\n#  2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    I      2.8   1.64  16.8  52.0 214.   408.   523\n#  5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406    551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"extraindo-e-substituindo-números","chapter":"Capítulo 7 Manipulação de dados","heading":"7.2.2 Extraindo e substituindo números","text":"funções extract_number() e replace_number() pode ser usado para extrair ou substituir números. Como exemplo, extrairemos o número de cada genótipo em maize criando uma nova coluna, HIB_NUM, inserindo após coluna HIB.Para substituir números de uma determinada coluna por uma substituição especificada, use replace_number(). Por padrão, os números são substituídos por \"\". O argumento drop epull também podem ser usados, como mostrado acima.","code":"\nmaize %>% \n  add_cols(HIB_NUM = extract_number(HIB),\n           .after = HIB)\n# # A tibble: 780 x 11\n#    AMB   HIB   HIB_NUM REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr>   <dbl> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1          1 I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1          1 I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1          1 I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1          1 I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1          1 I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1          1 II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1          1 II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1          1 II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1          1 II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1          1 II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nreplace_number(maize, HIB)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H     I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H     I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H     I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H     I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H     I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H     II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H     II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H     II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H     II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H     II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nreplace_number(maize,\n               REP,\n               pattern = \"^I$\",\n               replacement = \"REP_1\")\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H1    REP_1  2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H1    REP_1  2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H1    REP_1  2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H1    REP_1  2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H1    REP_1  2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"extraindo-substituindo-e-removendo-strings","chapter":"Capítulo 7 Manipulação de dados","heading":"7.2.3 Extraindo, substituindo e removendo strings","text":"funções extract_string() e replace_string() são usados mesmo contexto de extract_number() e replace_number(), mas para lidar com seqüências de caracteres.Para substituir strings, podemos usar função replace_strings().Para remover todas seqüências de caracteres de um quadro de dados, use remove_strings().","code":"\nextract_string(maize, HIB)\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    H     I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    H     I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    H     I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    H     I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    H     I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    H     II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    H     II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    H     II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    H     II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    H     II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nreplace_string(maize,\n               HIB,\n               replacement = \"HIB_\")\n# # A tibble: 780 x 10\n#    AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 A1    HIB_1 I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2 A1    HIB_1 I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3 A1    HIB_1 I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4 A1    HIB_1 I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5 A1    HIB_1 I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6 A1    HIB_1 II     2.12  1.8   15    51.4 203.   383.   529\n#  7 A1    HIB_1 II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8 A1    HIB_1 II     2.97  1.84  15    53.4 204.   387.   528\n#  9 A1    HIB_1 II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10 A1    HIB_1 II     3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows\nremove_strings(maize)\n# # A tibble: 780 x 10\n#      AMB   HIB   REP  APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1     1     1    NA  2.45  2.39  16.9  52.1 228.   375.   609\n#  2     1     1    NA  2.5   1.43  14.4  50.7 187.   437.   427\n#  3     1     1    NA  2.69  1.52  16.5  54.7 230.   464.   497\n#  4     1     1    NA  2.8   1.64  16.8  52.0 213.   408.   523\n#  5     1     1    NA  2.62  1.55  15.9  51.6 224.   406.   551\n#  6     1     1    NA  2.12  1.8   15    51.4 203.   383.   529\n#  7     1     1    NA  3.15  1.78  10.9  41.9  75.2  256.   294\n#  8     1     1    NA  2.97  1.84  15    53.4 204.   387.   528\n#  9     1     1    NA  3.1   1.78  13.6  50.8 187.   348.   538\n# 10     1     1    NA  3.02  1.6   16.3  53.9 250.   430.   582\n# # ... with 770 more rows"},{"path":"manipula.html","id":"formatando-strings","chapter":"Capítulo 7 Manipulação de dados","heading":"7.2.4 Formatando strings","text":"função tidy_strings() organiza cadeias de caracteres, colunas não numéricas ou quaisquer colunas selecionadas em um quadro de dados, colocando todas palavras em maiúsculas, substituindo qualquer espaço, tabulação e caracteres de pontuação por _ e colocando _ entre letras maiúsculas e minúsculas. Considere seguintes cadeias de caracteres: messy_env por definição deve representar um nível único ambiente de fator (ambiente 1). messy_gen mostra seis genótipos, emessy_int representa interação desses genótipos com o ambiente 1.Esses vetores de caracteres são visualmente confusos. Vamos arrumá-los.O tidy_strings() também funciona para arrumar um quadro de dados inteiro ou colunas específicas. Vamos criar um quadro de dados ‘bagunçado’ contexto de testes de melhoramento de plantas.","code":"\nmessy_env <- c(\"ENV 1\", \"Env 1\", \"Env1\", \"env1\", \"Env.1\", \"Env_1\")\nmessy_gen <- c(\"GEN1\", \"gen 2\", \"Gen.3\", \"gen-4\", \"Gen_5\", \"GEN_6\")\nmessy_int <- c(\"Env1Gen1\", \"Env1_Gen2\", \"env1 gen3\", \"Env1 Gen4\", \"ENV_1GEN5\", \"ENV1GEN6\")\ntidy_strings(messy_env)\n# [1] \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\" \"ENV_1\"\ntidy_strings(messy_gen)\n# [1] \"GEN_1\" \"GEN_2\" \"GEN_3\" \"GEN_4\" \"GEN_5\" \"GEN_6\"\ntidy_strings(messy_int)\n# [1] \"ENV_1_GEN_1\" \"ENV_1_GEN_2\" \"ENV_1_GEN_3\" \"ENV_1_GEN_4\" \"ENV_1_GEN_5\"\n# [6] \"ENV_1_GEN_6\"\nlibrary(tibble)\ndf <- tibble(Env = messy_env,\n             gen = messy_gen,\n             Env_GEN = interaction(Env, gen),\n             y = rnorm (6, 300, 10))\ndf\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <fct>       <dbl>\n# 1 ENV 1 GEN1  ENV 1.GEN1   294.\n# 2 Env 1 gen 2 Env 1.gen 2  300.\n# 3 Env1  Gen.3 Env1.Gen.3   291.\n# 4 env1  gen-4 env1.gen-4   302.\n# 5 Env.1 Gen_5 Env.1.Gen_5  293.\n# 6 Env_1 GEN_6 Env_1.GEN_6  318.\ntidy_strings(df)\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <chr>       <dbl>\n# 1 ENV_1 GEN_1 ENV_1_GEN_1  294.\n# 2 ENV_1 GEN_2 ENV_1_GEN_2  300.\n# 3 ENV_1 GEN_3 ENV_1_GEN_3  291.\n# 4 ENV_1 GEN_4 ENV_1_GEN_4  302.\n# 5 ENV_1 GEN_5 ENV_1_GEN_5  293.\n# 6 ENV_1 GEN_6 ENV_1_GEN_6  318.\ntidy_strings(df, gen)\n# # A tibble: 6 x 4\n#   Env   gen   Env_GEN         y\n#   <chr> <chr> <fct>       <dbl>\n# 1 ENV 1 GEN_1 ENV 1.GEN1   294.\n# 2 Env 1 GEN_2 Env 1.gen 2  300.\n# 3 Env1  GEN_3 Env1.Gen.3   291.\n# 4 env1  GEN_4 env1.gen-4   302.\n# 5 Env.1 GEN_5 Env.1.Gen_5  293.\n# 6 Env_1 GEN_6 Env_1.GEN_6  318."},{"path":"manipula.html","id":"selecionar-linhas-por-sua-posição","chapter":"Capítulo 7 Manipulação de dados","heading":"7.3 Selecionar linhas por sua posição","text":"função slice() é usada para selecionar linhas por sua posição ordinal tibble. Os tibbles agrupados usam posição ordinal dentro grupo.","code":"\n# seleciona as três primeiras linhas\nslice(maize, 1:3)\n# # A tibble: 3 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# Seleciona as 3 últimas linhas\nslice(maize, 778:n())\n# # A tibble: 3 x 10\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A4    H13   III    2.2   0.9   12.1  40.9  92.8  322.   288\n# 2 A4    H13   III    2.15  1.07  10.6  46.0  91.4  300.   305\n# 3 A4    H13   III    2.19  1.12  14.5  51.9 144.   352.   408\n# seleciona as duas primeiras linhas de cada ambiente\nmaize %>%\n  group_by(AMB) %>%\n  slice(1:2)\n# # A tibble: 8 x 10\n# # Groups:   AMB [4]\n#   AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427\n# 3 A2    H1    I      3.06  1.89  18.2  54.4  244.  421.   581\n# 4 A2    H1    I      3.04  1.89  15.4  53.4  193.  369.   523\n# 5 A3    H1    I      2.12  1.03  18.5  52.0  214.  382.   560\n# 6 A3    H1    I      2     1.05  19.9  53.3  253.  444.   570\n# 7 A4    H1    I      2.13  1.1   12.8  47.6  144.  280.   516\n# 8 A4    H1    I      2.3   1.25  13.1  50.0  140.  230.   609"},{"path":"manipula.html","id":"combinando-os-verbos-para-manipulação","chapter":"Capítulo 7 Manipulação de dados","heading":"7.4 Combinando os verbos para manipulação","text":"Esta sessão tem o objetivo de demonstrar como os verbos dplyr em conjunto com funções pivot_longer() pacote tidyr21 e column_to_rownames()  pacote tibble22 podem ser combinados para construir uma matriz dupla entrada onde linhas correspondem aos genótipos e colunas correspondem aos ambientes. Esta matriz será preenchida com o valor médio da MGRA considerando apenas duas primeiras repetições de cada híbrido em cada ambiente.\r\n\r\n\r\n\r\n\r\nNote que mesma tabela dupla entrada pode ser obtida com função make_mat() pacote metan.","code":"\nmaize %>%\n  filter(REP %in% c(\"I\", \"II\")) %>%\n  group_by(AMB, HIB) %>%\n  summarise(MGRA_me = mean(MGRA)) %>%\n  pivot_wider(names_from = HIB, values_from = MGRA_me) %>%\n  round_cols(digits = 1)\n# # A tibble: 4 x 14\n# # Groups:   AMB [4]\n#   AMB      H1   H10   H11   H12   H13    H2    H3    H4    H5    H6    H7    H8\n#   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1 A1     200.  185.  199.  174.  222.  205.  201.  204.  190.  238.  185.  198.\n# 2 A2     194.  151.  169.  136.  158.  219.  201.  193.  180.  204.  148.  116 \n# 3 A3     147.  117   127.  153.  186.  161.  146.  150   147.  127.  146.  149.\n# 4 A4     195.  180.  170.  191.  167   156.  148.  182.  209.  164.  195.  182.\n# # ... with 1 more variable: H9 <dbl>\nmaize %>%\n  filter(REP %in% c(\"I\", \"II\")) %>%\n  make_mat(AMB, HIB, MGRA) %>% \n  round_cols(digits = 1)\n#       H1   H10   H11   H12   H13    H2    H3    H4    H5    H6    H7    H8\n# A1 200.2 185.1 199.3 174.5 221.9 204.9 201.3 204.4 189.5 238.2 184.8 198.2\n# A2 193.8 151.3 168.7 136.2 158.3 218.8 200.6 193.3 180.1 204.5 148.2 116.0\n# A3 147.2 117.0 127.1 153.1 186.5 160.6 146.4 150.0 147.1 127.4 146.4 148.7\n# A4 195.2 179.5 169.5 190.9 167.0 155.5 147.7 182.1 208.7 164.3 195.4 181.9\n#       H9\n# A1 203.4\n# A2 107.2\n# A3 117.3\n# A4 154.8"},{"path":"manipula.html","id":"trabalhando-com-duas-tabelas-ao-mesmo-tempo","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5 Trabalhando com duas tabelas ao mesmo tempo","text":"","code":""},{"path":"manipula.html","id":"junções-com-mutação-de-dados","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.1 Junções com mutação de dados","text":"É raro que uma análise de dados envolva apenas uma única tabela de dados. Na prática, diversas tabela podem existir e ferramentas flexíveis para combiná-las são necessárias. dplyr, existem três famílias de verbos que permitem trabalhar com duas tabelas ao mesmo tempo, permitindo: () juntar tabelas, (ii) Filtrar registros e (iii) realizar operações.Os seguintes códigos criam três novos conjuntos de dados. maize2 contém dados de duas repetições para os híbridos H1:H5 nos ambientes H1 e H2. mean_h e mean_a contém médias para os híbridos e ambientes, respectivamente. Juntando coluna MGRA e NGRA da tabela mean_h na tabela maize2 considerando variáveis com mesmo nome nas duas tabelas (neste caso, HIB)Juntando colunas da tabela mean_a na tabela maize2","code":"\nmaize_small <- \n  maize %>%\n  filter(HIB %in% c(\"H1\", \"H2\", \"H3\")) %>%\n  filter(AMB %in% c(\"A1\", \"A2\"))\n\nmaize2 <- \n  maize_small %>% \n  means_by(AMB, HIB) %>% \n  select(AMB:APLA) %>% \n  round_cols(digits = 1)\nmaize2\n# # A tibble: 6 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H1      2.7\n# 2 A1    H2      2.8\n# 3 A1    H3      2.9\n# 4 A2    H1      2.9\n# 5 A2    H2      2.9\n# 6 A2    H3      2.9\n\nmean_h <- \n  maize_small %>%\n  means_by(HIB) %>% \n  select(HIB, contains(\"A\")) %>% \n  round_cols(digits = 1)\nmean_h\n# # A tibble: 3 x 5\n#   HIB    APLA  AIES  MGRA  NGRA\n#   <chr> <dbl> <dbl> <dbl> <dbl>\n# 1 H1      2.8   1.7  195.  506.\n# 2 H2      2.9   1.5  211.  577.\n# 3 H3      2.9   1.7  195.  542.\n\nmean_a <-\n  maize_small %>%\n  means_by(AMB) %>% \n  select(AMB, contains(\"ES\")) %>% \n  round_cols(digits = 1)\nmean_a\n# # A tibble: 2 x 4\n#   AMB    AIES  CESP  DIES\n#   <chr> <dbl> <dbl> <dbl>\n# 1 A1      1.5  15.3  51.6\n# 2 A2      1.8  15.4  51.8\nleft_join(maize2, mean_h %>% select(HIB, MGRA, NGRA), by = \"HIB\")\n# # A tibble: 6 x 5\n#   AMB   HIB    APLA  MGRA  NGRA\n#   <chr> <chr> <dbl> <dbl> <dbl>\n# 1 A1    H1      2.7  195.  506.\n# 2 A1    H2      2.8  211.  577.\n# 3 A1    H3      2.9  195.  542.\n# 4 A2    H1      2.9  195.  506.\n# 5 A2    H2      2.9  211.  577.\n# 6 A2    H3      2.9  195.  542.\nfull_join(maize2, mean_a, by = \"AMB\")\n# # A tibble: 6 x 6\n#   AMB   HIB    APLA  AIES  CESP  DIES\n#   <chr> <chr> <dbl> <dbl> <dbl> <dbl>\n# 1 A1    H1      2.7   1.5  15.3  51.6\n# 2 A1    H2      2.8   1.5  15.3  51.6\n# 3 A1    H3      2.9   1.5  15.3  51.6\n# 4 A2    H1      2.9   1.8  15.4  51.8\n# 5 A2    H2      2.9   1.8  15.4  51.8\n# 6 A2    H3      2.9   1.8  15.4  51.8"},{"path":"manipula.html","id":"junções-com-filtragem-de-dados","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.2 Junções com filtragem de dados","text":"Filtrando linhas da tabela maize2 com base nas variáveis que combinam na tabela mean_h (neste caso, coluna HIB) Filtrando linhas da tabela maize2 com base nas variáveis que NÃO combinam na tabela mean_h (neste caso, coluna HIB) ","code":"\nsemi_join(maize2, mean_h, by = c(\"HIB\", \"APLA\"))\n# # A tibble: 3 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H3      2.9\n# 2 A2    H2      2.9\n# 3 A2    H3      2.9\nanti_join(maize2, mean_h, by = c(\"HIB\", \"APLA\"))\n# # A tibble: 3 x 3\n#   AMB   HIB    APLA\n#   <chr> <chr> <dbl>\n# 1 A1    H1      2.7\n# 2 A1    H2      2.8\n# 3 A2    H1      2.9"},{"path":"manipula.html","id":"operações-com-conjuntos-vetores","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.3 Operações com conjuntos (vetores)","text":"Operações com conjuntos são importantes na análise de dados agronômicos. Por exemplo, se um genótipo foi selecionado por um determinado índice nos ambientes , B e C, então, este determinado genótipo é interseção dos ambientes , B e C. base R fornece funções para operações de conjunto, mas funciona com dois conjuntos de uma vez apenas. Vamos ver como podemos estimar interseção de três conjuntos com funções R de base.Observe que precisamos chamar intersect() várias vezes neste caso. O novo grupo de funções set_union(), set_difference() e set_intersect() pacote metan supera o problema de computação de união, interseção e diferenças de dois conjuntos apenas com funções de base R.","code":"\n(A <- letters[1:4])\n# [1] \"a\" \"b\" \"c\" \"d\"\n\n(B <- letters[2:5])\n# [1] \"b\" \"c\" \"d\" \"e\"\n\n(C <- letters[3:7])\n# [1] \"c\" \"d\" \"e\" \"f\" \"g\"\n\n(D <- letters[1:12])\n#  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\"\n\nset_lits <- list(A = A, B = B, C = C, D = D)\n\n# intersecção de A, B e C\nintersect(intersect(A, B), C)\n# [1] \"c\" \"d\"\n# Intersecção de A e B\nset_intersect(A, B)\n# [1] \"b\" \"c\" \"d\"\n\n# Intersecção de A, B e C\nset_intersect(A, B, C)\n# [1] \"c\" \"d\"\n\n\n# União de todos os conjuntos\n# Todas as funções entendem um objeto de classe lista\n\nset_union(set_lits)\n#  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\"\n\n# Intersecção de todos os conjuntos\nset_intersect(set_lits)\n# [1] \"c\" \"d\""},{"path":"manipula.html","id":"operações-com-conjuntos-data-frames","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.4 Operações com conjuntos (data frames)","text":"Nesta seção será demonstrado como é possivel utilizar operações de cojuntos como interseção e união. É esperado que entradas x e y tenham mesmas variáveis. Para isto, vamos criar dois novos conjuntos de dados chamados data_1_to_5 e data_3_to_10, quais contém, respectivamente cinco primeiras linhas\r\ne linhas 3 10 de maize. Note que função slice()  é utilizada para selecionar linhas com base em sua posição.","code":"\nlibrary(tidyverse)\ndata_1_to_5 <- \n  maize %>%\n  slice(1:5) %>% \n  add_cols(id = 1:5, .before = 1)\ndata_3_to_10 <- \n  maize %>%\n  slice(3:10) %>% \n  add_cols(id = 3:10, .before = 1)"},{"path":"manipula.html","id":"interseção-de-conjuntos","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.4.1 Interseção de conjuntos","text":"função set_intersect() (interseção de conjunto) retorna somente linhas presentes nos dois conjuntos, neste caso, linhas 3, 4 5 conjunto maize","code":"\nset_intersect(data_1_to_5, data_3_to_10)\n# # A tibble: 3 x 11\n#      id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1     3 A1    H1    I      2.69  1.52  16.5  54.7  230.  464.   497\n# 2     4 A1    H1    I      2.8   1.64  16.8  52.0  213.  408.   523\n# 3     5 A1    H1    I      2.62  1.55  15.9  51.6  224.  406.   551"},{"path":"manipula.html","id":"união-de-conjuntos","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.4.2 União de conjuntos","text":"função set_union() (união de conjunto) junta os dois conjuntos sem que haja duplicação de registros.","code":"\nset_union(data_1_to_5, data_3_to_10)\n# # A tibble: 10 x 11\n#       id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#    <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1     1 A1    H1    I      2.45  2.39  16.9  52.1 228.   375.   609\n#  2     2 A1    H1    I      2.5   1.43  14.4  50.7 187.   437.   427\n#  3     3 A1    H1    I      2.69  1.52  16.5  54.7 230.   464.   497\n#  4     4 A1    H1    I      2.8   1.64  16.8  52.0 213.   408.   523\n#  5     5 A1    H1    I      2.62  1.55  15.9  51.6 224.   406.   551\n#  6     6 A1    H1    II     2.12  1.8   15    51.4 203.   383.   529\n#  7     7 A1    H1    II     3.15  1.78  10.9  41.9  75.2  256.   294\n#  8     8 A1    H1    II     2.97  1.84  15    53.4 204.   387.   528\n#  9     9 A1    H1    II     3.1   1.78  13.6  50.8 187.   348.   538\n# 10    10 A1    H1    II     3.02  1.6   16.3  53.9 250.   430.   582"},{"path":"manipula.html","id":"diferença-de-conjuntos","chapter":"Capítulo 7 Manipulação de dados","heading":"7.5.4.3 Diferença de conjuntos","text":"função set_difference()  (diferença de conjunto, ou complementar) cria uma tabela somente com os registros em data_1_to_5 que não estão em data_3_to_10.","code":"\nset_difference(data_1_to_5, data_3_to_10)\n# # A tibble: 2 x 11\n#      id AMB   HIB   REP    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <int> <chr> <chr> <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1     1 A1    H1    I      2.45  2.39  16.9  52.1  228.  375.   609\n# 2     2 A1    H1    I      2.5   1.43  14.4  50.7  187.  437.   427"},{"path":"graph.html","id":"graph","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"Capítulo 8 Gráficos com o pacote ggplot2","text":"“O gráfico simples trouxe mais informações à mente analista de dados\r\nque qualquer outro dispositivo.” — John Tukey","code":""},{"path":"graph.html","id":"o-pacote-ggplot2","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.1 O pacote ggplot2","text":"O ggplot2 (https://ggplot2.tidyverse.org/)\\indt{ggplot2} é um pacote R para produção de gráficos que diferentemente da maioria dos outros pacotes, apresenta uma profunda gramática baseada livro grammar graphics (Wilkinson 2005). Os gráficos originados em ggplot2 são baseados em camadas, e cada gráfico tem três componentes chave: data, os dados de onde o gráfico será criado; aes() (aesthetic mappings), que controla o mapeamento estético e propriedades visuais gráfico; e ao menos uma camada que irá descrever como cada observação será renderizada. Camadas são usualmente criadas utilizando uma função geom_(). referência principal ao pacote é o livro Ggplot2 : elegant graphics data analysis (Wickham 2009).","code":""},{"path":"graph.html","id":"meu-primeiro-gráfico-em-ggplot2","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.2 Meu primeiro gráfico em ggplot2","text":"seguir, vamos discutir os aspcetos básicos para construção de gráficos utilizando o pacote ggplot2. função plot_grid() pacote cowplot23 foi utilizado aqui para organizar os gráficos em forma de painéis. O pacote qqplotr24 também é utilizado como uma extensão pacote ggplot225 para confecção de gráficos tipo Q-Q plots. Os dados contidos na aba gg arquivo data_R.xlsx serão utilizados. Estes dados podem ser carregados pelo seguinte comando.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\ndados_gg <- import(url, sheet = \"gg\")\nstr(dados_gg)\n# 'data.frame': 120 obs. of  6 variables:\n#  $ AMB  : chr  \"E1\" \"E1\" \"E1\" \"E1\" ...\n#  $ GEN  : chr  \"G1\" \"G1\" \"G1\" \"G2\" ...\n#  $ BLOCO: num  1 2 3 1 2 3 1 2 3 1 ...\n#  $ RG   : num  2167 2503 2427 3208 2933 ...\n#  $ PH   : num  44.9 46.9 47.8 45.2 45.3 ...\n#  $ MMG  : num  31.3 32.9 32.3 29 30.5 ..."},{"path":"graph.html","id":"as-camadas-de-um-gráfico-ggplot2","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.3 As camadas de um gráfico ggplot2","text":"ggplot2, os gráficos são construídos camada por camada (ou, layers, em inglês). Neste exemplo, vamos confecionar um gráfico onde o eixo x será representado pela variável RG e o eixo y pela variável PH.Este comando criou um gráfico e armazenou objeto p1, que será plotado posteriormente. Observe que o primeiro argumento da função é o data frame onde nossos dados foram armazenados. função aes()  descreve como variáveis são mapeadas (neste caso RG eixo x e PH eixo y). função geom_point() definiu que forma geométrica ser utilizada é baseada em pontos, gerando, assim, um gráfico de dispersão. Isto é tudo que precisa ser feito para confecção de um gráfico simples.","code":"\np1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n      geom_point()"},{"path":"graph.html","id":"aesthetics-estética","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.4 Aesthetics (estética)","text":"“O maior valor de uma imagem é quando ela nos obriga perceber\r\no que nunca esperamos ver.” — John TukeyAlterar estética dos gráficos ggplot2 é uma tarefa relativamente simples. gráfico anterior, os valores PH e RG foram plotados sem nenhum tipo de mapeamento estético. Digamos que marcadores com diferentes cores para cada ambiente poderia nos ajudar compreender melhor o padrão presente em nossos dados. Vamos confecionar este gráfico.Ao incluirmos colour = AMB dentro da função aes, dizemos ao ggplot que os pontos devem ser mapeados esteticamente (neste caso utilziando cores) para cada nível fator AMB presente em nossos dados. Digamos que em vez de utilizar diferentes cores, os ambientes deveriam ser representados por diferentes tipos de marcadores (quadrados, triângulo, etc.) Neste caso, o argumento colour = AMB deveria ser substituído por shape = AMB.\r\nFigure 8.1: Gráfico de dispersão padrão (p1) e com pontos mapeados por cores (p2) e marcadores (p3) para cada nível fator ‘AMB’.\r\nExercício 4Constua um gráfico semelhante ao anterior, onde o tamanho dos pontos deve ser baseado em uma terceira variável nosso conjunto de dados, neste exemplo, MMG.Resposta","code":"\np2 <- ggplot(dados_gg, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()\np3 <- ggplot(dados_gg, aes(x = RG, y = PH, shape = AMB)) +\n      geom_point()\nplot_grid(p1, p2, p3,\n          ncol = 3,\n          labels = c(\"p1\", \"p2\", \"p3\"),\n          rel_widths = c(1, 1.2, 1.2))"},{"path":"graph.html","id":"facet-facetas","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.5 Facet (facetas)","text":"Mapeando os diferentes níveis de AMB para diferentes cores, incluímos em um único gráfico os dados de todos os ambientes. Mas, e se nosso objetivo fosse realizar um gráfico para cada ambiente? O ggplot2 tem uma poderosa ferramenta para isto: funções facet_. Ao utilziar estas funções, o conjunto de dados é subdividido e um gráfico é construído para cada um destes subconjuntos. Vamos ver como elas podem nos ajudar em nossso problema. Neste exemplo, um gráfico completamente diferente anterior é gerado com apenas uma simples modificação: excluímos mapeamento estético o argumento colour = AMB e incluímos uma nova função, facet_wrap(~AMB). Neste caso, informamos que um gráfico deveria ser realizado para cada ambiente. Simples, não? exemplo anterior, utilizamos função facet_wrap() para confeccionar um gráfico foi criado para cada nível fator AMB.Substitua função facet_wrap(~ AMB) por facet_grid(~ AMB) e compare os dois gráficos.","code":"\nfac1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n        geom_point()+\n        facet_wrap(~AMB)"},{"path":"graph.html","id":"theme-temas","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.6 Theme (temas)","text":"Cada gráfico criado com função ggplot() tem um tema padrão. Tema, aqui, é toda propriedade relacionada ao aspecto visual gráfico, que não foi definida na função aes() e que pode ser modificada utilizando função theme() (veja ?theme). O ggplot2 já conta com alguns temas personalizados para facilitar nosso trabalho. Considerando o exemplo anterior, vamos utilziar função theme_bw() (preto e branco) e função theme() para modificar propriedades visuais gráfico.\r\nFigure 8.2: Gráfico de dispersão considerando confecção de um gráfico para cada nível de um fator(f1) e modificações na propriedades tema de um gráfico ggplot2 (f2)\r\nOs argumentos inseridos dentro das função theme() modificaram aparência nosso gráfico. Inúmeros outros argumentos são disponíveis, fazendo com que os gráficos originados sejam completamente personalizáveis. Digamos que precisamos confecionar diversos gráficos e gostaríamos de manter o mesmo tema gráfico acima. Seria exaustivo e desinteressante informar cada vez estes argumentos para cada gráfico, não? Felizmente, outra poderosa ferramenta proporcionada pelo ggplot2 é possibilidade de confecionarmos nossos próprios temas. Para isto, vamos executar o seguinte comando para criar um tema personalizado (my_theme()). Este tema pode então ser aplicado como uma camada adicional cada gráfico que confecionarmos. Para evitar necessidade da inclusão deste tema em cada gráfico gerado, iremos definir este tema como padrão utilizando função theme_set() .Exercício 5Constua um gráfico semelhante ao observado acima, onde diferentes cores devem ser atribuídas para cada genótipo. Em adição, aplique o tema personalizado que acabamos de criar ao gráfico.Resposta","code":"\nfac2 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n        geom_point() +\n        facet_wrap(~AMB) +\n        theme_bw() +\n        theme(panel.grid = element_blank(), # remove as linhas do corpo do gráfico\n             # sem bordas entre os painéis\n              panel.spacing = unit(0, \"cm\"),\n             # modifica o texto dos eixos\n              axis.text = element_text(size = 12, colour = \"black\"),\n             # cor dos marcadores\n              axis.ticks = element_line(colour = \"black\"),\n             # tamanho dos marcadores\n              axis.ticks.length = unit(.2, \"cm\"), \n             #cor da borda\n              panel.border = element_rect(colour = \"black\", fill = NA, size = 0.5))+\n       # título dos eixos\n       labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\") \n\nplot_grid(fac1, fac2, labels = c(\"f1\", \"f2\"))\nmy_theme <- function () {\n  theme_bw() %+replace% # permite que os valores informados possam ser sobescritos\n    theme(axis.ticks.length = unit(.2, \"cm\"),\n          axis.text = element_text(size = 12, colour = \"black\"),\n          axis.title = element_text(size = 12, colour = \"black\"),\n          axis.ticks = element_line(colour = \"black\"),\n          panel.border = element_rect(colour = \"black\", fill = NA, size = 0.5),\n          panel.grid =  element_blank())\n}\ntheme_set(my_theme())"},{"path":"graph.html","id":"geoms-geometria","chapter":"Capítulo 8 Gráficos com o pacote ggplot2","heading":"8.7 Geoms (geometria)","text":"funções geom_ definem qual forma geométrica será utilizada para visualização dos dados gráfico. Até agora, utilizamos função geom_point()  para construir gráficos de dispersão. Basicamente, qualquer outro tipo de gráfico pode ser criado dependendo da função geom_ utilizada. Dentre diversas disponíveis pacote ggplot2 funções geom_ mais utilizadas são:geom_abline(): para retas definidas por um intercepto e uma inclinação;geom_hline(): para retas horizontais definidas por um intercept y;geom_vline(): para retas verticais definidas por um intercept x;geom_boxplot(): para boxplots;geom_histogram(): para histogramas de frequência;geom_smooth(): ajusta uma função para o conjunto de dados e mostra uma banda de confiança;geom_density(): para densidades;geom_area(): para áreas;geom_bar(): para barras;geom_errorbar() para barras de erro;Deste ponto em diante, vamos confeccionar alguns exemplos utilziando algumas destas funções (ou combinações destas funções) incluindo argumentos de mapeamento de estética e temas vistos até agora. \r\nFigure 8.3: Gráfico de dispersão, combinando pontos e linhas de regressão.\r\nExercício 6\r\ngráfico s1, uma regressão linear foi ajustada quando incluímos função geom_smooth(method = \"lm\", se = F).Como este gráfico pode nos ajudar compreender relação entre variáveis RG e PH? Ao incluir o argumento colour = AMB, uma regressão para cada ambiente foi ajustada (s2).Como este gráfico pode nos ajudar compreender relação entre variáveis RG e PH? Ao incluir o argumento colour = AMB, uma regressão para cada ambiente foi ajustada (s2).Modifique o gráfico s2 para que os ambientes ainda continuem sendo mapeados por cores, mas uma única linha de regressão seja ajustada.Modifique o gráfico s2 para que os ambientes ainda continuem sendo mapeados por cores, mas uma única linha de regressão seja ajustada.RespostaGráficos tipo boxplot\r\nFigure 8.4: Gráfico tipo boxplot combinando mapeamentos estéticos e inclusão de linhas.\r\nlinha orizontal tracejada representa média geral GY. Seis estatísticas são mostradas neste boxplot. mediana (linha horizontal), média (ponto) caixas inferior e superior correspondem ao primeiro e terceiro quartil (percentis 25 e 75, respectivamente). linha vertical superior se estende da caixa até o maior valor, não maior que \\(1,5 \\times {IQR}\\) (onde IQR é amplitude interquartílica). linha vertical inferior se estende da caixa até o menor valor, de máximo, \\(1,5 \\times {IQR}\\). Dados além das linhas horizontais podem ser considerados outliers.\\(~\\)Gráficos tipo histograma\r\nFigure 8.5: Gráfico tipo histograma com estimativas de função de probabilidade kernel e normal.\r\nhistograma (h1), linha preta representa estimativa de densidade Kernel (Silverman 1998). linha vermelha representa estimativa da função de probabilidade normal. Para isto, escala eixo y foi mudada de contagem para densidade.\\(~\\)Gráficos tipo barra\r\nFigure 8.6: Gráfico tipo barras, com mapeamento estético e barras de erro.\r\nafirmação de que um gráfico ggplot2 é feito em camadas fica mais evidente aqui. gráfico p1, barras representam médias geral dos híbridos em nosso conjunto de dados. segundo gráfico, um novo argumento visto (fill = AMB). Isto informa que barras devem ser coloridas para cada nível fator AMB. função stat_summary(),  também vista pela primeira vez aqui, foi utilizada segundo gráfico para substituir função geom_bar(). Com isto, foi possível incluir médias (fun = mean e geom = \"bar), bem como barras de erro (fun.data = mean_se e geom = \"errorbar\"). Gráficos de dispersão com linhas de valores preditos\r\nFigure 8.7: Gráfico de dispersão combinado com inclusão de curvas ajustadas.\r\nGráficos tipo quantil-quantil (Q-Q plots)Esta função é muito util para verificar normalidade dos resíduos da ANOVA e regressões lineares ou não lineares. programação abaixo foi utilizada artigo de Lúcio, Santos, Olivoto (2017) para demonstrar intepretação dos gráficos. funções stat_qq_band(), stat_qq_line() e stat_qq_point() pacote qqplotr26 serão utilizadas. Este é uma das inúmeras extensões pacote ggplot2 que podem ser encontradas aqui.27.\r\nFigure 8.8: Gráfico quantil-quantil de conjuntos de dados com assimetria à esquerda, direita e com distribuição normal.\r\n   \r\nNestes exemplos vimoss alguns gráficos simples que podem ser originados pelo ggplot2. potencialidades deste pacote, entanto vão muito além. Uma galeria com diversos exemplos de gráficos ggplot2 com códigos disponíveis pode ser vista aqui.28Note que gráfico acima, funções pacote qqplotr foram carregadas utilizando qqplotr::. Neste caso, indicamos que função desejada é uma função deste pacote. Isto é útil, principalmente quando dois pacotes tem funções com o mesmo nome. Utilizando :: especificamos de qual pacote função deve ser carregada.","code":"\n\ns1 <- ggplot(dados_gg, aes(x = RG, y = PH)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+ # estima uma regressão linear\n      labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\")\n\ns2 <- ggplot(dados_gg, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+\n      labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\")\nplot_grid(s1, s2, labels = c(\"s1\", \"s2\"), rel_widths  = c(1, 1.2))\n\nmean_rg <- mean(dados_gg$RG) # calcula a média geral do RG\nbox1 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_boxplot()\n\nbox2 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_boxplot(width = 0.5, col = \"black\", fill = \"gray\")+ # boxplot\n        # mostra a média por um ponto\n        stat_summary(geom = \"point\", fun.y = mean) + \n        # adiciona uma linha na média geral\n        geom_hline(yintercept = mean_rg, linetype = \"dashed\")+ \n        labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\")\n\nplot_grid(box1, box2, labels = c(\"b1\", \"b2\"))\n\nh1 <- ggplot(dados_gg, aes(x = RG)) +\n      geom_histogram()\n\nh2 <- ggplot(dados_gg, aes(x = RG)) +\n      geom_histogram(binwidth = 200, colour = \"black\", \n                     aes(y = ..density.., fill = ..count..)) +\n      geom_density() +\n      stat_function(fun = dnorm,\n                    color = \"red\",\n                    size = 1,\n                    args = list(mean = mean(dados_gg$RG),\n                                sd = sd(dados_gg$RG))) +\n      labs(x = \"rendimento de grãos\", y = \"Densidade\")\n\nplot_grid(h1, h2, rel_widths = c(1, 1.4), labels = c(\"h1\", \"h2\"))\n\nbar1 <- ggplot(dados_gg, aes(x = GEN, y = RG)) +\n        geom_bar(stat = \"summary\",\n                fun = mean,\n                position = \"dodge\")\n\nbar2 <- ggplot(dados_gg, aes(x = GEN, y = RG, fill = AMB)) +\n        stat_summary(fun = mean,\n                     geom = \"bar\",\n                     col = \"black\",\n                     width = 0.8,\n                     position = position_dodge()) + \n        stat_summary(fun.data = mean_se,\n                     geom = \"errorbar\",\n                     width = 0.2,\n                     position = position_dodge(0.8))\n \nplot_grid(bar1, bar2, rel_widths = c(0.8, 1.2), labels = c(\"bar1\", \"bar2\"))\n#### Polinômio de segundo grau\ndado_reg = tibble(dose = c(15,20,25,30,35,40),\n                  prod = c(65,70,73,75,69,62))\nl1 <- ggplot(dado_reg, aes(dose, prod))+\n      geom_point()+\n      stat_smooth(method = \"lm\",\n                  formula = \"y ~ poly(x, 1)\",\n                  se = FALSE)\nl2 <- ggplot(dado_reg, aes(dose, prod))+\n      geom_point()+\n      stat_smooth(method = \"lm\",\n                  formula = \"y ~ poly(x, 2)\",\n                  linetype = \"dashed\",\n                  col = \"black\",\n                  level = 0.95)\n\nplot_grid(l1, l2, labels = c(\"l1\", \"l2\"))\n# simulando dados com diferentes assimetrias\nassimetria <- tibble(esquerda = rbeta(5000,1,7),\n                     direita = rbeta(5000,7,1),\n                     normal = rnorm(5000,5,3))\n\nassimetria_graf <- pivot_longer(assimetria, everything()) # organiza os dados para usar facete_wrap\nggplot(assimetria_graf, aes(sample = value))+\n       facet_wrap(~name, scales = \"free\")+\n       qqplotr::stat_qq_band(fill = \"gray\")+\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\nlabs(x = \"Theoretical quantiles\", y = \"Sample quantiles\")"},{"path":"exporta.html","id":"exporta","chapter":"Capítulo 9 Exportando dados","heading":"Capítulo 9 Exportando dados","text":"Será demonstrado nessa seção como exportar dados e tabelas gerados dentro software R. Quanto saída de resultados, será dado enfase saídas com extensões .csv, .txt e .xlsx. Quanto aos gráficos, será mostrado como salvar figuras em alta resolução.","code":""},{"path":"exporta.html","id":"exportando-com-diferentes-extensões","chapter":"Capítulo 9 Exportando dados","heading":"9.1 Exportando com diferentes extensões","text":"função export() d pacote rio (https://cran.r-project.org/web/packages/rio/index.html) pode ser utilizada para exportar objetos R em arquivos dos mais diversos formatos.Em arquivos .csv, os valores são separados por vírgula. exemplo abaixo, é mostrado como o objeto quantitativo pode ser salvo em um arquivo .csv com nome quanti_exemplo. Para salvar saídas em extensão .txt ou .xlsx, basta substituir extensão arquivo.formato .xlsx é possível informar em qual planilha o objeto será salvo. Neste caso, planilhas existentes não serão modificadas.Também é possível salvar diferentes objetos em diferentes planilhas mesmo arquivo. Vamos considerar que cada nível fator TIPO objeto quanti deve ser salvo em uma planilha diferente.Neste caso, um arquivo chamado quanti_exemplo_plan.xlsx contendo planilhas linear, quadratico e cubico foi criado diretório padrão.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nquanti <- import(url, sheet = \"QUANTI\")\nhead(quanti)\n# exportar para o diretóri padrão (csv)\nexport(quanti, file = \"quanti_exemplo.csv\")\n# exportar para o diretóri padrão (txt)\nexport(quanti, file = \"quanti_exemplo.txt\")\n# exportar para o diretóri padrão (xslx)\nexport(quanti, file = \"quanti_exemplo.xlsx\")\nexport(quanti, file = \"quanti_exemplo.xlsx\", which = \"quanti2\")\nlinear <- subset(quanti, TIPO == \"LINEAR\")\nquadratico <- subset(quanti, TIPO == \"QUADRÁTICA\")\ncubico <- subset(quanti, TIPO == \"CÚBICA\")\nexport(list(linear = linear,\n            quadratico = quadratico, \n            cubico = cubico),\n       file = \"quanti_exemplo_plan.xlsx\")"},{"path":"exporta.html","id":"exportanto-gráficos","chapter":"Capítulo 9 Exportando dados","heading":"9.2 Exportanto gráficos","text":"Os gráficos podem ser exportados clicando em Export  output dos gráficos. Você pode escolher entre salvar como imagem ou como PDF. Os formatos de imagem disponíveis são: .PNG, .TIFF, .JPEG, .BMP , .SVG  e .ESP . outra opção é salvar em um arquivo PDF . principal diferença entre estes formatos é o método de renderização utilizado na formação gráfico. Existem basicamente dois métodos de renderização de imagens: raster images e vector-based images. Imagens rasterizadas usam muitos pixels coloridos ou blocos de construção individuais para formar uma imagem completa. JPEGs, GIFs e PNGs e TIFFs são tipos de imagem raster mais comuns. Como imagens raster são criadas usando um número fixo de pixels coloridos, elas não podem ser redimensionadas drasticamente sem comprometer sua resolução. Quando esticados para caber em um espaço que eles não foram projetados para preencher, seus pixels ficam visivelmente granulados e imagem é distorcida. É importante que você salve os arquivos raster precisamente nas dimensões necessárias e com devida resolução para uma boa apresentação. Para salvar estas imagens é necessário informar Density Pixels per Inch (DPI) , ou seja, quantos pontos por polegada quadrada deverá conter imagem. Quanto maior este valor, maior será qualidade da imagem e também maior será seu tamanho (em Mb).Imagens vetoriais (vector-based graphics), por outro lado, permitem mais flexibilidade. Construídos usando fórmulas matemáticas em vez de blocos coloridos individuais, os tipos de arquivos vetoriais, como .PDF e .EPS *, são excelentes para criar gráficos de alta resolução sem que seu redimensionamento prejudique qualidade gráfico. maioria das revistas cintíficas aceitam todos estes tipos de formatos. Na dúvida, escolha sempre o formato .PDF (ou .EPS)!Salvando figuras como imagensSalvando figuras em PDFUma alternativa prática para salvar em pdf imagens é através da função pdf(). Os argumentos width e height correspondem dimensão da figura (em polegadas) e pointsize corresponde ao tamanho da fonte. função ggsave()  também pode ser utilizada para salvar os gráficos para o diretório de trabalho. Por padrão, esta função salva o último gráfico mostrado. Para salvar-mos o gráfico p1, gerado anteriormente, basta executar o seguinte comando.Exportando figuras como imagens em alta qualidadeCom funções tiff() , png() , jpeg()  e bmp()  é possível salvar gráficos como imagem em alta resolução. Como na função pdf() , width e height correspondem dimensão. Porém, nestas funções é possível escolher unidade através argumento units e resolução através argumento res.","code":"\npdf(\"Figura1.pdf\", width = 5, height = 4)\n\np1 <- ggplot(dados_oat, aes(x = RG, y = PH, colour = AMB)) +\n      geom_point()+\n      geom_smooth(method = \"lm\", se = F)+\n      my_theme()+\n      labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\")\np1\n\ndev.off()\nggsave(\"Figure_ggsave.pdf\", p1)\ntiff(filename = \"Figura3.tiff\", width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\npng(filename = \"Figura3.png\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\njpeg(filename = \"Figura3.jpeg\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()\n\nbmp(filename = \"Figura3.bmp\",width = 10, height = 8, \n     units = \"cm\",pointsize = 12, \"lzw\",res = 1200)\np1\ndev.off()"},{"path":"analdata.html","id":"analdata","chapter":"Capítulo 10 Análise de dados experimentais","heading":"Capítulo 10 Análise de dados experimentais","text":"“Muito melhor uma resposta aproximada à pergunta certa, que muitas vezes é vaga, que uma resposta exata à pergunta errada, que sempre pode ser feita com precisão.” — John TukeyNesta seção será abordado aspectos relacionados análise de experimentos agrícolas, com ênfase na utilização de testes paramétricos. Esta seção será dividida em três partes principais:Parte 1: estatistica básica: Medidas de tendência central e de variabilidade. Intervalos de confiança para média. Testes de hipóteses para verificar igualdade entre médias de uma ou duas amostras.Parte 1: estatistica básica: Medidas de tendência central e de variabilidade. Intervalos de confiança para média. Testes de hipóteses para verificar igualdade entre médias de uma ou duas amostras.Parte 2: delineamentos básicos: delineamentos experimentais inteiramente casualisado (DIC) e blocos ao acaso (DBC). Pressupostos  dos modelos estatisticos. Testes complementares (média e regressão).Parte 2: delineamentos básicos: delineamentos experimentais inteiramente casualisado (DIC) e blocos ao acaso (DBC). Pressupostos  dos modelos estatisticos. Testes complementares (média e regressão).Parte 3: análise de covariância: Análise de covariância como uma ferramenta estatística para redução erro experimental.Parte 3: análise de covariância: Análise de covariância como uma ferramenta estatística para redução erro experimental.Parte 4: modelos lineares generalizados: Modelos Lineares Generalizados aplicados análise de dados não gaussianos.Parte 4: modelos lineares generalizados: Modelos Lineares Generalizados aplicados análise de dados não gaussianos.Parte 5: experimentos fatoriais: experimentos fatorias e experimentos com parcelas subdivididas.Parte 5: experimentos fatoriais: experimentos fatorias e experimentos com parcelas subdivididas.","code":""},{"path":"analdata.html","id":"ebasic","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1 Estatistica básica","text":"","code":""},{"path":"analdata.html","id":"medidas-de-tendência-central","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.1 Medidas de tendência central","text":"Nesta seção mostraremos como calular medidas de tendência central e medidas de variabilidade. medidas de tendencia central são valores que representam um conjunto de dados. Entre mais comuns podemos citar média, mediana e moda.O R calcula média  e mediana  através das funções mean()  e median() , porém não calcula moda. blog Ridículas, mantido pelo LEG da UFPR, dois métodos são dicutidos. Incentivamos leitura material.","code":"\nset.seed(1)\nAmostra1 <- rnorm(100, 12, 3) # Gera uma amostra com distribuição normal\nAmostra2 <- rpois(100, 12) # Gera uma amostra com distribuição Poisson\nmean(Amostra1) # média\n# [1] 12.32666\nmedian(Amostra1) # mediana\n# [1] 12.34173"},{"path":"analdata.html","id":"medidas-de-variabilidade","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.2 Medidas de variabilidade","text":"seguintes medidas de variabilidade podem ser computadas, com suas respectivas funções:Desvio padrão sd()Variância var()Amplitude total range()Amplitude interquartílica IQR()O desvio padrão e variância podem ser obtidas com funções sd()  e var(), respectivamente. Não existe R base uma função para computar o coeficiente de variação, então vamos criá-la utilizando abordagem function():  distribuição dos dados pode ser determinada utilizando histograma de frequências, QQ-Plos e Box-Plot (conforme visto anteriormente). Estatísticas como amplitude, erro padrão da média, intervalo de confiança, entre outros, podem ser obtidas com função desc_stat()  pacote metan29. Esta função permite computar estatisticas para uma ou mais variáveis de um data frame ou um vetor de dados numéricos. Para um exemplo numérico, consulte seção 7.3.1.","code":"\nsd(Amostra1)\n# [1] 2.694598\nvar(Amostra1)\n# [1] 7.260859\nrange(Amostra1)\n# [1]  5.35590 19.20485\nIQR(Amostra1)\n# [1] 3.557364\nCV <- function(dados){\n  if(!class(dados) == \"numeric\"){\n    stop(\"Os dados precisam ser numéricos\")\n    } #Indica que os dados devem ser numéricos\n  media <- mean(dados)\n  sd <- sd(dados)\n  CV <- (sd/media) * 100\n  return(CV) # Valor que será retornado pela função\n}\n\nCV(Amostra1)\n# [1] 21.85992"},{"path":"analdata.html","id":"resumindo-dados-com-o-pacote-dplyr","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.3 Resumindo dados com o pacote dplyr","text":"Diversos verbos pacote dplyr podem ser utilizados para resumir conjuntos de dados. Iniciaremos com função count() para contar valores que se repetem em uma determinada variável. Por exemplo, é possível identificar qual é o valor de APLA que mais se repete utilizandoPara identificar quais os valores distintos de APLA foram observados função distinct() é usada.Utilizando função summarise() é possível criar uma ou mais variáveis escalares resumindo variáveis de um tibble existente. Como resultado, uma linha é retornada. O seguinte código calcula média global da MGRA e retorna o n utilizado na estimativa.Muitas vezes é necessário computar uma determinada função (como média) para cada nível de uma variável categórica. Felizmente, o pacote dplyr possibilita que isto seja realizado facilmente. Continuamos mesmo exemplo anterior. Neste caso, entanto, o objetivo é calcular média da MGRA para cada híbrido. Utilizando função group_by()  antes da função summarise()  uma linha de resultado para cada nível fator híbrido é retornado.Até aqui vimos como média (global ou para cada híbrido) da MGRA pode ser calculada. Quase sempre, entanto, quando calculamos média (ou qualquer outra medida) em um conjunto de dados, queremos fazê-la para todas (ou algumas) variáveis numéricas dos dados. Implementar isto com dplyr é relativamente fácil. Para isto, é utilizada função across() que aplica uma função (ou um conjunto de funções) um conjunto de colunas.Veremos como across() pode ser utilizada para calcular média para variáveis numéricas conjunto maize. exemplo abaixo, () aplica uma função (neste caso .numeric()) todas variáveis e seleciona aquelas para quais função retorna TRUE. Assim, média somente é calculada para variáveis numéricas.Funções próprias podem ser aplicadas dentro da função summarise() para computar uma estatística personalizada. Como exemplo, vamos criar uma função chamada mse que retornará o valor da média e o erro padrão da média e aplicá-la todas variáveis que iniciam \"M\", para cada nível fator AMB.Se desejamos computar mais de uma função para variáveis específicas, então o próximo código nos ajudará. Note que para aplicar mais de uma função é necessário criar uma lista ou vetor com o nome das funções. Neste caso, os sufixos _m e _sd representam média e o desvio padrão, respectivamente.Variáveis categóricas podem ser criadas utilizando função case_when() . case_when() é particularmente útil dentro da função mutate() quando você quer criar uma nova variável que depende de uma combinação complexa de variáveis existentes. exemplo abaixo, uma nova variável será criada, dependendo dos valores de APLA, AIES ou CESP","code":"\n\ncount(maize, APLA, sort = TRUE)\n# # A tibble: 143 x 2\n#     APLA     n\n#    <dbl> <int>\n#  1  2.6     20\n#  2  2.8     20\n#  3  2.5     16\n#  4  2       14\n#  5  2.92    14\n#  6  2.1     13\n#  7  2.3     12\n#  8  2.7     12\n#  9  1.92    11\n# 10  2.04    11\n# # ... with 133 more rows\ndistinct(maize, APLA)\n# # A tibble: 143 x 1\n#     APLA\n#    <dbl>\n#  1  2.45\n#  2  2.5 \n#  3  2.69\n#  4  2.8 \n#  5  2.62\n#  6  2.12\n#  7  3.15\n#  8  2.97\n#  9  3.1 \n# 10  3.02\n# # ... with 133 more rows\nmaize %>% \n  summarise(MGRA_mean = mean(MGRA),\n            n = n())\n# # A tibble: 1 x 2\n#   MGRA_mean     n\n#       <dbl> <int>\n# 1      173.   780\nmaize %>% \n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA),\n            n = n())\n# # A tibble: 13 x 3\n#    HIB   MGRA_mean     n\n#  * <chr>     <dbl> <int>\n#  1 H1         184.    60\n#  2 H10        164.    60\n#  3 H11        167.    60\n#  4 H12        157.    60\n#  5 H13        180.    60\n#  6 H2         187.    60\n#  7 H3         169.    60\n#  8 H4         184.    60\n#  9 H5         184.    60\n# 10 H6         188.    60\n# 11 H7         171.    60\n# 12 H8         160.    60\n# 13 H9         153.    60\nmaize %>% \n  summarise(across(where(is.numeric), mean))\n# # A tibble: 1 x 7\n#    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n# 1  2.48  1.34  15.2  49.5  173.  339.  512.\nmse <- function(x){\n  me = round(mean(x), 3)\n  se = round(sd(x)/sqrt(n()), 3)\n  return(paste0(me, \"+-\", se))\n}\nmaize %>% \n  group_by(AMB) %>%\n  summarise(across(starts_with(\"M\"), mse))\n# # A tibble: 4 x 3\n#   AMB   MGRA           MMG           \n# * <chr> <chr>          <chr>         \n# 1 A1    199.437+-3.232 360.337+-4.164\n# 2 A2    168.436+-3.381 333.815+-4.86 \n# 3 A3    146.811+-2.787 317.718+-4.421\n# 4 A4    177.072+-3.118 342.796+-4.195\nmaize %>%\n  group_by(AMB) %>%\n  summarise(across(starts_with(\"M\"), list(m = mean, sd = sd)))\n# # A tibble: 4 x 5\n#   AMB   MGRA_m MGRA_sd MMG_m MMG_sd\n# * <chr>  <dbl>   <dbl> <dbl>  <dbl>\n# 1 A1      199.    45.1  360.   58.1\n# 2 A2      168.    47.2  334.   67.9\n# 3 A3      147.    38.9  318.   61.7\n# 4 A4      177.    43.5  343.   58.6\n\nmaize %>% \n  mutate(\n    CASE = case_when(\n      MGRA > 280 | APLA < 1.3 | NGRA > 820 ~  \"Selecionar\",\n      APLA > 2.3 ~ \"Alto\",\n      MGRA < 130 ~ \"Pouco produtivo\",\n      TRUE ~ \"Outro\"\n    )\n  ) %>% \n  select_non_numeric_cols()\n# # A tibble: 780 x 4\n#    AMB   HIB   REP   CASE \n#    <chr> <chr> <chr> <chr>\n#  1 A1    H1    I     Alto \n#  2 A1    H1    I     Alto \n#  3 A1    H1    I     Alto \n#  4 A1    H1    I     Alto \n#  5 A1    H1    I     Alto \n#  6 A1    H1    II    Outro\n#  7 A1    H1    II    Alto \n#  8 A1    H1    II    Alto \n#  9 A1    H1    II    Alto \n# 10 A1    H1    II    Alto \n# # ... with 770 more rows"},{"path":"analdata.html","id":"dstat","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.4 Estatística descritiva com pacote metan","text":"O pacote metan fornece uma estrutura simples e intuitiva para o cálculo de estatísticas descritivas. Um conjunto de funções pode ser usado para calcular rapidamente estatísticas descritivas mais usadas.Para calcular os valores médios para cada nível de um fator, por exemplo para cada nível fator HIB conjunto de dados maize, usamos funçãomeans_by().seguintes funções _by() estão disponíveis para calcular principais estatísticas descritivas por níveis de um fator.cv_by () Para cálculo coeficiente de variação.max_by () Para calcular valores máximos.means_by () Para calcular meios aritméticos.min_by () Para compilar valores mínimos.n_by () Para obter o comprimento.sd_by () Para calcular o desvio padrão amostral.sem_by () Para calcular o erro padrão da média.","code":"\nmeans_by(maize, HIB)\n# # A tibble: 13 x 8\n#    HIB    APLA  AIES  CESP  DIES  MGRA   MMG  NGRA\n#  * <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     2.62  1.50  15.1  51.2  184.  365.  507.\n#  2 H10    2.31  1.26  15.1  48.4  164.  320.  504.\n#  3 H11    2.39  1.27  15.2  48.8  167.  333.  501.\n#  4 H12    2.44  1.28  14.3  48.6  157.  316.  502.\n#  5 H13    2.54  1.35  15.0  50.6  180.  340.  538.\n#  6 H2     2.60  1.38  15.3  50.9  187.  356.  527.\n#  7 H3     2.59  1.41  14.5  49.4  169.  346.  491.\n#  8 H4     2.58  1.43  15.7  49.2  184.  346.  535.\n#  9 H5     2.57  1.37  15.6  49.9  184.  341.  542.\n# 10 H6     2.56  1.41  15.8  51.5  188.  363.  516.\n# 11 H7     2.40  1.32  15.4  49.5  171.  345.  498.\n# 12 H8     2.33  1.21  15.0  48.4  160.  322.  496.\n# 13 H9     2.36  1.27  15.0  47.6  153.  311.  496."},{"path":"analdata.html","id":"funções-úteis","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.4.1 Funções úteis","text":"Outras funções úteis também são implementadas. Todos eles funcionam naturalmente com %>%, lidam com dados agrupados com group_by() e várias variáveis (todas variáveis numéricas de .data por padrão).av_dev () calcula o desvio médio absoluto.ci_mean () calcula o intervalo de confiança para média.cv () calcula o coeficiente de variação.freq_table () Calcula fábula de frequência.hm_mean (), gm_mean () calcula médias harmônica e geométrica, respectivamente. média harmônica é o recíproco da média aritmética dos recíprocos. média geométrica é enésima raiz de n produtos.kurt () calcula curtose como usada SAS e SPSS.range_data () Calcula o intervalo dos valores.sd_amo (), sd_pop () Calcula amostra e desvio padrão populacional, respectivamente.sem () calcula o erro padrão da média.skew () calcula assimetria usada SAS e SPSS.sum_dev () calcula soma dos desvios absolutos.sum_sq_dev () calcula soma dos desvios ao quadrado.var_amo (), var_pop () calcula amostra e variação populacional.valid_n () Retorna o comprimento válido (não NA) de um dado.","code":""},{"path":"analdata.html","id":"a-função-desc_stat","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.4.2 A função desc_stat()","text":"Para calcular todas estatísticas de uma só vez, podemos usar desc_stat(). Esta função pode ser usada para calcular medidas de tendência central, posição e dispersão. Por padrão (stats = \"main\"), sete estatísticas (coeficiente de variação, máximo, média, mediana, mínimo, desvio padrão da amostra, erro padrão e intervalo de confiança da média) são calculadas. Outros valores permitidos são \"\" para mostrar todas estatísticas, \"robust\" para mostrar estatísticas robustas, \"quantile\" para mostrar estatísticas quantílicas ou escolher uma (ou mais) estatísticas usando um vetor separado por vírgula com os nomes das estatísticas, por exemplo, stats = c(\"mean, cv\"). Também podemos usar hist = TRUE para criar um histograma para cada variável. Aqui, auxiliares selecionados também podem ser usados argumento ....Todas estatísticas para todas variáveis numéricasEstatísticas robustas usando select helpersFunções quantílicas escolhendo nomes de variáveisCrie um histograma para cada variável","code":"\ndesc_stat(maize, stats = \"all\")\n# # A tibble: 7 x 32\n#   variable av.dev     ci    cv  gmean  hmean     iqr    kurt     mad    max\n#   <chr>     <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n# 1 AIES      0.268 0.0221 23.4    1.30   1.26   0.502 -0.729    0.363   2.39\n# 2 APLA      0.322 0.0264 15.1    2.45   2.42   0.64  -0.292    0.474   3.3 \n# 3 CESP      1.65  0.152  14.3   15.0   14.5    2.6    2.83     1.93   20.4 \n# 4 DIES      3.02  0.260   7.46  49.4   49.3    5.29  -0.285    3.82   59.7 \n# 5 MGRA     39.0   3.35   27.5  166.   157.    69.0   -0.56    51.0   291.  \n# 6 MMG      51.5   4.46   18.7  332.   325.    87.8   -0.0484  66.2   546.  \n# 7 NGRA     88.4   7.98   22.2  498.   482.   148.     0.321  110.    903   \n# # ... with 22 more variables: mean <dbl>, median <dbl>, min <dbl>, n <dbl>,\n# #   n.valid <dbl>, n.missing <dbl>, n.unique <dbl>, ps <dbl>, q2.5 <dbl>,\n# #   q25 <dbl>, q75 <dbl>, q97.5 <dbl>, range <dbl>, sd.amo <dbl>, sd.pop <dbl>,\n# #   se <dbl>, skew <dbl>, sum <dbl>, sum.dev <dbl>, sum.sq.dev <dbl>,\n# #   var.amo <dbl>, var.pop <dbl>maize %>%\r\n  desc_stat(contains(\"N\"),\r\n            stats = \"robust\")\r\n# # A tibble: 1 x 5\r\n#   variable     n median   iqr    ps\r\n#   <chr>    <dbl>  <dbl> <dbl> <dbl>\r\n# 1 NGRA       780    517  148.  110.maize %>%\r\n  desc_stat(APLA, AIES, CESP,\r\n            stats = \"quantile\")\r\n# # A tibble: 3 x 7\r\n#   variable     n   min   q25 median   q75   max\r\n#   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl>\r\n# 1 AIES       780   0.5  1.09   1.38  1.59  2.39\r\n# 2 APLA       780   1    2.16   2.52  2.8   3.3 \r\n# 3 CESP       780   0.8 14     15.4  16.6  20.4maize %>%\r\n  desc_stat(APLA, AIES, CESP,\r\n            hist = TRUE)# # A tibble: 3 x 9\r\n#   variable    cv   max  mean median   min sd.amo     se     ci\r\n#   <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>\r\n# 1 AIES      23.4  2.39  1.34   1.38   0.5  0.314 0.0113 0.0221\r\n# 2 APLA      15.1  3.3   2.48   2.52   1    0.375 0.0134 0.0264\r\n# 3 CESP      14.3 20.4  15.2   15.4    0.8  2.16  0.0774 0.152"},{"path":"analdata.html","id":"estatísticas-por-níveis-de-fatores","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.4.3 Estatísticas por níveis de fatores","text":"Para calcular estatísticas para cada nível de um fator, use o argumento . Além disso, é possível selecionar estatísticas serem computadas usando o argumento stats, que é um único nome estatístico, por exemplo,\"mean\"ou um vetor de nomes separados por vírgula com \" início e apenas o final vetor. Tenha em atenção que os nomes das estatísticas NÃO diferenciam maiúsculas de minúsculas, por exemplo, são reconhecidos \"mean\", \"Mean\" ou \"MEAN\". Vírgula ou espaços podem ser usados para separar os nomes das estatísticas.Todas opções abaixo funcionarão:\r\nstats = c (\"mean, se, cv, max, min\")\r\nstats = c (\"mean se cv max min\")\r\nstats = c (\"MEAN, Se, CV max MIN\")\r\nTodas opções abaixo funcionarão:stats = c (\"mean, se, cv, max, min\")stats = c (\"mean se cv max min\")stats = c (\"MEAN, Se, CV max MIN\")Para calcular estatísticas descritivas por mais de uma variável de agrupamento, precisamos passar dados agrupados para o argumento .data com funçãogroup_by(). Vamos calcular média, o erro padrão da média e o tamanho da amostra para variáveis EP eEL para todas combinações dos fatores AMB eHIB.Quando estatísticas são calculadas para níveis de um fator, função desc_wider() pode ser utilizada para converter uma estatística calculada em um conjunto de dados em formato wide, ou seja, variáveis nas colunas, fatores nas linhas com o valor da estatística escolhida preenchendo tabela.","code":"desc_stat (maize,\r\n          contains(\"C\"),\r\n          stats = (\"mean, se, cv, max, min\"),\r\n          by = AMB)\r\n# # A tibble: 4 x 7\r\n#   AMB   variable  mean    se    cv   max   min\r\n#   <chr> <chr>    <dbl> <dbl> <dbl> <dbl> <dbl>\r\n# 1 A1    CESP      15.6 0.171  15.3  20.3   0.8\r\n# 2 A2    CESP      15.2 0.148  13.6  19.2   8.7\r\n# 3 A3    CESP      14.7 0.145  13.8  19.9   7.5\r\n# 4 A4    CESP      15.1 0.148  13.7  20.4   8.2\nstats <- \nmaize %>% \n  group_by(AMB, HIB) %>% \n  desc_stat(APLA, AIES, CESP,\n            stats = c(\"mean, se, n\"))\nstats\n# # A tibble: 156 x 6\n#    AMB   HIB   variable  mean     se     n\n#    <chr> <chr> <chr>    <dbl>  <dbl> <dbl>\n#  1 A1    H1    AIES      1.68 0.0599    15\n#  2 A1    H1    APLA      2.72 0.0695    15\n#  3 A1    H1    CESP     15.4  0.451     15\n#  4 A1    H10   AIES      1.62 0.0506    15\n#  5 A1    H10   APLA      2.78 0.0716    15\n#  6 A1    H10   CESP     16.1  0.662     15\n#  7 A1    H11   AIES      1.58 0.0302    15\n#  8 A1    H11   APLA      2.75 0.0236    15\n#  9 A1    H11   CESP     16.6  0.332     15\n# 10 A1    H12   AIES      1.54 0.0456    15\n# # ... with 146 more rows\ndesc_wider(stats, se)\n# # A tibble: 52 x 5\n#    AMB   HIB     AIES   APLA  CESP\n#    <chr> <chr>  <dbl>  <dbl> <dbl>\n#  1 A1    H1    0.0599 0.0695 0.451\n#  2 A1    H10   0.0506 0.0716 0.662\n#  3 A1    H11   0.0302 0.0236 0.332\n#  4 A1    H12   0.0456 0.0582 0.498\n#  5 A1    H13   0.0562 0.05   0.295\n#  6 A1    H2    0.0327 0.0507 0.637\n#  7 A1    H3    0.0383 0.0323 0.543\n#  8 A1    H4    0.0448 0.043  0.437\n#  9 A1    H5    0.0315 0.0442 0.618\n# 10 A1    H6    0.0447 0.0704 0.347\n# # ... with 42 more rows"},{"path":"analdata.html","id":"testes-de-aderência","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.5 Testes de aderência","text":"Testes de aderência distribuições teóricas também são de grande utilizada para ciências agrárias. O teste de Shapiro-Wilk, realizado pela função shapiro.test() , é amplamente utilizada para realizar o teste de normalidade dos dados. Para testar aderência outras distribuições teóricas, o teste de Kolmolgorov-Smirnov (função ks.test())  é uma alternativa.","code":"\n# Teste de Shapiro-Wilk\nshapiro.test(Amostra1)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra1\n# W = 0.9956, p-value = 0.9876\nshapiro.test(Amostra2)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra2\n# W = 0.95815, p-value = 0.002976\n\n# Kolmogorov–Smirnov\n# Amostra1 e Amostra2 provém da mesma distribuição?\nks.test(Amostra1, Amostra2)\n# \n#   Two-sample Kolmogorov-Smirnov test\n# \n# data:  Amostra1 and Amostra2\n# D = 0.29, p-value = 0.0004453\n# alternative hypothesis: two-sided\n\n# Amostra1 ~ N(12, 3)?\nks.test(Amostra1, \"pnorm\", 12, 3)\n# \n#   One-sample Kolmogorov-Smirnov test\n# \n# data:  Amostra1\n# D = 0.094659, p-value = 0.3317\n# alternative hypothesis: two-sided"},{"path":"analdata.html","id":"intervalos-de-confiança","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.6 Intervalos de confiança","text":"estimação por intervalo não fornece idéia da margem de erro cometida ao estimar um determinado parâmetro (Ferreira 2009). Por isso, para verificar se uma dada hipótese \\(H_0\\) (de igualdade) é ou não verdadeira, deve-se utilizar intervalos de confiança ou testes de hipóteses. construção destes intervalos, e particularidades dos testes de hipóteses, serão discutidos seguir. Recomendamos como literatura o livro Estatística Básica30 escrito pelo Prof. Daniel Furtado Ferreira da UFV. Para verificar normalidade dos dados, funções shapiro.test() e ks.test() e os gráficos QQ-Plot são de grande utilidade.  Será demostrado como testar hióteses para uma e duas médias pelo teste t de Student, o que exiege que os dados tenham distribuição normal univariada (já discutido anteriormente) ou bivariada (dados emparelhados). Para testar normalidade bivariada, basta testar normalidade da diferença entre variáveis: partir de um intervalo  que tenha alta probabilidade de conter o valor paramétrico, é possível diferenciar duas estimativas (Ferreira 2009). O intervalo de confiança de uma média amostral de 95% é dado por:\\[\r\nP\\left[ {\\bar X - {t_{\\alpha /2}}\\frac{S}{{\\sqrt n }} \\le \\mu  \\le \\bar X + {t_{\\alpha /2}}\\frac{S}{{\\sqrt n }}} \\right] = 1 - \\alpha \r\n\\]Na expressão acima, \\(\\bar X\\) é média, \\(S\\) é o desvio padrão e \\(-t_{\\alpha /2}\\) e \\(+t_{\\alpha /2}\\) são os quantis inferior e superior, respectivamente, da distribuição t de Student. O intervalo acima indica que o valor parâmetro (\\(\\mu\\)) tem 95% de chance de estar contido intervalo. Ressalta-se que expressão acima está relacionada com precisão e não com acurácia da estimativa. Para calcular esse intervalo, podemos utilizar função t.teste() .O intervalo de confiança é, por default, de 95%. Poém, pode-se modificar através argumento conf.level. Para gerar os gráficos de intervalo de confiança será utilizada função ggplot() pacote ggplot2. \r\nFigure 10.1: Gráficos de intervalo de confiança (mais de uma variável)\r\n","code":"\nAmostra3 <- Amostra1 - Amostra2\nshapiro.test(Amostra3)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  Amostra3\n# W = 0.97923, p-value = 0.1158\nresult <- t.test(Amostra1)\nresult$conf.int # Intervalo de confiança\n# [1] 11.79200 12.86133\n# attr(,\"conf.level\")\n# [1] 0.95\nresult$estimate # média\n# mean of x \n#  12.32666\nresult <- t.test(Amostra1, conf.level = 0.99)\nresult1 <- t.test(Amostra1, conf.level = 0.90)\nresult$conf.int # Intervalo de confiança\n# [1] 11.61895 13.03437\n# attr(,\"conf.level\")\n# [1] 0.99\nresult1$conf.int # Intervalo de confiança\n# [1] 11.87925 12.77407\n# attr(,\"conf.level\")\n# [1] 0.9\nset.seed(100) #Ajusta a semente para reprodução dos números\nAmostra1 <- rnorm(100,10,10)\nAmostra2 <- rnorm(100,10,24)\nAmostra3 <- rnorm(100,25,15)\nAmostra4 <- rnorm(100,20,30)\n\ndados_IC <- tibble(\n    Factor = c(\"Amostra 1\",\"Amostra 2\", \"Amostra 3\", \"Amostra 4\"),\n    LL = c(t.test(Amostra1)$conf.int[1],\n           t.test(Amostra2)$conf.int[1],\n           t.test(Amostra3)$conf.int[1],\n           t.test(Amostra4)$conf.int[1]),\n    Mean = c(t.test(Amostra1)$estimate,\n             t.test(Amostra2)$estimate,\n             t.test(Amostra3)$estimate,\n             t.test(Amostra4)$estimate),\n    UL = c(t.test(Amostra1)$conf.int[2],\n           t.test(Amostra2)$conf.int[2],\n           t.test(Amostra3)$conf.int[2],\n           t.test(Amostra4)$conf.int[2]))\n\n# Gráfico com barras de erros\ncbPalette = c(\"red\", \"blue\", \"green\", \"pink\") # armazenando as cores\nerr1 = ggplot(data = dados_IC, aes(y = Mean, x = Factor, colour = Factor)) + \n              geom_point(size = 2.5)+ # adiciona um ponto ao gráfico\n              geom_errorbar(aes(ymax = UL, ymin = LL), width = 0.1)+\n              scale_color_manual(values = cbPalette)+ \n              coord_flip()+ # \"inverte\" o \"x\" e o \"y\" \n              expand_limits(y = c(0.4,0.6))+\n              theme(legend.position = \"none\")\n\n# Gráfico de barras com barras de erros \nerr2 = ggplot(data = dados_IC, aes(y = Mean, x = Factor)) + \n              geom_bar(aes(fill = Factor), stat = \"identity\", position = \"dodge\")+\n              geom_errorbar(aes( ymax = UL, ymin = LL), width = 0.2)+\n              expand_limits(y = c(0,30)) +\n              scale_fill_manual(values = cbPalette)+\n              theme(legend.position = \"none\")\n\nplot_grid(err1, err2)"},{"path":"analdata.html","id":"teste-de-hipóteses-para-amostras-independentes","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.7 Teste de hipóteses para amostras independentes","text":"Os testes de hipóteses  aqui demonstrados tem como objetivo () verificar se determianda amostra difrere ou não de zero (\\({H_0}:\\mu = 0\\)) e (ii) se duas amostras são ou não iguais (\\({H_0}:{\\mu _1} = {\\mu _2}\\)). Para testar hipóteses pode-se utilizar função t.teste(). Utilizaremos como amostras os dados da variável MGRA conjunto maize. primeira amostra corresponde contém os valores de A1 e segunda os valores de A2Alternativamente, o pacote ggstatplot31 pode ser utilizado para confecionar gráficos que incluem teste de hipóteses.O teste t pode ser utilizado para testar tanto hipóteses tipo \\({H_A}:\\mu > 0\\) ou \\({H_A}:\\mu < 0\\). Porém, para isso, devemos utilizar o argumento alternative para indicar que o teste utilizado é unilateral.Outro pressuposto para realizar o teste t é homogeneidade das variâncias. Quando variâncias são heterogêneas, o grau de liberdade utilizado é calculado pela aproximação de Welch-Satterthwaite: \\[\r\n\\nu  \\cong \\frac{{{{\\left( {\\frac{{S_1^2}}{{{n_1}}} + \\frac{{S_2^2}}{{{n_2}}}} \\right)}^2}}}{{\\frac{{{{\\left( {\\frac{{S_1^2}}{{{n_1}}}} \\right)}^2}}}{{{n_1} - 1}} + \\frac{{{{\\left( {\\frac{{S_2^2}}{{{n_2}}}} \\right)}^2}}}{{{n_2} - 1}}}}\r\n\\]","code":"\nAmostra1 <- maize %>% filter(AMB == \"A1\") %>% select(MGRA) %>% pull()\nAmostra2 <- maize %>% filter(AMB == \"A2\") %>% select(MGRA) %>% pull()\n\nt.test(Amostra1) # testa se a amostra difere de zero\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value < 2.2e-16\n# alternative hypothesis: true mean is not equal to 0\n# 95 percent confidence interval:\n#  193.0630 205.8111\n# sample estimates:\n# mean of x \n#   199.437\nt.test(Amostra1, Amostra2) # testa se as amostras difrem entre si\n# \n#   Welch Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 387.21, p-value = 1.144e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80515 40.19763\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356\nt.test(Amostra1, alternative = \"greater\") # unilateral a direita\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value < 2.2e-16\n# alternative hypothesis: true mean is greater than 0\n# 95 percent confidence interval:\n#  194.0956      Inf\n# sample estimates:\n# mean of x \n#   199.437\nt.test(Amostra1, alternative = \"less\") # unilateral a esquerda\n# \n#   One Sample t-test\n# \n# data:  Amostra1\n# t = 61.71, df = 194, p-value = 1\n# alternative hypothesis: true mean is less than 0\n# 95 percent confidence interval:\n#      -Inf 204.7784\n# sample estimates:\n# mean of x \n#   199.437\nvar.test(Amostra1, Amostra2) # Teste F para variâncias\n# \n#   F test to compare two variances\n# \n# data:  Amostra1 and Amostra2\n# F = 0.91355, num df = 194, denom df = 194, p-value = 0.5295\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.688888 1.211488\n# sample estimates:\n# ratio of variances \n#          0.9135534\nt.test(Amostra1, Amostra2, var.equal = FALSE) # Por default, usa Welch-Satterthwaite\n# \n#   Welch Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 387.21, p-value = 1.144e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80515 40.19763\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356\n\nAmostra3 <- rnorm(30, 10, 5)\nAmostra4 <- rnorm(30, 18, 5)\nvar.test(Amostra3, Amostra4)\n# \n#   F test to compare two variances\n# \n# data:  Amostra3 and Amostra4\n# F = 0.65152, num df = 29, denom df = 29, p-value = 0.2545\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.3101021 1.3688474\n# sample estimates:\n# ratio of variances \n#          0.6515231\nt.test(Amostra1, Amostra2, var.equal = TRUE) # Quando variâncias são iguais\n# \n#   Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = 6.6279, df = 388, p-value = 1.142e-10\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  21.80520 40.19757\n# sample estimates:\n# mean of x mean of y \n#  199.4370  168.4356"},{"path":"analdata.html","id":"teste-de-hipóteses-para-amostras-dependentes","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.1.8 Teste de hipóteses para amostras dependentes","text":"formas de comparação discutidas acima consideram amostras como sendo independentes entre si. Para dados emparelhados, deve-se utiliza o argumento paired =  TRUE.Observa-se que existe uma diferença valor da estatistica teste, nos graus de liberdade, valor tabelado e, consequentemente, p-valor. Para duas amostras independentes, o teste para diferença é dado por\\[\r\n{t_c} = \\frac{{{{\\bar X}_1} - {{\\bar X}_2}}}{{S\\sqrt {\\frac{1}{{{n_1}}} + \\frac{1}{{{n_2}}}} }} \\sim {t_{\\left( {\\alpha ,\\nu } \\right)}}\r\n\\]Onde \\(\\alpha\\) é probabilidade de erro, \\(\\nu\\) é o grau de liberdade (nº total de obervações-2), \\(S\\) é média ponderada desvio padrão, \\({\\bar X}_1\\) e \\({\\bar X}_2\\) são média das amostras 1 e 2, respectivamente, e \\(n_1\\) e \\(n_1\\) e \\(n_2\\) são os tamanhos de amostra da amostra 1 e 2, respectivamente. resultado acima, obervamos que \\(\\nu = 58\\).caso de amostras pareadas (dependentes), estatística teste é dada por \\[\r\n{t_c} = \\frac{{\\bar d - {\\mu _0}}}{{\\frac{{{S_d}}}{{\\sqrt n }}}} \\sim {t_{\\left( {\\alpha ,\\nu } \\right)}} \r\n\\]Onde \\(\\alpha\\) é probabilidade de erro, \\(\\nu\\) é o grau de liberdade (nº de diferenças-1), \\(\\bar d\\) é média das diferenças, \\(S_d\\) é o desvio padrão das diferenças e \\(n\\) é o número de diferenças. resultado acima,obervamos que \\(\\nu = 29\\).","code":"\nAmostra1 <- rnorm(30,10,5)\nAmostra2 <- rnorm(30,15,5)\n\nvar.test(Amostra1, Amostra2) # Teste F para variâncias\n# \n#   F test to compare two variances\n# \n# data:  Amostra1 and Amostra2\n# F = 0.74981, num df = 29, denom df = 29, p-value = 0.4429\n# alternative hypothesis: true ratio of variances is not equal to 1\n# 95 percent confidence interval:\n#  0.3568811 1.5753388\n# sample estimates:\n# ratio of variances \n#          0.7498058\n\nt.test(Amostra1, Amostra2, var.equal = TRUE) # Independentes\n# \n#   Two Sample t-test\n# \n# data:  Amostra1 and Amostra2\n# t = -3.2434, df = 58, p-value = 0.001961\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  -7.208743 -1.706572\n# sample estimates:\n# mean of x mean of y \n#  9.873579 14.331237\nt.test(Amostra1, Amostra2, var.equal = TRUE, paired = TRUE) # Emparelhadas\n# \n#   Paired t-test\n# \n# data:  Amostra1 and Amostra2\n# t = -3.6981, df = 29, p-value = 0.000902\n# alternative hypothesis: true difference in means is not equal to 0\n# 95 percent confidence interval:\n#  -6.922951 -1.992364\n# sample estimates:\n# mean of the differences \n#               -4.457658"},{"path":"analdata.html","id":"dbasic","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2 Delineamentos básicos","text":"análises realizadas até agora tinham como objetivo verificar existência de diferenças entre médias de duas amostras. Porém, quando deseja-se estudar o efeito de “grupos de fatores” sobre determinado fenômeno, análise da variância (ANOVA) é indicada. ANOVA atribui diversos fatores partes da variabilidade dos dados (Casella 2008). Os delineamentos experimentais também são parte importante da ANOVA. Será dado mais destaque aos mais comuns: o delineamento inteiramente casualisado (DIC)  e o blocos ao acaso (DBC) . O bloqueamento tem como objetivo remover parte da variabilidade. Como não se deseja encotrar diferença entre os blocos, análises complementares não são realizadas para este fator.Nessa seção será demostrado como analisar dados experimentais utilizando estes dois delineamentos. Em um primeiro momento, serão demonstrados experimentos unifatorias e, posteriormente, experimentos bifatoriais com e sem parcelas subdivididas. Formas de como verificar se os pressupostos  modelo estatistico estão sendo cumpridos, e formas de contornar este problema caso estejam sendo violados, também serão demonstrados. Por fim, após análise da variância, serão mostrados os testes complementares utilizados para tratamentos quali e quantitativos.","code":""},{"path":"analdata.html","id":"princípios-básicos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.1 Princípios básicos","text":"Os principios básicos da experimentação são casualisação e repetição. repetição possibilita que o erro seja estimado, e casualisação que eles sejam independentes.","code":""},{"path":"analdata.html","id":"pressupostos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.2 Pressupostos","text":"Independente delineamento, os pressupostos  modelo estatístico são que os erros são independentes, homocedásticos e normais:\\[\r\n{\\boldsymbol{\\varepsilon }} \\sim {\\textrm N}\\left( {0,{\\boldsymbol{}}{\\sigma ^2}} \\right)\r\n\\]formas de realizar esse diagnóstico é através de testes estatísticos e gráficos de diagnósticos. Uma ferramente para contornar o problema de violação dos pressupostos é transformação dos dados. Existem várias formas de transformar os dados (cada uma adequada um caso específico), mas será dado enfase à transformação Box-Cox.","code":""},{"path":"analdata.html","id":"estimação","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.3 Estimação","text":"estimativa dos parâmetros é realizado pelo método dos minimos quadrados. Devido matriz delineamento ser de posto incompleto (modelo superparametrizado), uma restrição deve ser imposta ao fator tratamentos para que estimativa efeito dos fatores seja única (\\(\\sum {{t_i}} = 0\\)). Reparametrizações ou combinações lineares também podem ser utilizados para garantir unicidade dos parâmetros (Rencher Schaalje 2008).","code":""},{"path":"analdata.html","id":"reparametrização-condições-marginais-ou-combinações-lineares","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.4 Reparametrização, condições marginais ou combinações lineares?","text":"funções lm()  e aov()  são usualmente utilizadas para realizar análise da variância. Ambas utilizam reparametrização o modelo para estimar os parâmetros. Vamos ver isso exemplo abaixo utilizando o conjunto de dados QUALIAbaixo é apresentado o calculo da ANOVA de mod1 através de uma aboragem matricial. Portanto verifica-se que reparametrização é o método utilizado pelas funções aov() e lm(). Demonstrar como o procedimento de estimação e cálculo da ANOVA serviu apenas para verificar de maneira didática como funções R contornam o problema da singularidade da matriz delineamento. Conforme já relatado acima, funções aov() e lm() podem ser utilizadas para realizar ANOVA. Porém, nesta seção será dado enfase funções pacote ExpDes.pt,  cujas funções possibilitam analisar dados uni e bifatoriais; este último com e sem parcelas subdivididas.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\ndados <- import(url, sheet = \"QUALI\")\n\n## Usando a função aov()\nmod1 <- aov(RG ~ HIBRIDO, data = dados)\ncoef(mod1)\n#  (Intercept) HIBRIDONP_10  HIBRIDONP_2  HIBRIDONP_3  HIBRIDONP_4  HIBRIDONP_5 \n#     10.27750     -3.99450     -1.44550     -0.79750     -0.79675     -1.52675 \n#  HIBRIDONP_6  HIBRIDONP_7  HIBRIDONP_8  HIBRIDONP_9 \n#     -2.94125     -3.23250     -3.03250     -3.28525\n\nmod2 <- lm(RG ~ HIBRIDO, data = dados)\ncoef(mod2)\n#  (Intercept) HIBRIDONP_10  HIBRIDONP_2  HIBRIDONP_3  HIBRIDONP_4  HIBRIDONP_5 \n#     10.27750     -3.99450     -1.44550     -0.79750     -0.79675     -1.52675 \n#  HIBRIDONP_6  HIBRIDONP_7  HIBRIDONP_8  HIBRIDONP_9 \n#     -2.94125     -3.23250     -3.03250     -3.28525\n\n## Abordagem matricial\ny <- as.matrix(dados$RG)\nx <- model.matrix(~HIBRIDO, data = dados)\nbeta <- solve(t(x) %*% x) %*% t(x) %*% y\nbeta\n#                  [,1]\n# (Intercept)  10.27750\n# HIBRIDONP_10 -3.99450\n# HIBRIDONP_2  -1.44550\n# HIBRIDONP_3  -0.79750\n# HIBRIDONP_4  -0.79675\n# HIBRIDONP_5  -1.52675\n# HIBRIDONP_6  -2.94125\n# HIBRIDONP_7  -3.23250\n# HIBRIDONP_8  -3.03250\n# HIBRIDONP_9  -3.28525\n# Anova com uma abrodagem matricial\nSQtrat <- t(beta) %*% t(x) %*% y - ((sum(y))^2)/40\nSQtotal <- t(y) %*% y - ((sum(y))^2)/40\nSQres <- SQtotal - SQtrat\nFc <- (SQtrat / 9) / (SQres / 30)\np_val <- pf(Fc, 9, 30, lower.tail = FALSE)\ncat(\"\\nSQtrat = \", SQtrat)\n# \n# SQtrat =  65.66176\ncat(\"\\nSQtotal = \", SQtotal)\n# \n# SQtotal =  171.8282\ncat(\"\\nSQres = \", SQres)\n# \n# SQres =  106.1664\ncat(\"\\nFc = \", Fc)\n# \n# Fc =  2.061599\ncat(\"\\np_val = \", p_val, \"\\n\")\n# \n# p_val =  0.06654502\n\nanova(mod1)\n# Analysis of Variance Table\n# \n# Response: RG\n#           Df  Sum Sq Mean Sq F value  Pr(>F)  \n# HIBRIDO    9  65.662  7.2958  2.0616 0.06655 .\n# Residuals 30 106.166  3.5389                  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"analdata.html","id":"delineamento-inteiramente-casualizado-dic","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.5 Delineamento inteiramente casualizado (DIC)","text":"","code":""},{"path":"analdata.html","id":"modelo-estatístico","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.5.1 Modelo estatístico","text":"O delineamento inteiramente casualizado (DIC) é um delineamento adequado para áreas uniformes (parcelas são uniformes), onde não há necessidade de controle local (bloqueamento). Neste delineamento, os tratamentos devem ser distribuidos aleatoriamente nas parcelas.O modelo DIC é dado por \\[\r\n{Y_{ij}} = m + {t_i} + {\\varepsilon _{ij}}\r\n\\]Onde \\(m\\) é média geral experimento, \\(t_i\\) é o efeito de tratamentos, sendo estimado por \\(\\hat t_i = \\bar Y_{.} - \\bar Y_{..}\\) com seguinte restrição: \\(\\sum_i \\hat t_i = 0 ~~~~\\forall_i\\) (leia-se, o somatório dos efeitos de tratamento é zero para todo tratamento ). \\(\\epsilon_{ij}\\) é o erro experimental estimado por (\\(\\hat e_{ij} = Y_{ij} - m - \\hat t_i\\)) onde \\({e_{ij}}\\sim NID(0,{\\sigma ^2})\\).Experimentos em DIC serão analisados utilizando da função dic()  pacote ExpDes.pt. Os argumentos desta função são:Para maiores detalhes, ver o pdf pacote32 ou ir em ajuda (digitar ?ExpDes.pt console).","code":""},{"path":"analdata.html","id":"dic-com-fatores-qualitativos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.5.2 DIC com fatores qualitativos","text":"Os dados utilizados neste exemplo estão na planilha QUALI conjunto de dados data_R.xlsx. Os próximos códigos carregam o conjunto de dados e criam um gráfico tipo boxplot para explorar o padrão dos dados. Analizando o boxplot acima é razoável dizer que médias dos tratamentos são diferentes, principalmente comparando o NUPEC_10 com NUPEC_1. Esta suspeita de diferença, entanto, deve ser suportada com realização da análise de variância. pacote ExpDes.pt, quando os fatores são qualitativos, análise complementar aplicada é comparção de médias. função dic() pacote retorna tabela da ANOVA, análise de pressupostos  (normalidade e homogeneidade) e o teste de comparação de médias.funções pacote ExpDes.pt utilizam os dados “anexados” ao ambiente de trabalho, ou seja, um argumento data = . não existe para suas funções. Note que exemplo abaixo foi utilizado função (qualitativo, dic(...)). Isto permite acessar variáveis presentes data frame. Uma outra maneira de realizar esta mesma análise é utilizando função attach(qualitativo), qual carregará o data frame ambiente R, assim é possível utilizar função dic(...). Após realizada análise, é recomendado executar o comando detach(qualitativo) para “limpar” os dados ambiente de trabalho.interpretação da significância, ou seja, se médias de produtividade dos híbridos foram significativamente diferentes uma determinada probabilidade de erro é feita verificando-se o valor de “Pr>fc” na ANOVA. figura abaixo mostra distribuição F considerando os graus de liberdade de tratamento e erro \\(F_{9, 30}\\) e nos ajuda compreender um pouco melhor isto. O valor de F calculado em nosso exemplo foi de 2.0616, o que resulta em uma probabilidade de erro acumulada de 0.066545 (6,654%). Na figura abaixo, esta probabilidade de erro acumulada está representada pela cor vermelha. Para que uma diferença significativa 5% de probabilidade de erro tivesse sido observada, o valor de F calculado deveria ter sido 2.2107 [qf(0.05, 9, 30, lower.tail = FALSE)], representado neste caso pela cor verde gráfico.Em sequência ANOVA, função retorna o resultado da análise complementar solicitada. Neste exemplo, o teste de Tukey (5% de erro) é utilizado. Este teste realiza todas combinações possíveis entre médias (por isso o nome comparação múltipla de medias), comparando se diferença entre duas médias é maior ou menor que uma diferença mínima significativa (DMS). Esta DMS é calculada pela seguinte fórmula \\(DMS = q \\times \\sqrt{QME/r}\\), onde q é um valor tabelado, considerando o número de tratamentos e o GL erro; QME é o quadrado médio erro; e r é o número de repetições (ou blocos). O valor de q pode ser encontrado apêndice 1. Para este caso, considerando 10 e 30 como o número de tratamentos e o GL erro, respectivamente, o valor de q é 5,76, que aplicado na fórmula resulta em \\(DMS = 5,76 \\times \\sqrt{3.5389/4}=5.417\\). Logo, diferença mínima entre duas médias para que estas sejam significativamente diferentes (5% de erro), deve ser de 5,417 toneladas. Como diferença entre maior média (NUPEC_01) e menor média (NUPEC_10), foi de 3,994 toneladas, média de todos os tratamentos foram consideradas iguais.Considerando nosso exemplo, parece razoável dizer que 10,27 t é uma produção maior que 6,28 t. Então, é justo perguntar: O que pode ter acontecido para que médias não tenham sido consideradas diferentes considerando probabilidade de erro, mesmo tendo fortes indícios de que elas seriam? primeira opção que nos vem mente –e que na maioria das vezes é encontrada em artigos científicos– é que alterações rendimento de grão observadas fora resultado acaso; ou seja, neste caso, há probabilidade de 6,65% de que uma diferença pelo menos tão grande quanto observada estudo possa ser gerada partir de amostras aleatórias se os tratamentos não aferatem variável resposta. Logo, recomendação estatística neste caso, seria por optar por qualquer um dos tratamentos. ponto de vista prático, sabemos que esta recomendação está totalmente equivocada. Neste ponto surge uma importante (e polêmica) questão: interpretação p-valor. Um p-valor de 0,05 não significa que há uma chance de 95% de que determinada hipótese esteja correta. Em vez disso, significa que se hipótese nula verdadeira e todas outras suposições feitas forem válidas, haverá 5% de chance que diferenças ao menos tão grandes quanto observadas podem ser obtidas de amostras aleatórias. É preciso ter em mente que o p-valor relatado pelos testes é um significado probabilístico, não biológico. Assim, em experimentos biológicos interpretação desta estatística deve ser cautelosa, pois um p-valor não pode indicar importância de uma descoberta. Por exemplo, um medicamento pode ter um efeito estatisticamente significativo nos níveis de glicose sangue dos pacientes sem ter um efeito terapêutico. Sugerimos leitura de cinco interessantes artigos relacionados este assunto (Altman Krzywinski 2017; Baker 2016; Chawla 2017; Krzywinski Altman 2013; Nuzzo 2014).fórmula da DMS descrita acima é utilizada apenas se (e somente se) o número de repetições de todos os tratamentos é igual. Caso algum tratamento apresente um número inferir de repetições, fato comumente observado em experimentos de campo devido presença de parcelas perdidas, DMS deste par de médias em específico deve ser corrigida. Geralmente, análises complementares são realizadas quando ANOVA indica significância para um determinado fator de variação, entanto, o teste Tukey pode revelar diferença entre médias, mesmo quando o teste F não indicar essa diferença. Isto pode ser observado, principalmente quando probabilidade de erro muito próxima de 5%, por exemplo, Pr>Fc = 0.0502. recíproca também é verdadeira. O teste Tukey pode indicar que médias não diferem, se Pr>Fc = 0.0492, por exemplo.Em adição à justificativa anterior (alterações rendimento de grão observadas fora resultado acaso), existem pelo menos mais três razões potenciais para não regeição da hipótese \\(H_0\\) em nosso exemplo: () um experimento mal projetado com poder insuficiente para detectar uma diferença (à 5% de erro) entre médias; (ii) os tratamentos foram mal escolhidos e não refletiram adequadamente hipótese inicial estudo; ou (iii) o experimento foi indevidamente instalado e conduzido sem supervisão adequada, com baixo controle de qualidade sobre os protocolos de tratamento, coleta e análise de dados. Esta última opção parece ser mais razoável aqui. É possivel observar boxplot para o fator bloco que o bloco 4 parece ter uma média superior aos outros blocos. Sabe-ser que DIC, toda diferença entre repetições de um mesmo tratamento comporão o erro experimental. Logo, neste exemplo, área experimental não era homogênea como se pressupunha na instalação experimento. Isto ficará claro, posteriormente, ao analisarmos o mesmo conjunto de dados, entanto considerando um .É possível extrair os erros através de mod3$residuos. Também é possível fazer diagnóstico dos pressupostos modelo estatístico através de gráficos utilizando função plotres(): \r\nFigure 10.2: Gráfico de resíduos gerado pela função plotres()\r\nO p-valor teste de Shapiro-Wilk indicou que os resíduos  não seguem uma seguem uma distribuição normal, o que pode ser confirmado pelo QQ-Plot. O argumento hvar = \"levene\" na função é utilizado para testar homogeneidade dos resíduos. De acordo com o resultado teste, os resíduos podem ser considerados homogêneos.Exercício 7Utilize os pacotes dplyr e ggplot2 para confeccionar um gráfico de barras onde o eixo y é representado pelos híbridos e o eixo x pelo rendimento de grãos.Adicione uma linha vertical tracejada mostrando média global experimento.Adicione letra “” em todas barras para identificar não diferença entre médias.Resposta","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nqualitativo <- import(url, sheet = \"QUALI\")\np1 <- ggplot(qualitativo, aes(HIBRIDO, RG))+\n      geom_hline(yintercept = mean(qualitativo$RG), linetype = \"dashed\")+\n      geom_boxplot()+\n      stat_summary(geom = \"point\", fun = mean, shape = 23)+\n      theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\np2 <- ggplot(qualitativo, aes(factor(BLOCO), RG))+\n      geom_hline(yintercept = mean(qualitativo$RG), linetype = \"dashed\")+\n      geom_boxplot()+\n      stat_summary(geom = \"point\", fun = mean, shape = 23)\nplot_grid(p1, p2)\n\nmod3 <- with(qualitativo, dic(HIBRIDO, RG))\n# ------------------------------------------------------------------------\n# Quadro da analise de variancia\n# ------------------------------------------------------------------------\n#            GL      SQ     QM     Fc    Pr>Fc\n# Tratamento  9  65.662 7.2958 2.0616 0.066545\n# Residuo    30 106.166 3.5389                \n# Total      39 171.828                       \n# ------------------------------------------------------------------------\n# CV = 23.02 %\n# \n# ------------------------------------------------------------------------\n# Teste de normalidade dos residuos ( Shapiro-Wilk ) \n# Valor-p:  3.544449e-05 \n# ATENCAO: a 5% de significancia, os residuos nao podem ser considerados normais!\n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# Teste de homogeneidade de variancia \n# valor-p:  0.0006414681 \n# ATENCAO: a 5% de significancia, as variancias nao podem ser consideradas homogeneas!\n# ------------------------------------------------------------------------\n# \n# De acordo com o teste F, as medias nao podem ser consideradas diferentes.\n# ------------------------------------------------------------------------\n#    Niveis   Medias\n# 1    NP_1 10.27750\n# 2   NP_10  6.28300\n# 3    NP_2  8.83200\n# 4    NP_3  9.48000\n# 5    NP_4  9.48075\n# 6    NP_5  8.75075\n# 7    NP_6  7.33625\n# 8    NP_7  7.04500\n# 9    NP_8  7.24500\n# 10   NP_9  6.99225\n# ------------------------------------------------------------------------# Warning: `expand_scale()` is deprecated; use `expansion()` instead.\nplotres(mod3)"},{"path":"analdata.html","id":"dic-com-fatores-quantitativos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.5.3 DIC com fatores quantitativos","text":"Vimos anteriormente que os fatores qualitativos são comparados através de comparações de médias. caso de fatores quantitativos, o comum é utilizar regressões como análise complementar. Neste tipo de análise \\(SQ_{Trat}\\) é decomposta, e cada polinômio explicará parte desta soma de quadrados. O maior grau significativo polinômio determinará qual regressão escolhida. Para implementar essa análise, utilizando função dic(), basta indicar como FALSE argumentos quali. O arquivo de dados “QUALITATIVO.xlsx” contém três exemplos, com comportamento linear, quadrático e cúbico. Utilizando o que aprendemos ggplot2 até agora, vamos criar um gráfico para visualisar estes dados.   \r\nFigure 10.3: Exemplo de regressão com comportamento linear, quadrático e cúbico\r\nComo exemplo didático, vamos analisar os dados que parecem ter comportamento quadrático para ver se, estatisticamente, isto é confirmado. Para isto, utilizamos função subset()  para criar um novo conjunto de dados que contenha somente o nível “QUADRÁTICA” dos nosso dados originais. Para evitar uma longa saída, os resultados desta seção em específico não foram mostrados.Abaixo vamos reproduzir o exemplo para os usuários das funções aov() ou lm().  O polinômio  de segundo grau deve ser o escolhido, pois ele foi o maior grau significativo (p-valor menor que 0,05). Os desvios da regressão nada mais são que FALTA DE AJUSTE  dos modelos. Porém, como é comum em ciências agrárias, ajusta-se apenas polinômios até terceira ordem, devido dificuldade de interpretção de polinômios com ordens superiores. É importante ressaltar que aqui estamos falando de regressões polinomiais. Como será visto posteriormente, falta de ajuste é inaceitável para modelos de regressão múltipla. O valor de \\(R^2\\) é dado pela razão entre \\(SQ_{Regressão}\\) e \\(SQ_{Tratamento}\\). exemplo acima, o \\(R^2\\) da regressão quadrática (selecionada) é:\\[\r\n{R^2} = \\frac{{S{Q_{{\\rm{Regressão}}}}}}{{S{Q_{{\\rm{Tratamento}}}}}} = \\frac{{2,4900 + 1,5465}}{{4,0505}} = 0.9965\r\n\\]Percebe-se claramente pelos resultados acima que significância grau polinômio está diretamente relacionado com quanto ele “contribui” para explicar \\(SQ_{Trat}\\). forma mais prática de analisar uma regressão é através de gráficos. função dic() fornece essa alternativa, através da função graphics(). Nesta função existe dois argumentos obrigatórios: , onde indicamos o objeto que contém saída da análise experimento e grau, onde indicamos o grau polinômio.Gráfico de resultados: uma proposta\r\nFigure 10.4: Gráfico de linhas gerado pela função ggplot()\r\n   Este é um exemplo simples de uso da função ggplot(). Porém, como visto até aqui, esta função (mesmo em aplicações simples) tem suas complexidades. Pensando nisso, o pacote metan33 foi desenvolvido e contém funções úteis que facilitam confeção de gráficos. partir dos experimentos fatoriais, apenas funções deste pacote serão utilizados para confecção dos gráficos.Exercício 8Utilize função plot_lines() pacote metan** para confeccionar um gráfico semelhante ao anterior. Para maiores detalhes veja ?metan::plot_lines.Resposta","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nquantitativo_todos <- import(url, sheet = \"QUANTI\")\nggplot(quantitativo_todos, aes(DOSEN, RG, col = TIPO)) +\n  geom_point() +\n  geom_smooth()\nquantitativo <- filter(quantitativo_todos, TIPO  ==  \"QUADRÁTICA\")\ncrd_Reg <- with(quantitativo, dic(DOSEN, RG, quali = FALSE, hvar = \"levene\"))\nReg <- quantitativo\nTotal <- aov(RG ~ 1, data = Reg)\nSaturado <- aov(RG ~ factor(DOSEN), data = Reg)\nLinear <- lm(RG ~ DOSEN, data = Reg)\nQuadratica <- lm(RG ~ poly(DOSEN, 2, raw = TRUE), data = Reg)\nCubica <- lm(RG ~ poly(DOSEN, 3, raw = TRUE), data = Reg)\nanova(Total, Linear, Quadratica, Cubica, Saturado) \n# Analysis of Variance Table\n# \n# Model 1: RG ~ 1\n# Model 2: RG ~ DOSEN\n# Model 3: RG ~ poly(DOSEN, 2, raw = TRUE)\n# Model 4: RG ~ poly(DOSEN, 3, raw = TRUE)\n# Model 5: RG ~ factor(DOSEN)\n#   Res.Df    RSS Df Sum of Sq       F    Pr(>F)    \n# 1     19 3.4242                                   \n# 2     18 2.7430  1   0.68121 20.3242 0.0004164 ***\n# 3     17 0.5862  1   2.15679 64.3488 8.327e-07 ***\n# 4     16 0.5861  1   0.00009  0.0027 0.9593568    \n# 5     15 0.5028  1   0.08332  2.4858 0.1357289    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nggplot(quantitativo, aes(x = DOSEN, y = RG))+ # Indicar dados e variáveis\ngeom_point(color = \"blue\", size = 2) + # Adiociona e edita os pontos\ngeom_smooth(method = \"lm\",\n            formula = y ~ poly(x, 2, raw = TRUE),\n            color = \"red\",\n            fill = \"grey\")"},{"path":"analdata.html","id":"delineamento-blocos-ao-acaso-dbc","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.6 Delineamento blocos ao acaso (DBC)","text":"delineamento de blocos ao acaso uma restrição  na casualisação é imposta visando agrupar unidades experimentais uniformes dentro de um bloco, de maneira que heterogeneidade da área experimental fique entre os blocos. O bloquemento tem como objetivo reduzir o erro experimental, “transferindo” parte erro experimental para efeito de bloco. O modelo DBC é dado por\\[\r\n{Y_{ij}} = m + {b_j} + {t_i} + {\\varepsilon _{ij}}\r\n\\]Onde \\(m\\) é média geral experimento, \\(b_j\\) é o efeito de bloco, \\(t_i\\) é o efeito de tratamentos e \\(\\epsilon_{ij}\\) é o erro experimental. pacote ExpDes.pt, este delineamento é executado pela função dbc(). Os argumentos desta função são:Percebe-se que apenas um argumento foi adicionado função: bloco.","code":""},{"path":"analdata.html","id":"dbc-com-fatores-qualitativos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.6.1 DBC com fatores qualitativos","text":"Neste exemplo, vamos realizar ANOVA com os mesmos dados exemplo , agrupando médias pelo teste Scott-Knott utilizando o argumento mcomp = \"sk\":Considerando o bloco como um fator de variação experimento, pode-se afirmar que média de produtividade difere entre os híbridos testados. Como o efeito de bloco foi significativo Pr>Fc < 0.05, conclui-se que escolha pelo delineamento em blocos casualizados foi correta, e, principalmente, que disposição dos blocos na área experimental possibilitou lograr heterogeneidade entre os blocos, deixando homogeneidade dentro de cada bloco. O Fc para o fator de tratamento “HÍBRIDOS” foi de 18.716, qual acumula uma probabilidade de erro de somente 0.0000002029.Incluindo o bloco como fonte de variação nota-se que 3 graus de liberdade (GL) que antes compunham o erro “saíram” e passaram compor o GL bloco. Assim, neste exemplo, o GL erro foi de 27. entanto, uma grande parte da soma de quadrados erro (aproximadamente 90%) observada delineamento DIC era oriunda efeito de bloco. Com isto, soma de quadrado erro considerando o delineamento DBC foi de apenas 10.525 e, mesmo com “perda” de 3 GL para compor o bloco, o quadrado médio erro foi de somente 0.390 (89% menor quando comparado com o delineamento DIC). Consequentemente o valor F calculado para o fator HÍBRIDO foi significativo. Cabe ressaltar que soma de quadrados para o fator de variação HÍBRIDO não muda se o delineamento DIC ou DBC.Uma sugestão para apresentação dos dados é confecção de um gráfico de barras. Para isto, utilizaremos função plot_bars() pacote metan. Utilizando o argumento lab.bars é possível adicionar letras teste SK. Por padrão, barras de erro mostrando o erro padrão da média são adicionadas. Para suprimir estas barras use errorbar = FALSE. É possível também adicionar os valores às barras usando values = TRUE, ordenar em ordem crescente ou decrescente, bem como modificar cor e preenchimento das barras facilmente","code":"\nmod4 <- with(qualitativo, dbc(HIBRIDO, BLOCO, RG, mcomp = \"sk\"))\n# ------------------------------------------------------------------------\n# Quadro da analise de variancia\n# ------------------------------------------------------------------------\n#            GL      SQ     QM     Fc      Pr>Fc\n# Tratamento  9  65.662  7.296 18.716 2.0298e-09\n# Bloco       3  95.642 31.881 81.785 1.1000e-13\n# Residuo    27  10.525  0.390                  \n# Total      39 171.828                         \n# ------------------------------------------------------------------------\n# CV = 7.64 %\n# \n# ------------------------------------------------------------------------\n# Teste de normalidade dos residuos \n# valor-p:  0.6834664 \n# De acordo com o teste de Shapiro-Wilk a 5% de significancia, os residuos podem ser considerados normais.\n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# Teste de homogeneidade de variancia \n# valor-p:  0.2599294 \n# De acordo com o teste de oneillmathews a 5% de significancia, as variancias podem ser consideradas homogeneas.\n# ------------------------------------------------------------------------\n# \n# Teste de Scott-Knott\n# ------------------------------------------------------------------------\n#    Grupos Tratamentos   Medias\n# 1       a        NP_1 10.27750\n# 2       a        NP_4  9.48075\n# 3       a        NP_3  9.48000\n# 4       b        NP_2  8.83200\n# 5       b        NP_5  8.75075\n# 6       c        NP_6  7.33625\n# 7       c        NP_8  7.24500\n# 8       c        NP_7  7.04500\n# 9       c        NP_9  6.99225\n# 10      c       NP_10  6.28300\n# ------------------------------------------------------------------------\np1 <- plot_bars(qualitativo, x = HIBRIDO, y = RG)\np2 <- plot_bars(qualitativo,\n                x = HIBRIDO,\n                y = RG,\n                lab.bar = c(\"a\", \"c\", \"b\", \"a\", \"a\", \"b\", \"c\", \"c\", \"c\", \"c\"),\n                order = \"desc\",\n                errorbar = FALSE,\n                values = TRUE,\n                n.dodge = 2)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"analdata.html","id":"dbc-com-fatores-quantitativos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.6.2 DBC com fatores quantitativos","text":"Para realizar análise de regressão considerando um delineamento DBC basta utilizar função dbc()  incluindo o seguinte argumento quali = FALSE.Exercício 9Rode programação para os dados quantitativos considerando o delineamento DBC e compare os resultados com aquela obtida pela função dic() para os mesmos dados.Rode programação para os dados quantitativos considerando o delineamento DBC e compare os resultados com aquela obtida pela função dic() para os mesmos dados.estimativas dos parâmetros da regressão são mesmas?estimativas dos parâmetros da regressão são mesmas?O p-valor para o testes de hipótese para efeito de tratamento é o mesmo?O p-valor para o testes de hipótese para efeito de tratamento é o mesmo?Resposta","code":""},{"path":"analdata.html","id":"análise-de-experimentos-utilizando-modelos-mistos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.2.6.3 Análise de experimentos utilizando modelos mistos","text":"função gamem() pacote metan pode ser utilizada para analizar dados de experimentos unifatoriais utilizando um modelo misto de acordo com seguinte equação:\\[\r\ny_{ij}= \\mu  + \\alpha_i + \\tau_j + \\varepsilon_{ij}\r\n\\]onde \\(y_ {ij}\\) é o valor observado para o -ésimo genótipo na j-ésima repetição (= 1, 2, … g; j = 1, 2,. ., r); sendo g e r o número de genótipos e repetições, respectivamente; \\(\\alpha_i\\) é o efeito aleatório -ésimo genótipo; \\(\\tau_j\\) é o efeito fixo da j-ésima repetição; e \\(\\varepsilon_ {ij}\\) é o erro aleatório associado \\(y_{ij}\\). Neste exemplo, usaremos os dados de exemplo data_g pacote metan.maneira mais fácil de obter os resultados modelo acima é usando função get_model_data(). Vamos fazer isso.Teste de razão de máxima verossimilhançaComponentes de variânciaMédias preditasNo exemplo acima, o design experimental foi o de blocos completos casualizados. Também é possível analisar um experimento conduzido em alfa-lattice com função gamem(), baseado na seguinte equação:\\begin{gather} y_{ijk}= \\mu + \\alpha_i + \\gamma_j + (\\gamma \\tau)_{jk} + \\varepsilon_{ijk} \\end{gather}onde \\(y_ {ijk}\\) é o valor observado -ésimo genótipo k-ésimo bloco da j- ésima repetição (= 1, 2, … g; j = 1, 2, .., r; k = 1, 2, .., b); respectivamente; \\(\\alpha_i\\) é o efeito aleatório -ésimo genótipo; \\(\\gamma_j\\) é o efeito fixo da j-ésima repetição; \\((\\gamma \\tau)_{jk}\\) é o efeito aleatório k-ésimo bloco incompleto aninhado na repetição j; e \\(\\varepsilon_{ijk}\\) é o erro aleatório associado \\(y_{ijk}\\). Neste exemplo, usaremos os dados de exemplo data_alpha pacote metan.","code":"\ngen_mod <- gamem(data_g, GEN, REP,\n                 resp = c(ED, CL, CD, KW, TKW, NKR))\n# Method: REML/BLUP\n# Random effects: GEN\n# Fixed effects: REP\n# Denominador DF: Satterthwaite's method\n# ---------------------------------------------------------------------------\n# P-values for Likelihood Ratio Test of the analyzed traits\n# ---------------------------------------------------------------------------\n#     model       ED       CL    CD     KW     TKW   NKR\n#  Complete       NA       NA    NA     NA      NA    NA\n#  Genotype 2.73e-05 2.25e-06 0.118 0.0253 0.00955 0.216\n# ---------------------------------------------------------------------------\n# Variables with nonsignificant Genotype effect\n# CD NKR \n# ---------------------------------------------------------------------------\nget_model_data(gen_mod, \"lrt\")\n# Class of the model: gamem\n# Variable extracted: lrt\n# # A tibble: 6 x 8\n#   VAR   model     npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 ED    Genotype     4  -91.9  192. 17.6      1   0.0000273 \n# 2 CL    Genotype     4  -86.2  180. 22.4      1   0.00000225\n# 3 CD    Genotype     4  -52.5  113.  2.45     1   0.118     \n# 4 KW    Genotype     4 -165.   339.  5.00     1   0.0253    \n# 5 TKW   Genotype     4 -190.   389.  6.72     1   0.00955   \n# 6 NKR   Genotype     4  -96.3  201.  1.53     1   0.216\nget_model_data(gen_mod, \"genpar\")\n# Class of the model: gamem\n# Variable extracted: genpar\n# # A tibble: 11 x 7\n#    Parameters     ED     CL     CD      KW      TKW    NKR\n#    <chr>       <dbl>  <dbl>  <dbl>   <dbl>    <dbl>  <dbl>\n#  1 Gen_var     5.37   4.27   0.240 181.     841.     2.15 \n#  2 Gen (%)    68.8   75.1   27.4    39.2     45.2   21.6  \n#  3 Res_var     2.43   1.41   0.634 280.    1018.     7.80 \n#  4 Res (%)    31.2   24.9   72.6    60.8     54.8   78.4  \n#  5 Phen_var    7.80   5.68   0.873 461.    1859.     9.94 \n#  6 H2          0.688  0.751  0.274   0.392    0.452  0.216\n#  7 h2mg        0.869  0.901  0.532   0.659    0.712  0.452\n#  8 Accuracy    0.932  0.949  0.729   0.812    0.844  0.673\n#  9 CVg         4.84   7.26   3.10    9.16     9.13   4.82 \n# 10 CVr         3.26   4.18   5.05   11.4     10.0    9.19 \n# 11 CV ratio    1.49   1.74   0.615   0.803    0.909  0.525\nget_model_data(gen_mod, \"blupg\")\n# Class of the model: gamem\n# Variable extracted: blupg\n# # A tibble: 13 x 7\n#    GEN      ED    CL    CD    KW   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     50.2  30.7  15.8  153.  354.  29.5\n#  2 H10    44.4  25.1  15.5  129.  268.  31.7\n#  3 H11    47.2  26.6  15.6  143.  297.  31.3\n#  4 H12    47.8  26.1  15.2  148.  293.  30.0\n#  5 H13    50.3  27.4  15.9  170.  319.  31.2\n#  6 H2     50.3  30.0  16.3  156.  338.  29.6\n#  7 H3     47.2  28.6  16.1  142.  331.  30.2\n#  8 H4     46.1  27.8  16.2  145.  310.  31.8\n#  9 H5     49.8  30.1  16.2  156.  309.  31.3\n# 10 H6     49.7  31.6  15.2  140.  325.  28.6\n# 11 H7     48.7  30.0  15.5  153.  346.  30.0\n# 12 H8     46.3  29.0  15.8  143.  339.  29.5\n# 13 H9     44.4  27.0  15.8  131.  301.  30.4\ngen_alpha <- gamem(data_alpha, GEN, REP, YIELD, block = BLOCK)\n# Method: REML/BLUP\n# Random effects: GEN, BLOCK(REP)\n# Fixed effects: REP\n# Denominador DF: Satterthwaite's method\n# ---------------------------------------------------------------------------\n# P-values for Likelihood Ratio Test of the analyzed traits\n# ---------------------------------------------------------------------------\n#      model    YIELD\n#   Complete       NA\n#   Genotype 1.18e-06\n#  rep:block 3.35e-03\n# ---------------------------------------------------------------------------\n# All variables with significant (p < 0.05) genotype effect\nget_model_data(gen_alpha, \"lrt\")\n# Class of the model: gamem\n# Variable extracted: lrt\n# # A tibble: 2 x 8\n#   VAR   model      npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>     <int>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 YIELD Genotype      5  -58.4  127. 23.6      1   0.00000118\n# 2 YIELD rep:block     5  -50.9  112.  8.61     1   0.00335\nget_model_data(gen_alpha, \"details\")\n# Class of the model: gamem\n# Variable extracted: details\n# # A tibble: 6 x 2\n#   Parameters YIELD                   \n#   <chr>      <chr>                   \n# 1 Ngen       24                      \n# 2 OVmean     4.4795                  \n# 3 Min        2.8873 (G03 in B6 of R3)\n# 4 Max        5.8757 (G05 in B1 of R1)\n# 5 MinGEN     3.3431 (G03)            \n# 6 MaxGEN     5.1625 (G01)\nget_model_data(gen_alpha, \"genpar\")\n# Class of the model: gamem\n# Variable extracted: genpar\n# # A tibble: 13 x 2\n#    Parameters      YIELD\n#    <chr>           <dbl>\n#  1 Gen_var        0.143 \n#  2 Gen (%)       48.5   \n#  3 rep:block_var  0.0702\n#  4 rep:block (%) 23.8   \n#  5 Res_var        0.0816\n#  6 Res (%)       27.7   \n#  7 Phen_var       0.295 \n#  8 H2             0.485 \n#  9 h2mg           0.798 \n# 10 Accuracy       0.893 \n# 11 CVg            8.44  \n# 12 CVr            6.38  \n# 13 CV ratio       1.32"},{"path":"analdata.html","id":"transformação-de-dados","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.3 Transformação de dados","text":"Em todos os exemplos apresentados até aqui, os resíduos  devem cumprir os seguintes pressupsotos: normalidade, homocedasticidade e independência:\\[\r\n{\\boldsymbol{\\varepsilon }} \\sim {\\rm N}\\left( {0,{\\boldsymbol{}}{\\sigma ^2}} \\right)\r\n\\]Esses pressupostos  são necessários para que o teste F seja utilizado na análise de variância. Sob normalidade  dos resíduos e hipótese nula \\(H_0\\), razão entre somas de quadrado de tratamento e resíduo tem distribuição F (Rencher Schaalje 2008). Já em condições de não normalidade dos resíduos, o poder teste  (probabilidade de rejeitar \\(H_0\\)) é reduzido. Apesar disso, não há grandes mudanças erro tipo quando pressuposição de normalidade é violada (Senoglu Tiku 2001), e por isso ele é considerado robusto.Apesar teste F de ser robusto desvios da normalidade, é comum que ela seja cumprida para que o teste seja aplicado. Quando pressuposições  não são cumpridas, um dos procedimentos mais comum é transformar os dados. transformação Box-Cox (Box Cox 1964)  é uma das mais comuns. Ela consiste em transformar os valores de \\(Y_i\\) por \\(Y_i(\\lambda)\\), sendo o valor de \\(\\lambda\\) estimado por máxima verossimilhança. Após transformação de \\(Y_i\\) por \\(Y_i(\\lambda)\\) os dados seguem distribuição normal com variância constante.função boxcox() , pacote MASS, pode ser utilizada para estimar o valor de \\(\\lambda\\). Uma sequência de valores de \\(\\lambda\\) são estimados, e o escolhido é aquele que maximiza função de log-verossimilhança. modelo considerando o delineamento inteiramente casualizado (qualitativo), o pressuposto  de normalidade foi violado. O próximo passo é encontrar o valor de \\(\\lambda\\) para transformar variáveis, utilizando para isso função boxcox().\r\nFigure 10.5: Gráfico gerado pela função boxcox() para identificar o valor de lambda\r\nPercebe-se que o intervalo de \\(\\lambda\\) cruza pelo valor zero, indicando que transformação log é mais adequada. Uma forma mais prática de encontrar o valor de \\(\\lambda\\) que maximiza função de log-verossimilhança  é utilizar função locator() . Após executar função abaixo, basta clicar com o cursor sobre o ponto que maximiza função para que coordenadas sejam mostradas console.Utilizando transformação log(), os resíduos ainda continuaram sendo não normais, apesar de se aproximaram mais da distribuição normal neste caso. Quando transformação não eficiente, recomenda-se utilização de testes não paramétricos, por exemplo, caso de um delineamento inteiramente casualizado, o teste de Kruskal-Wallis ou de um delineamento de blocos completos casualisados, o teste de Friedman.","code":"\nMASS::boxcox(RG ~ HIBRIDO, data = qualitativo,\n             lambda = seq(-2, 2, length = 20)) # Indica os valores de lambda \nlocator(n = 1)\n\nmod5.1  = with(qualitativo, dic(HIBRIDO, log(RG)))\nplotres(mod5.1)"},{"path":"analdata.html","id":"ancova","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4 Análise de covariância (ANCOVA)","text":"","code":""},{"path":"analdata.html","id":"introdução","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4.1 Introdução","text":"Neta seção, veremos alguns conceitos estatísticos e uma aplicação numérica de uma técnica interessante que pode ser útil, se corretamente utilizada, para reduzir o erro experimental em experimentos agronômicos: análise de covariância (ANCOVA). ANCOVA é um modelo linear geral que combina princípios de ANOVA e regressão para avaliar se médias de uma variável dependente são iguais entre níveis de uma variável independente categórica (tratamento), controlando estatisticamente os efeitos de outras variáveis contínuas que não são de interesse primário. Tais variáveis contínuas são medidas em cada unidade experimental e são chamadas covariáveis. Idealmente, estas variáveis devem ser determinadas antes que os tratamentos tenham sido atribuídos às unidades experimentais ou, mínimo, que os valores das covariáveis não sejam afetados pelos tratamentos aplicados (Snedecor Cochran 1967). ANCOVA tem várias aplicações. contexto da experimentação agronômica, ela é frequentemente utilizada reduzir variabilidade experimento pela contabilização da variabilidade nas unidades experimentais que não puderam ser controladas pelo design experimental.Considere um experimento conduzido em uma área em que unidades experimentais exibem considerável variabilidade que não pode ser controlada utilizando estratégias de bloqueio. O pesquisador acredita, digamos, baseado em experiências passadas, que uma ou mais características das unidades experimentais podem ajudar descrever parte da variabilidade entre unidades experimentais. Nesta condição, o pesquisador pode utilizar resultados de experimentos passados observados nas mesmas unidades experimentais –excluido o efeito de tratamento– como uma covariável. Outra estratégia –embora um pouco distante em tempos de recursos financeiros limitados– poderia ser realização de uma análise de solo em cada unidade experimental e utilizar os resultados obtidos como covariáveis. Neste sentido, ao utilizar informações relacionadas características fisico-químicas solo como uma covariável, o pesquisador tenta explicar variabilidade nas unidades experimentais que não podem ser convenientemente controladas por outra técnica experimental.","code":""},{"path":"analdata.html","id":"modelo-estatístico-1","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4.2 Modelo estatístico","text":"Em uma estrura de tratamentos fixos, unifatorial, conduzidos em delineamento inteiramente casualizado, o modelo da ANOVA tradicional visto anteriormente pode ser escrito da seguinte forma quando uma covariável numérica (\\(X\\)) também foi mensurada em cada unidade experimental.\\[\r\nY_{ij} = \\mu + \\tau_i + \\beta(X_{ij} - \\bar X_{..}) + \\varepsilon_{ij}\r\n\\]onde \\(Y_{ij}\\) é o valor observado -ésimo tratamento na j-ésima repetição; \\(\\mu\\) é média geral; \\(\\tau_i\\) é o efeito -ésimo tratamento; \\(\\beta\\) é o coeficiente de regressão de Y em X e \\(\\varepsilon_{ij}\\) é o erro aleatório.pressuposições modelo da ANCOVA são mesmas que ANOVA, ou seja, aditividade dos efeitos de bloco e tratamento (em DBC), normalidade, homogeneidade e independêcia dos resíduos, em adição à duas importantes considerações adicionais: () independência entre covariável e o efeito tratamento; e (ii) homogeneidade dos coeficientes angulares da regressão, que serão tratados à partir daqui como slopes. Considerando que os pressupostos da ANOVA já são conhecidos, veremos com mais detalhes estes dois últimos à seguir.","code":""},{"path":"analdata.html","id":"independência-entre-a-covariável-e-os-efeitos-de-tratamento","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4.3 Independência entre a covariável e os efeitos de tratamento","text":"Vimos anteriormente que covariável deve ser independente efeito tratamento. figura 6 mostra três diferentes cenários. primeiro () represetação esquemática de um delineamento unifatorial conduzido em DIC é mostrada. Neste exemplo, variância da variável resposta (RG) pode ser dividida em duas partes. Uma correspondente aos efeitos dos tratamentos e outra devido ao erro experimental, ou variância não explicada pelos tratamentos. Aqui, vimos novamente que toda variância não explicada pelos termos modelo irá compor o erro experimental. segundo exemplo (b) um cenário ideal para ANCOVA é representado. Nesta condição, covariável compartilha sua variância apenas com o pouco da variância RG que é atualmente inexplicado pelos efeitos dos tratamentos . Em outras palavras, é completamente independente dos tratamentos. Este cenário é o único em que ANCOVA tradicional é apropriada.Regra para análise de covariânciaO terceiro exemplo, (c) representa uma situação em que pessoas costumam usar ANCOVA quando não deveriam. Nesta situação, o efeito da covariável se sobrepõe ao efeito tratamento. Em outras palavras, o efeito tratamento é confundido com o efeito da covariável. Em situções como esta, covariável reduzirá (estatisticamente falando) o efeito tratamento, pois explica uma parte da variação que seria atribuída ao efeito de tratamento. Assim, efeitos espúrios de tratamento podem surgir, comprometendo interpretação da ANCOVA (Stevens 2009).ANCOVA nem sempre é uma solução magica. O problema compartilhamento da variância da covariável com variância de tratamento é comum e muitas vezes é ignorado ou incompreendido pelos pesquisadores (Miller Chapman 2001). Em uma ampla revisão, Miller e Chapman citam muitas situações em que pessoas aplicam ANCOVA de maneira incorreta. Recomendamos leitura deste artigo. Felizmente, modelos generalizados de ANCOVA que permitem tratar esta interação tratamento-covariável têm sido desenvolvidos (Mayer et al. 2014), mas isto está além objetivo deste material.","code":""},{"path":"analdata.html","id":"homogeneidade-dos-slopes-da-regressão","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4.4 Homogeneidade dos slopes da regressão","text":"Baseado modelo estatístico apresentado, percebe-se que quando uma ANCOVA é realizada, buscamos uma relação geral entre variável dependente e covariável; ou seja, uma regressão global é ajustada aos dados, ignorando à que nível tratamento uma determinada obsevação pertence. Ao ajustar esse modelo geral, supomos, portanto, que essa relação geral seja verdadeira para todos os níveis fator tratamento. Por exemplo, se houver uma relação positiva (slope positivo) entre covariável e variável dependente primeiro nível (\\(T_1\\)), presumimos que há uma relação positiva (slope positivo) em todos os outros níveis também. Vamos tentar tornar esse conceito um pouco mais concreto. figura 7 mostra um gráfico de dispersão que exibe uma relação hipotética entre covariável e variável dependente em duas condições: heterogeneidade de slope (esquerda) e homogeneidade de slope (direita). Na primeira condição, relação entre variável dependente e covariável é positiva para os tratamentos \\(T_1\\) e \\(T_2\\), mas para o \\(T_3\\), esta relação parece ser negativa. Esta é uma condição em que utilização da ANCOVA não é indicada. Na segunda condição, relação entre variável dependente e covariável é muito semelhante entre os tratamentos.\r\nFigure 10.6: Gráfico de dispersão e linhas de regressão entre variável dependente e covariável. diferentes cores represetam três tratamentos hipotéticos.\r\n   ","code":""},{"path":"analdata.html","id":"um-exemplo-numérico","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.4.5 Um exemplo numérico","text":"Até aqui, passamos por uma breve introdução abordando o modelo mais simples de ANCOVA. Esta técnica, entanto, pode ser utilizada em qualquer delineamento experimental. Para maiores informações, recomendamos leitura de três bons materiais: (Rutherford 2001), um livro específico para ANCOVA; (Field, Miles, Field 2012), pp. 462, com aplicação em R; e (Scheiner Gurevitch 2001), pp. 77, com aplicação em SAS.Vamos agora, utilizando um exemplo numérico, demonstrar como esta análise pode ser realizada software R e identificar como ela pode ser útil na análise de experimentos agronônicos. Os dados são apresentados em (Snedecor Cochran 1967), pp. 428 e são resultantes de um experimento com 6 tratamentos (cultivares), conduzido em DBC com 4 blocos. variável resposta mensurada em cada unidade experimental foi gramas de espigas (GE) em adição uma covariável, número de plantas por unidade experimental (NPLA). Para realizar uma ANCOVA, recomendamos que seguintes etapas sejam seguidas:Assumindo que o R será utilizado, insira os dados e instale os pacotes necessários.Assumindo que o R será utilizado, insira os dados e instale os pacotes necessários.Explore os seus dados: Faça uso de gráficos para explorar o padrão encontrado nos dados. sugestão aqui é utilizar gráficos tipo boxplot, visto riqueza de informações proporcionada por este tipo de gráficos.Explore os seus dados: Faça uso de gráficos para explorar o padrão encontrado nos dados. sugestão aqui é utilizar gráficos tipo boxplot, visto riqueza de informações proporcionada por este tipo de gráficos.Verifique se covariável e os tratamentos são independentes: Execute uma ANOVA com covariável como o variável dependente para verificar se covariável não difere significativamente entre os níveis da variável independente (tratamento). Se um resultado significativo observado, interrompa análise aqui.Verifique se covariável e os tratamentos são independentes: Execute uma ANOVA com covariável como o variável dependente para verificar se covariável não difere significativamente entre os níveis da variável independente (tratamento). Se um resultado significativo observado, interrompa análise aqui.ANCOVA: assumindo que tudo estava bem nas etapas 2 e 3, execute análise principal de covariância.ANCOVA: assumindo que tudo estava bem nas etapas 2 e 3, execute análise principal de covariância.Verifique homogeneidade dos slopes da regressão: execute novamente ANCOVA, incluindo, agora, interação entre variável independente e covariável. Se esta interação é significativa, então você não pode assumir homogeneidade dos slopes da regressão.Verifique homogeneidade dos slopes da regressão: execute novamente ANCOVA, incluindo, agora, interação entre variável independente e covariável. Se esta interação é significativa, então você não pode assumir homogeneidade dos slopes da regressão.Compute análises complementares: encontrada diferença significativa para tratamento na etapa 4 e assumindo que etapa 5 indicou homogeneidade dos slopes da regressão, análises complementares –como comparações múltiplas de médias– podem então ser realizadas.Compute análises complementares: encontrada diferença significativa para tratamento na etapa 4 e assumindo que etapa 5 indicou homogeneidade dos slopes da regressão, análises complementares –como comparações múltiplas de médias– podem então ser realizadas.Vamos agora ver cada uma destas etapas detalhadamente.1. Download dos dados e pacotes necessáriosO seguinte código é utilizado para instalar/carregar os pacotes necessários bem como para fazer o upload dos dados e armazenar dataframe covar    2. Explorando os dadosA construção de gráficos tipo boxplot para variável resposta e covariável são importantes, pois permitem identificar presença de possíveis outliers nos dados além de facilitar visualização de padrões de associação entre variáveis.\r\nFigure 10.7: Gráfico boxplot da variável independente (GE) e da covariável (NPUE) em cada nível tratamento (T).\r\n    linha tracejada horizontal representa média geral de cada variável. Seis estatísticas são mostradas neste boxplot. mediana (linha horizontal); média (losango vermelho); caixas inferior e superior correspondem ao primeiro e terceiro quartis (percentis 25 e 75, respectivamente); linha vertical superior se estende da caixa até o maior valor, não maior que \\(1,5 \\times {IQR}\\) (onde IQR é amplitude interquartílica). linha vertical inferior se estende da caixa até o menor valor, de máximo, \\(1,5 \\times {IQR}\\). Dados além destas linhas podem ser considerados outliers.3. Identificando independência entre covariável e os tratamentosA independência entre covariavel e os efeitos de tratamento, demostrada graficamente na figura 6 é um importante pressuposto da ANCOVA. Esta independência é checada realizando uma ANOVA considerando covariável como variável dependente. função aov() é utilizada para o ajuste modelo. O primeiro argumento, NPUE, é variável resposta (neste caso covariável) que queremos analisar. O operador ~ informa que os seguintes argumentos irão ser considerados como fontes de variação modelo. Neste caso, TRAT e BLOCO são incluídos.O resultado da ANOVA acima indica que covariável é independente dos efeitos de tratamento. Neste caso, prosseguimos para o próximo passo. Se um resultado significativo encontrado nesta etapa, realização da ANCOVA não é recomendada.4. O modelo da ANCOVAPara realizar ANCOVA, utilizaremos mesma função aov(). Agora, entanto, variável dependente (lado esquerdo operador ~) será GE e covariável NPUE será incluída modelo. Aqui, função anova() é substituída pela função Anova() pacote car para que soma de quadrado tipo III (em linguagem SAS) seja computada. Este tipo de soma de quadrados é utilizada, pois cada efeito é ajustado para todos os outros termos modelo, diferentemente tipo (padrão na função anova()), onde adição de cada efeito é feita sequencialmente e depende de como os termos modelo são ordenados (Langsrud 2003).Analisando primeiro os valores de significância, fica claro que covariável prediz significativamente variável dependente (p-valor < 0,01). Portanto, variável gramas de espiga por parcela é influenciada pelo número de plantas por parcela. Da mesma forma, o efeito de tratamento também foi significativo.   \r\nFigure 10.8: Gráfico residual modelo ANCOVA obtido pela função autoplot().\r\nFigura abaixo obtida com função autoplot(), mostra 4 gráficos. Os dois primeiros são os mais importantes para nós aqui. O primeiro (Residual vs fitted) pode ser utilizado para identificar homogeneidade das variâncias. Uma distribuição aleatória dos pontos gráfico deve ser observada. Quando um padrão de distibuição é observado –como, por exemplo, distribuição dos pontos em forma de funil– uma investigação deve ser realizada, pois este padrão indica possiblidade de heterogeneidade das variâncias. O segundo gráfico (Normal Q-Q) nos informa quanto normalidade dos resíduos, ou seja, é desejado que os pontos sejam distribuídos ao redor da linha diagonal. Em nosso caso, os pontos foram uniformemente distribuídos, o que confirma o resultado teste estatístico observado anteriormente.utilização de gráficos residuais para identificar os pressupostos de modelos ainda não é muito difundida. entanto, estes gráficos apresentam muitas vantagens em relação aos métodos estatísticos, principalmente quando os testes são aplicados em conjuntos de dados com um alto tamanho de amostra. Uma ampla discussão comparando métodos estatísticos e gráficos na verificação de pressupostos dos modelos é apresentada por Kozak Piepho (2017).5. Homogeneidade dos slopes da regressãoVimos anteriormente que homogeneidade dos slopes é um pressuposto importante na ANCOVA. Para identificarmos isto em nosso exemplo, vamos criar um gráfico semelhate ao apresentado na figura 7.\r\nFigure 10.9: Regressões ajustadas para cada tratamento entre variável dependente e covariável. linha preta tracejada representa regressão geral. O slope desta regressão é utilizado para obtenção das médias ajustadas.\r\nO gráfico acima mostra uma regressão linear ajustada para cada tratamento (linhas coloridas) e uma regerssão linear geral (linha pontilhada) entre covariável e variável resposta. Observando cada reta ajustada, é razoável dizer que os slopes podem ser considerados homogêneos, ou seja, todos eles apresentam valor positivo. Um teste de hipótese pode ser utilizado para testarmos se os slopes podem ou não ser considerados homogêneos. Para isto, basta incluirmos o termo de interação entre covariável e o tratamento modelo ancova ajustado anterioremente. função update() pode ser utilizada para este fim, como segue:Como já suspeitávamos, interação covariável \\(\\times\\) tratamento não foi significativa, indicando homogeneidade dos slopes.Na função update() acima, o operador .~. significa “manter mesma variável resposta e preditores objeto ancova” e + NPUE:TRAT significa “adicionar o termo de interação ao modelo ancova”.6. Análises complementaresNos últimos cinco tópicos, vimos em detalhe os principais passos para o cálculo da ANCOVA. Conseguimos identificar que covariável influencia variável resposta, que ela é idependente dos nossos tratamentos e os slopes das regressões de cada tratamento são homogêneos. Duas etapas restam para nós agora: () identificar o quanto ganhamos –em termos de sucesso preditivo– considerando inclusão da covariável modelo; e (ii) quais são médias ajustadas apra cada tratamento. Uma maneira simples de identificar se inclusão da covariável melhorou predição modelo, é por meio da criação de um gráfico de dispersão com uma linha de referência 1:1 (valores preditos vs observados). Testes estatísticos de seleção de modelos –como, por exemplo, o AIC– também podem serem utilizados. Nesta etapa, vamos criar um novo dataframe chamado covar_pred, qual conterá, além dos dados originais, os valores preditos pela ANOVA e ANCOVA.  Neste gráfico, linha pontilhada representa linha de referência 1:1. Fica evidente ao observar o gráfico acima que utilização da covariável melhorou capacidade preditiva modelo, pois os valores preditos estavam mais próximos da linha de referência. Os valores de AIC também indicaram que o modelo ANCOVA foi mais preciso.Gráficos tipo 1:1 são úteis para identificar capacidade preditiva de modelos. Dois cuidados, entanto, precisam ser tomados. Primeiro, por ser um gráfico gráfico de dispersão com uma linha de referência 1:1, os eixos x e y devem estar na mesma escala. Segundo, é assumido que linha diagonal tem intercepto igual zero e slope igual um. Assim, um teste de hipotese para estes parâmetros pode ser útil.O próximo passo é obtermos médias de GE para cada tratamento ajustadas para um mesmo valor de NPUE. Considerando estas variáveis como Y e X, respectivamente, este valor é dado pela expressão:\\[\r\n\\hat m_i = \\bar{Y}_{.}- \\hat \\beta(\\bar{X}_{.}- \\bar X_{..})\r\n\\]Onde \\(\\bar{Y}_{.}\\) é média ajustada de Y -ésimo tratamento; \\(\\hat \\beta\\) é o slope da regressão linear estimada entre variável dependente e covariável; \\(\\bar{X}_{.}\\) é média da covariável para o -ésimo tratamento; e \\(\\bar{X}_{..}\\) é média geral da covariável. software R, médias ajustadas bem como os testes post hocs podem ser obtidos pelos seguintes códigos.figura acima mostra médias ajustadas para os dois métodos. barras azuis representam o intervalo de confiança 95% da média predita enquanto que setas vermelhas indicam comparação das médias pelo teste de Tukey. Cultivares com setas que não se sobrepõe apresentam médias significativamente diferentes. Note que amplitude intervalo de confiança é menor para o modelo da ANCOVA. Assim, vimos como este método pode ser útil na redução erro em análise de dados experimentais. Seguindo este exemplo, ANCOVA pode ser aplicada em experimentos onde uma potencial covariável está disponível, como por exemplo, severidade de ferrugem na folha observada em cada unidade experimental antes da aplicação de fungicida.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\n# dados\ncovar <- import(url, sheet = \"COVAR\")\n# duas primeiras colunas como fator\ncovar <- as_factor(covar, 1:2)\n\ncovar_ggplot <- \n  covar %>%\n  pivot_longer(names_to = \"variable\", values_to = \"val\", cols = c(NPUE, GE))\n# Estrutura dos dados\ncovar_ggplot\n# # A tibble: 48 x 4\n#    TRAT  BLOCO variable   val\n#    <fct> <fct> <chr>    <dbl>\n#  1 T1    1     NPUE        28\n#  2 T1    1     GE         202\n#  3 T1    2     NPUE        22\n#  4 T1    2     GE         165\n#  5 T1    3     NPUE        27\n#  6 T1    3     GE         191\n#  7 T1    4     NPUE        19\n#  8 T1    4     GE         134\n#  9 T2    1     NPUE        23\n# 10 T2    1     GE         145\n# # ... with 38 more rows\nmean_var <- \n  covar_ggplot %>%\n  group_by(variable) %>%\n  summarise(mean = mean(val))\n\nggplot(covar_ggplot, aes(x = TRAT, y = val)) +\n  geom_boxplot(fill = \"gray75\", width = 0.6) +\n  facet_wrap(~ variable, scales = \"free_y\") +\n  geom_hline(data = mean_var, aes(yintercept = mean), linetype = \"dashed\") +\n  stat_summary(fun = mean,\n               geom = \"point\",\n               shape = 23,\n               fill = \"red\")+\n  labs(x = \"Tratamentos\", y = \"valores observados\")\n# modelo ANOVA convencional para a covariável\nconvencional <- aov(NPUE ~  TRAT + BLOCO, data = covar)\n# Análise de variância\nanova(convencional) \n# Analysis of Variance Table\n# \n# Response: NPUE\n#           Df  Sum Sq Mean Sq F value Pr(>F)\n# TRAT       5  45.833  9.1667  1.2079 0.3524\n# BLOCO      3  21.667  7.2222  0.9517 0.4407\n# Residuals 15 113.833  7.5889\n# modelo ANOVA convencional\nancova <- aov(GE ~  TRAT + NPUE + BLOCO, data = covar)\n# Análise de variância\nAnova(ancova, type = 3)\n# Anova Table (Type III tests)\n# \n# Response: GE\n#             Sum Sq Df F value    Pr(>F)    \n# (Intercept)  169.9  1  1.7476  0.207376    \n# TRAT        3227.3  5  6.6381  0.002296 ** \n# NPUE        7391.0  1 76.0124 4.963e-07 ***\n# BLOCO       1502.4  3  5.1504  0.013158 *  \n# Residuals   1361.3 14                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nresiduals <- residuals(ancova)\n# normalidade dos resíduos\nshapiro.test(residuals)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals\n# W = 0.97894, p-value = 0.8755\n\n# Homogeneidade das variâncias\nleveneTest(residuals ~ TRAT, data = covar)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  5  1.1686 0.3624\n#       18\n\n# Teste de aditividade de Tukey\ntukey.add.test(covar$GE, covar$BLOCO, covar$TRAT)\n# \n# Tukey's one df test for additivity \n# F = 1.0000419   Denom df = 14    p-value = 0.3342722\n\n# interpretação gráfica\nautoplot(ancova) +\n  geom_point(col = \"black\") +\n  theme(text = element_text(size = 8),\n        axis.title = element_text(size = 8),\n        axis.text = element_text(size = 8),\n        aspect.ratio = 1)\nggplot(covar, aes(NPUE, GE, col = TRAT)) +\n  geom_point(aes(col = TRAT)) +\n  geom_smooth(aes(col = TRAT), method = \"lm\", se = F) +\n  geom_smooth(col = \"black\", linetype = \"dashed\", method = \"lm\", se = F)+\n  theme(legend.position = \"bottom\",\n        aspect.ratio = 1)+\n  guides(col = guide_legend(nrow = 1, byrow = TRUE))+\n  labs(x =\"Número de plantas por parcela\",\n       y = \"Gramas de espigas por parcela\")\n# Incluindo o termo de interação\nancova_int = update(ancova, .~. + NPUE:TRAT)\n# Análise de variância\nAnova(ancova_int, type = 3) \n# Anova Table (Type III tests)\n# \n# Response: GE\n#             Sum Sq Df F value    Pr(>F)    \n# (Intercept)  300.0  1  2.8462 0.1258641    \n# TRAT         597.9  5  1.1344 0.4084810    \n# NPUE        3906.0  1 37.0530 0.0001821 ***\n# BLOCO       1311.4  3  4.1467 0.0421214 *  \n# TRAT:NPUE    412.5  5  0.7827 0.5867580    \n# Residuals    948.7  9                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanovaa = aov(GE ~  TRAT + BLOCO, data = covar)\n\nancova_pred = covar %>% mutate(pred = predict(ancova), metodo = \"ANCOVA\")\nanova_pred =  covar %>% mutate(pred = predict(anovaa), metodo = \"ANOVA\")\npreditos = rbind(ancova_pred, anova_pred)\nAIC(anovaa, ancova)\n#        df      AIC\n# anovaa 10 229.6856\n# ancova 11 187.0242\n# gráfico 1:1\nggplot(preditos, aes(x = pred, y = GE))+\n  geom_point(aes(col = TRAT)) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  xlim(130, 270) +\n  ylim(130, 270) +\n  theme(aspect.ratio = 1,\n        panel.spacing = unit(0,\"cm\")) +\n    guides(col = guide_legend(nrow = 1, byrow = TRUE))+\n  theme(legend.position = \"bottom\", legend.title = element_blank())+\n    facet_wrap(~metodo)+\n  labs(x = \"Gramas de espigas estimado\",\n       y = \"Gramas de espiga observada \")\nmed_anova = emmeans(anovaa, ~ TRAT) # Média não ajustada\nmed_ancoova = emmeans(ancova, ~ TRAT) # Média ajustada para NPUE\nxlab = \"Gramas de espiga por parcela\"\nylab = \"Tratamento\"\nscale_x = scale_x_continuous(limits = c(140, 260))\np1 = plot(med_anova, comparisons = T, xlab = xlab, ylab = ylab) + scale_x\np2 = plot(med_ancoova, comparisons = T, xlab = xlab, ylab = ylab) + scale_x\nplot_grid(p1, p2, labels = c(\"ANOVA\", \"ANCOVA\"),\n          hjust = -1.5, vjust = 2.5, label_size = 8)"},{"path":"analdata.html","id":"general","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5 Análise de dados não gaussianos","text":"Dados com distribuições não normal são mais comuns que podemos imaginar. Na área agrícola, exemplos incluem percentagem de sementes que germinam (Binomial), contagem de ervas daninhas por parcela (Poisson), proporção de área foliar necrosada por uma determinada doença (Beta), escala de sintomas de uma determinada doença (Multinominal). Para todas distribuições, exceto normal, variância é dependente da média. Logo, assumindo efeito significativo dos tratamentos, o pressuposto da homogeneidade das variâncias será violado quando dados não normais são analizados (Stroup 2015). Neste momento, questão que surge é: como estes dados deveriam ser analizados? Antes de responder esta questão, vamos passar por uma breve história.","code":""},{"path":"analdata.html","id":"da-análise-de-variação-aos-modelos-lineares-mistos-generalizados","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5.1 Da análise de variação aos modelos lineares mistos generalizados","text":"(Fisher Mackenzie 1923), estudando variação de diferentes cultivares de batata, publicaram o primeiro trabalho demonstrando o uso da ANOVA para avaliação de experimentos agrícolas. O método da ANOVA se tornaria popular e amplamente utilizado após Fisher estabelecer bases teóricas e os pressupostos desta técnica (Fisher 1925, 1935). ideias de Fisher foram extendidas para experimentos mais complexos, mais tarde, por (Yates 1940). Até então, questão da ANOVA de dados não gaussianos parecia estar resolvida. O teorema central limite dava suporte tal análise. Mais tarde, (Bartlett 1947) demonstrou como transformações de dados não normais poderiam ser úteis para realização da ANOVA. Esta técnica permanece sendo utilizada até hoje.Nesta mesma época, terminologia “Modelos Mistos”  foi introduzida por (Eisenhart 1947). Mais tarde, (Henderson 1949); (Henderson 1950) propos equações modelo misto, das quais os BLUPs são soluções. Em 1953, este mesmo autor demonstrou os diferentes tipos de somas de quadrados (vistos na seção da análise de covariância) e descreveu alguns métodos para estimação dos componentes de variância em dados não ortogonais (Henderson 1953). Mais tarde (1971), estimação da Máxima Verossimilhança Restrita (REML) como um método para estimativa dos componentes de variância em um modelo com dados desbalanceados foi apresentada por (Patterson Thompson 1971).Um resumo até aquiO uso da ANOVA para análise de experimentos já estava consolidada;utilização de transformação de variáveis era uma técnica aceita para realização da ANOVA com dados não normais;teoria por tráz dos modelos mistos já era compreendida, entanto seu uso não era disseminado devido, principalmente, devido necessidade de operações matriciais complexas.(Nelder Wedderburn 1972) extenderam base modelo linear da ANOVA e regressão para acomodar suposições de probabilidade mais plausíveis aos dados observados, resultando nos conhecidos modelos lineares generalizados (GLM, generalized linear models). essência dessa generalização proporciona duas importantes mudanças nos pressupostos dos modelos: primeira é que os dados não necessariamente precisam ser normalmente distribuídos mas podem assumir qualquer distribuição da família exponencial de distribuições qual inclui, entre outras, distribuição Normal, Poisson e Binomial (Koopman 1936). segunda é que média não é necessariamente tomada como uma combinação linear de parâmetros, mas que alguma função da média é. Esta função é conhecida como função de ligação e nos GLMs relaciona média da variável resposta à combinação linear das variáveis explicativas (Nelder Wedderburn 1972). Semelhante evolução dos LMs para GLMs, os LMMs foram extendidos para modelos lineares mistos generalizados (GLMM, generalized linear mixed-effect model) por (Wolfinger O’connell 1993) e (Breslow Clayton 1993).","code":""},{"path":"analdata.html","id":"estratégias-para-análise-de-dados-não-gaussianos","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5.2 Estratégias para análise de dados não gaussianos","text":"Quando confrontados com dados não normais, maioria dos pesquisadores opta por uma das três opções seguir: () confiar na robustez da ANOVA clássica à pequenos desvios de normalidade em ensaios balanceados (Blanca et al. 2017) e realizá-la sem nenhum peso na consciência; (ii) realizar alguma transformação na variável e realizar ANOVA com dados transformados; (iii) utilizar algum teste não paramétrico. segunda opção parece ser mais utilizada. Variáveis são transformadas para que os pressupostos de normalidade e homogeneidade das variâncias seja cuprido –ou ao menos aproximado– (Bartlett 1947).O uso de testes não paramétricos também pode ser uma alternativa para análise de dados não normais. Os testes de Friedman (Friedman 1937) e de Kruskal & Wallis (Kruskal Wallis 1952) são alternativas não paramétricas para análise de variância nos delineamentos DBC e DIC (unifatoriais), respectivamente. aplicação destes testes não será o foco aqui, principalmente devido limitação com relação complexidade design experimental suportada por estes testes. Uma excelente aplicação prática, entanto, pode ser vista em (Field, Miles, Field 2012), pp. 653.Vale ressaltar que escolha por estes “atalhos”, entanto, nem sempre é solução definitiva. Dados de contagem com muitos valores zero não podem ser normalizados por transformação. Os testes não paramétricos, ao contrário de como muitos pensam, requerem, sim, alguns pressupostos. Por exemplo, o teste de Freedman é flexivel quando não normalidade mas só é valido se distribuição da variável resposta entre os grupos mesma em todos os outros aspectos (variância, assimetria, curtose). Quando estes pressupostos não são cumpridos, o poder teste é reduzido (Laurent Turk 2013). Assim, ao vez de encaixar os dados em estatísticas clássicas, os pesquisadores devem utilizar abordagens estatísticas que correspondam aos seus dados (Mora et al. 2008). Em casos de variáveis não normais, mais indicadas são os GLMs (Nelder Wedderburn 1972) ou GLMMs (Wolfinger O’connell 1993).Hábitos de aprendizado de que ANOVA pode ser aplicada diretamente dados transformados não ajudam –e muitas vezes impedem– rápida expansão uso de modelos mais sofisticados como os GLM(M)s. O uso de GLM(M)s, entanto, requer um aprendizado considerável. Dependendo da aplicação, subida pode ser íngreme. Apresentar uma introdução ao uso dos GLM(M)s em R para análise de dados não gaussianos e compará-los com os procedimentos tradicionais é nosso principal objetivo aqui.","code":""},{"path":"analdata.html","id":"introdução-aos-modelos-lineares-generalizados","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5.3 Introdução aos modelos lineares generalizados","text":"Vamos considerar um simples experimento que comparou eficiência de um herbicida pré-emergente controle de uma determinada erva daninha em comparação com um tratamento controle. O ensaio foi realizado em um delineamento DIC com oito repetições, sendo cada repetição caracterizada por um vaso. Logo, o ensaio foi composto por 16 vasos. Em cada vaso foram semeadas 50 sementes da determinada erva daninha e após um período previamente especificado, o número de sementes germinadas foi observado. Assumindo que germinação de cada semente é um processo independente, variável resposta em cada unidade experimental (\\(Y_{ij}\\)) tem uma distribuição binomial com N = 50 e probabilidade de germinação de \\(\\pi_{ij}\\) para o -ésimo tratamento na j-ésima repetição.Intuitivamente, primeira opção de muitos pesquisadores para analisar estes dados seria iniciar com uma aproximação normal. Com N = 50 –e considerando p = 0,5– mesmo dados binomiais apresentam boa aproximação normal. O modelo com aproximação normal seria:Variavel resposta: \\(p_{ij} = y_{ij}/50\\), onde \\(y_{ij}\\) é o número de sementes germinadas -ésimo tratamento na j-ésima repetiçãoVariavel resposta: \\(p_{ij} = y_{ij}/50\\), onde \\(y_{ij}\\) é o número de sementes germinadas -ésimo tratamento na j-ésima repetiçãoO modelo tradicional é então \\(p_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}\\) onde \\(\\mu\\) é média geral; \\(\\tau_i\\) é o efeito -ésimo tratamento e \\(\\varepsilon_{ij}\\) é o erro aleatório assumido \\(..d \\sim N(0, \\sigma^2 )\\).O modelo tradicional é então \\(p_{ij} = \\mu + \\tau_i + \\varepsilon_{ij}\\) onde \\(\\mu\\) é média geral; \\(\\tau_i\\) é o efeito -ésimo tratamento e \\(\\varepsilon_{ij}\\) é o erro aleatório assumido \\(..d \\sim N(0, \\sigma^2 )\\).Diferentemente modelo da ANOVA tradicional apresentado acima onde uma única equação é considerada, os GLMs são montados basicamente em três etapas (Stroup 2013):distribuição das observações: Em nosso exemplo, assumimos que \\(y_{ij} \\sim B(50, \\pi_i)\\);distribuição das observações: Em nosso exemplo, assumimos que \\(y_{ij} \\sim B(50, \\pi_i)\\);O preditor linear: Em nosso exemplo \\(\\eta_{ij} = \\eta + \\tau_{}\\). Aqui, \\(\\eta\\) é média geral e \\(\\tau_{}\\) é o efeito tratamento. Uma sutil mas importante diferença é que aqui o residual (\\(\\varepsilon_{ij}\\)) não é considerado.O preditor linear: Em nosso exemplo \\(\\eta_{ij} = \\eta + \\tau_{}\\). Aqui, \\(\\eta\\) é média geral e \\(\\tau_{}\\) é o efeito tratamento. Uma sutil mas importante diferença é que aqui o residual (\\(\\varepsilon_{ij}\\)) não é considerado.função de ligação: Foi mensionado anteriormente que nos GLMs média não é necessariamente tomada como uma combinação linear de parâmetros, mas que alguma função da média é. Dados de distribuição binomial são geralmente melhor ajustados utilizando o parâmetro canônico \\(log[\\pi/(1-\\pi)]\\) (função da média) que média em si. Esta função é função de ligação que relaciona distribuição das observações com o preditor linear. em nosso caso, \\(\\eta_{ij} = log[\\pi_{}/(1-\\pi_{})]\\). Assim, quando os parâmetros modelo são ajustados não é média que é estimada, mas função de ligação. Neste caso, estamos interessados em estimar probabilidade de sucesso (germinação) para cada tratamento, então \\(\\hat \\pi_{} = 1/(1 + e ^ {-\\hat \\eta_{}})\\).função de ligação: Foi mensionado anteriormente que nos GLMs média não é necessariamente tomada como uma combinação linear de parâmetros, mas que alguma função da média é. Dados de distribuição binomial são geralmente melhor ajustados utilizando o parâmetro canônico \\(log[\\pi/(1-\\pi)]\\) (função da média) que média em si. Esta função é função de ligação que relaciona distribuição das observações com o preditor linear. em nosso caso, \\(\\eta_{ij} = log[\\pi_{}/(1-\\pi_{})]\\). Assim, quando os parâmetros modelo são ajustados não é média que é estimada, mas função de ligação. Neste caso, estamos interessados em estimar probabilidade de sucesso (germinação) para cada tratamento, então \\(\\hat \\pi_{} = 1/(1 + e ^ {-\\hat \\eta_{}})\\).Dois exemplos numéricos serão implementados. O primeiro, considera análise de dados de proporção discreta. O segundo é voltado para análise de dados de contagem. Os dados utilizados são oriundos de um ensaio com nove cultivares de soja conduzidas em um delineamento DBC com quatro blocos. Em cada bloco, diversas variáveis foram mensuradas em 10 plantas aleatoriamente selecionadas. Para fins didáticos, somente variáveis de interesse serão utilizadas aqui. () proporção de legumes viáveis, obtido pela razão entre o número de legumes viáveis (com ao menos um grão) e o número de legumes total das 10 plantas. (ii) o número total de legumes das 10 plantas que continha quatro grãos. Para cada exemplo, análise será realizada considerando uma ANOVA normal, utilizando uma transformação indicada e utilizando um modelo misto generalizado.","code":""},{"path":"analdata.html","id":"dados-de-proporção","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5.4 Dados de proporção","text":"Por definição, proporção de legumes viáveis observado na j-ésima repetição -ésimo tratamento (\\(p_{ij}\\)) tem uma distribuição binomial onde \\(p_{ij} \\sim B(N, \\pi_{ij})\\), onde \\(\\pi_{ij}\\) denota probabilidade de uma observação aleatória tratamento bloco j apresentar característica de interesse – neste caso legume ser viável. Neste caso, o objetivo é estimar probabilidade de cada tratamento e utilizando testes de hipóteses compará-las. O valor esperado para o tratamento é dado então por \\(N\\pi_i\\) com variância \\(N\\pi_i(1-\\pi_i)\\).Mesmo em experimentos com tamanho de amostra grande, diferenças significativas \\(\\pi_i\\) resultarão em variãncias heterogêneas, violando o pressuposto da ANOVA. Dados oriundo de proporções (\\(p_{ij}\\)) são frequentemente transformados para escala \\(P^*_{ij} = sen^{-1} \\sqrt{P_{ij}}\\) (Rodrigues-Soares et al. 2018).Exemplo numéricoPara o exemplo, os dados de proporção de legumes viáveis descrito anteriormente serão utilizados. figura 25 mostra proporção de legumes viáveis para cada tratamento bem como distribuição dos pontos em torno de uma linha de probabilidade normal esperada.\r\nFigure 10.10: Proporção de legumes viáveis em nove cultivares de soja () e gráfico Q-Q plot (b).\r\nmédia número de legumes viáveis observado foi de 0.89 (89%). cultivar C1 apresenta um valor ligeiramente menor que outras cultivares. Em adição, distribuição desta variável parece seguir uma distribuição normal; entanto, este gráfico só deve ser utilizado para fins de exploração dos dados. inferência quanto normalidade deve ser realizada nos resíduos.Os códigos seguintes são utilizados para realizar análise considerando os três métodos. O primeiro modelo armazenado objeto convencional é ajustado sem nenhuma transformação, onde variável respota (NLV/NLT) é dada em função ~ fator tratamento (CULTIVAR) + bloco (REP). O segundo modelo armazenado objeto transform é ajustado com variável resposta transformada para o arcoseno da raiz quadrada da proporção (asin(sqrt(NLV/NLT))). O terceiro modelo armazenado objeto general é ajustado considerando um modelo linear misto generalizado considerando o fator bloco aleatório. Para implementação deste modelo considerou-se o seguinte:distribuição das observações: \\(y_{ij}|\\beta_j \\sim B(N, \\pi_{ij})\\), onde N é o número de legumes total em cada bloco, e \\(\\pi_{ij}\\) é probabilidade da ocorrência de legumes viáveis para o -ésimo tratamento j-ésimo bloco.distribuição das observações: \\(y_{ij}|\\beta_j \\sim B(N, \\pi_{ij})\\), onde N é o número de legumes total em cada bloco, e \\(\\pi_{ij}\\) é probabilidade da ocorrência de legumes viáveis para o -ésimo tratamento j-ésimo bloco.O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\). Note que adição efeito de bloco como aleatório torna nosso modelo um modelo linear misto generalizado. Este efeito foi considerado aleatório, principalmente pela ocorrência de diferentes números de legumes observado em cada bloco. (Stroup 2015) sugere fortemente considerar o efeito de bloco aleatório nestas condições.O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\). Note que adição efeito de bloco como aleatório torna nosso modelo um modelo linear misto generalizado. Este efeito foi considerado aleatório, principalmente pela ocorrência de diferentes números de legumes observado em cada bloco. (Stroup 2015) sugere fortemente considerar o efeito de bloco aleatório nestas condições.função de ligação: \\(\\eta_{ij}\\) = Logit(\\(\\pi_{ij}\\)) = \\(log[\\pi_{ij}/(1-\\pi_{ij})]\\)função de ligação: \\(\\eta_{ij}\\) = Logit(\\(\\pi_{ij}\\)) = \\(log[\\pi_{ij}/(1-\\pi_{ij})]\\) Os modelos ajustados foram salvos em seus respectivos objetos. Neste momento, uma investigação quanto aos pressuposstos destes modelos é necessária. funções shapiro.test() e leveneTest() são utilizadas para análise de normalidade e homogeneidade dos resíduos, respectivamente.Nada parece haver de errado com nossos modelos até aqui. De fato, à 5% de erro, tanto os resíduais modelo convencional quanto transform são considerados homocedásticos e normalmente distribuídos. Vamos, agora, uma abordagem gráfica.Testes de hipótese -incluíndo os testes de normalidade e homogeneidade- são sensíveis ao tamanho da amostra. Assim, quanto menor o tamanho da amostra (em nosso caso 27), menor é probabilidae de rejeição da hipótese \\(h_o\\) (resíduos normais). Por outro lado, em tamanhos de amostra grande, probabilidade de rejeição da hipótese \\(h_o\\) é maior. Este assunto é bem discutido por Kozak Piepho (2017), quais demonstraram que gráficos residuais são melhores que os testes estatísticos para verificar pressuposições da ANOVA.  \r\nFigure 10.11: Gráficos Q-Q plot dos resíduos obtidos na análise da proporção de legumes viáveis.\r\nConclusão até aqui: baseado em testes de aderência à normalidade e na interpretação gráfica, os pressupostos modelo da ANOVA convencional foram cumpridos. Então, é justo perguntar: Por que utilizar um modelo mais “complicado” se há evidências de que ANOVA nos dados transformados (ou mesmo nos dados originais) não trará maiores problemas? Antes de qualquer conclusão vamos observar os resultados dos três modelos.O código abaixo é utilizado para obter médias marginais dos modelos ajustados. Na ANOVA com os dados transformados e modelo generalizado, todas inferências são realizadas na escala transfromada mas médias são apresentadas na escala original. \r\nFigure 10.12: Médias estimadas, intervalos de confiança e comparação de médias para os modelos da ANOVA tradicional (), com dados transformados (b) e generalizado (c) para proporção de legumes viáveis.\r\nComparando os resultados dos três modelos na mesma escala, fica fácil observar que o modelo generalizado apresentou um intervalo de confiança das médias (barra azul) menor quando comparado com os procedimentos tradicionais. seta vermelha representa comparação das médias pelo teste de Tukey 5% de erro. Tratamentos com setas que não se sobrepõe diferem estatisticamente considerando probabilidade de erro.Neste ponto identificamos limitações modelo convencional da ANOVA. transformação indicada não ajudou muito. Todos os indícios levavam acreditar que não teríamos maiores problemas realizando ANOVA com os dados transformados –ou mesmo originais– devido boa aproximação normal dos erros. Estes modelos, entanto, indicaram que diferenças observadas entre médias fora resultado acaso, o que parece não fazer muito sentido quando observamos figura 30. Neste caso, o pesquisador reportaria estes resultados em seu artigo/relatório e apresentaria alguma justificativa para este “fracasso”. O que precisamos compreender, entanto, é que mesmo com uma boa aproximação normal dos erros, os dados ainda continuam sendo dados binomias! Se observarmos conjunto de dados veremos que cada bloco possui um número diferente de legumes totais. Ao considerar proporção de legumes viáveis e análisar essa variável, ignoramos completamente o N em nossa amostra. Poucos se dão conta, mas proporção de 0.5 de legumes viáveis é mesma considerando 5 legumes viáveis de um total de 10 legumes e 500 legumes viáveis de um total de 1000 legumes. precisão, entanto é diferente. O modelo generalizado aplicado neste exemplo considera esta diferença.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nSOJA <- import(url, sheet = \"SOJA\")\nmedlvia <- mean(SOJA$NLV / SOJA$NLT) # Média de legumes viáveis\n\n# Explorando os dados\np1 <- ggplot(SOJA, aes(x = CULTIVAR, y = NLV/NLT)) + \n             geom_boxplot(fill = \"gray\") +\n             geom_hline(yintercept = medlvia, linetype = \"dashed\")+\n             stat_summary(fun = mean, geom = \"point\", shape = 23, fill = \"red\") +\n             labs(x = \"Cultivares\", y = \"Taxa de legumes viáveis\")\np2 <- ggplot(SOJA, aes(sample = NLV/NLT))+\n             qqplotr::stat_qq_line(col = \"red\")+\n             qqplotr::stat_qq_point()+\n             qqplotr::stat_qq_band(alpha = 0.2)+\n             labs(x = \"Theoretical quantiles\", y = \"Sample quantiles\")\nplot_grid(p1, p2, labels = c(\"(a)\", \"(b)\"),\n          label_size = 10, vjust = 3, hjust = -5)\nconvencional = lm(NLV/NLT ~ CULTIVAR + REP, data = SOJA)\ntransform = lm(asin(sqrt(NLV/NLT)) ~ CULTIVAR + REP, data = SOJA)\ngeneral = glmer(cbind(NLV, NLT-NLV) ~ CULTIVAR + (1|REP),\n                family = binomial,\n                data = SOJA)\nshapiro.test(residuals(convencional))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(convencional)\n# W = 0.93971, p-value = 0.1198\nshapiro.test(residuals(transform))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(transform)\n# W = 0.95492, p-value = 0.2814\nshapiro.test(residuals(general, type = \"deviance\"))\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuals(general, type = \"deviance\")\n# W = 0.95317, p-value = 0.2557\nleveneTest(convencional$residuals ~ SOJA$CULTIVAR)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  8  0.0925 0.9991\n#       18\nleveneTest(transform$residuals ~ SOJA$CULTIVAR)\n# Levene's Test for Homogeneity of Variance (center = median)\n#       Df F value Pr(>F)\n# group  8  0.1512 0.9948\n#       18\nres <- tibble(Convencional = residuals(convencional),\n              Transformado = residuals(transform),\n              Generalizado = residuals(general, type = \"deviance\")) %>%\n       gather()\nggplot(res, aes(sample = value)) +\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\n       qqplotr::stat_qq_band(alpha = 0.2)+\n       facet_wrap(~key, scales = \"free\")+\n       labs(x = \"Quantis teóricos\", y =  \"Quantis observados\")\nmed_conv <- emmeans(convencional, ~ CULTIVAR,  type = \"response\")\nmed_trans <- emmeans(transform, ~ CULTIVAR, type = \"response\")\n# Warning in (function (object, at, cov.reduce = mean, cov.keep = get_emm_option(\"cov.keep\"), : There are unevaluated constants in the response formula\n# Auto-detection of the response transformation may be incorrect\nmed_gene <- emmeans(general, ~ CULTIVAR, type = \"response\")\n# gráficos para as médias\nscale_x <- scale_x_continuous(limits = c(0.75, 1))\nxlab <- \"Proporção de legumes viáveis\"\np3 <- plot(med_conv, comparisons = T, xlab = xlab) + scale_x\np4 <- plot(med_trans, comparisons = T, xlab = xlab) + scale_x\n# Note: Use 'contrast(regrid(object), ...)' to obtain contrasts of back-transformed estimates\np5 <- plot(med_gene, comparisons = T, xlab = xlab) + scale_x\nplot_grid(p3, p4, p5, ncol = 3, \n          labels = c(\"(a)\", \"(b)\", \"(c)\"), hjust = -2.5)"},{"path":"analdata.html","id":"dados-de-contagem","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.5.5 Dados de contagem","text":"Dados de contagem são muito comuns em experimentos agronômicos, tendo propriedade de serem não negativos e inteiros. Em probabilidade, estes tipos de dados seguem uma distribuição Poisson (Cochran 1940). exemplo de (Zoz et al. 2018), dados discretos (\\(D_{ij}\\)) são frequentemente transformados para escala \\(D^*_{ij} = \\sqrt{D_{ij}}\\) ou para \\(D^*_{ij} = \\sqrt{D_{ij}+0,5}\\) quando há um elevado número de zeros presente.Exemplo numéricoPara este exemplo, utilizaremos variável número de legumes com quatro grãos (NL4G) conjunto de dados SOJA. Os códigos abaixo são utilizado para exploração dos dados.  \r\nFigure 10.13: Número de legumes com quatro grãos em nove cultivares de soja () e gráfico Q-Q plot (b)\r\nmédia número de legumes viáveis observado foi aproximadamente três legumes. O gráfico Q-Q nos ajuda compreender que distribuição desta variável claramente não é normal. Uma grande quantidade de zero é observada, o que é lógico, pois existem cultivares com característica de não apresentar legumes com quatro grãos.Os códigos seguintes são utilizados para realizar análise considerando os três modelos. O primeiro modelo armazenado objeto convencional_count é ajustado sem nenhuma transformação, onde variável respota NL4G é dada em função (~) fator tratamento (CULTIVAR) \\(+\\) bloco (REP). O segundo modelo armazenado objeto transform_count é ajustado com variável resposta transformada para raiz quadrada número de legumes com quatro grãos (sqrt(NL4G)). O terceiro modelo armazenado objeto general_count é ajustado considerando um modelo linear misto generalizado com blocos aleatórios e considerando que os dados seguem uma distribuição Poisson. Para implementação deste modelo considerou-se o seguinte:distribuição das observações: \\(y_{ij}|\\beta_j \\sim P(\\lambda_{ij})\\).O preditor linear: \\(\\eta_{ij} = \\eta + \\tau_{} + \\beta_j\\), onde \\(\\beta_j \\sim N(0, \\sigma^2_b)\\).função de ligação: \\(\\eta_{ij}\\) = Log(\\(\\lambda_{ij}\\)) Conforme o primeiro exemplo, vamos realizar um diagóstico gráfico dos resíduos para os modelos ajustados.\r\nFigure 10.14: Gráficos Q-Q plot dos resíduos\r\nO gráfico Q-Q para estes modelos indicou que os resíduos são distribuídos normalmente, considerando o intervalo de confiança. transformação novamente não ajudou muito aqui. Embora não faça lógica testar normalidade dos resíduos modelo generalizado (visto que assumimos uma distribuição Poisson para os dados), o gráfico Q-Q mostra que os resíduos deste modelo se aproximou mais de uma distribuição normal comparado com os outros dois métodos.Exercício 10\r\nUtilize o teste de Shapiro-Wilk para testar hipótese de normalidade dos resíduos dos modelos ajustados anteriormente.RespostaTodos os três modelos indicaram diferenças significativas (5% de erro) para médias das cultivares. Para ANOVA convencional, o erro padrão para todos os tratamentos foi 0.94, o que não pode ser verdade porque variância e, portanto, os erros padrão, devem ser uma função da média em dados de contagem.\r\nFigure 10.15: Médias estimadas, intervalos de confiança e comparação de médias para os modelos da ANOVA tradicional (), com dados transformados (b) e generalizado (c) para o número de legumes com 4 grãos.\r\nPara ANOVA convencional e com dados transformados, o intervalo de confiança de 95% para média dos tratamentos apresentou limite inferior negativo para cultivares C1 e C9. Isto é ilógico, pois não podemos observar número de legumes com quatro grãos negativo. Para o modelo generalizado isto não foi observado. Novamente aqui identificamos limitação das técnicas convencional para análise de dados não normais. (Stroup 2013) demonstrou que em alguns casos, o uso de transformações pode ser mais impreciso que ANOVA aplicada diretamente aos dados não transformados. Observamos o mesmo aqui.utilização dos modelos generalizados requer certos cuidados, tais como definição da distribuição da variável à nível de observação e função de ligação ideal ser utilizada. Um resumo com principais funções utilizadas pode ser visto em (Stroup 2015). extensão dos modelos lineares generalizados para os modelo lineares mistos generalizados permite ainda contemplar importantes aspectos na avaliação de ensaios multi-ambientes , tais como modelagem da matriz de variância-covariância. O uso de modelos mistos também é fortemente recomendado em ensaios com delineamentos mais complexos, tais como parcelas sub-divididas ou sub-sub-divididas (Piepho Edmondson 2018).Procedimentos que contemplam análise considerando GLMs e GLMMs estão disponíveis nos principais softwares estatísticos. O PROC GLIMMIX e PROC GENMOD SAS, e função glmer() pacotes lme4 R podem ser utilizadas para implementação destes modelos.","code":"\n\nmednl4g <- mean(SOJA$NL4G)\n# Explorando os dados\np1 <- ggplot(SOJA, aes(x = CULTIVAR, y = NL4G)) + \n             geom_boxplot(fill = \"gray\") +\n             geom_hline(yintercept = mednl4g, linetype = \"dashed\") +\n             stat_summary(fun = mean, geom = \"point\",\n                         shape = 23, fill = \"red\") +\n     labs(x = \"Cultivares\", y = \"Número de legumes com quatro grãos\")\np2 <- ggplot(SOJA, aes(sample = NL4G)) +\n             qqplotr::stat_qq_line(col = \"red\") +\n             qqplotr::stat_qq_point() +\n             qqplotr::stat_qq_band(alpha = 0.2)+\n      labs(x = \"Quantis teóricos\", y =  \"Quantis observados\")\nplot_grid(p1, p2, labels = c(\"(a)\", \"(b)\"),\n                   label_size = 10, vjust = 3, hjust = -5)\nconvencional_cont <- lm(NL4G ~ CULTIVAR + REP, data = SOJA)\ntransform_cont <- lm(sqrt(NL4G) ~ CULTIVAR + REP, data = SOJA)\ngeneral_cont <- glmer(NL4G ~ CULTIVAR + (1|REP),\n                     family = poisson,\n                     data = SOJA)\nres <- tibble(Convencional = residuals(convencional_cont),\n              Transformado = residuals(transform_cont),\n              Generalizado = residuals(general_cont, type = \"deviance\")) %>%\n       gather()\n\nggplot(res, aes(sample = value)) +\n       qqplotr::stat_qq_line(col = \"red\")+\n       qqplotr::stat_qq_point()+\n       qqplotr::stat_qq_band(alpha = 0.2)+\n       facet_wrap(~key, scales = \"free\")\n\n# Médias ajustadas do número de grãos\nmed_conv_cont <- emmeans(convencional_cont, ~ CULTIVAR, type = \"response\")\nmed_trans_cont <- emmeans(transform_cont, ~ CULTIVAR, type = \"response\")\nmed_gene_cont <- emmeans(general_cont, ~ CULTIVAR, type = \"response\")\nscale_x <- scale_x_continuous(limits = c(-2, 15))\nxlab <- \"Número de legumes com 4 grãos\"\np7 <- plot(med_conv_cont, comparisons = T, xlab = xlab) + scale_x\np8 <- plot(med_trans_cont, comparisons = T, xlab = xlab) + scale_x\n# Note: Use 'contrast(regrid(object), ...)' to obtain contrasts of back-transformed estimates\np9 <- plot(med_gene_cont, comparisons = T, xlab = xlab) + scale_x\nplot_grid(p7, p8, p9, ncol = 3, \n          labels = c(\"(a)\", \"(b)\", \"(c)\"),\n          hjust = -2.5)"},{"path":"analdata.html","id":"efat","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6 Experimentos bifatoriais","text":"Experimentos fatoriais são muito comuns nas ciências agrárias, pois permitem o estudo de dois ou mais fatores em um mesmo experimento. Diversas são vantagens em se conduzir um experimento deste tipo. Dentre elas, podemos citar redução de custos, quando comparado à realizar um experimento para cada fator, otimização da área experimental e dos tratos culturais, bem como possibilidade de identificar o efeito de dois ou mais fatores sobre magnitude da variável resposta. Esta é, talvez, principal vantagem destes experimentos. Ao memo tempo, entanto, é fonte de um dos maiores desafios encontrados meio acadêmico. O surgimento de uma terceira fonte de variação, conhecida por interação. Vamos considerar como exemplo, um experimento que avaliou influencia de dois fatores, digamos \\(\\alpha\\) e \\(\\tau\\), em uma determinada variável resposta. O modelo estatístico considerado neste tipo de experimento é:\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + {\\rm{ }}\\mathop \\beta \\nolimits_{k}  + \\mathop \\alpha \\nolimits_i  + \\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) é o valor observado da combinação -ésimo nível fator \\(\\alpha\\) com o j-ésimo nível fator \\(\\tau\\) k-ésimo bloco; \\(\\mu\\) é média geral; \\(\\mathop \\beta \\nolimits_{k}\\) é o efeito bloco k; \\(\\mathop \\alpha \\nolimits_i\\) é o efeito -ésimo nível de \\(\\alpha\\) ; \\(\\mathop \\tau \\nolimits_j\\) é o efeito j-ésimo nível de \\(\\tau\\) ; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) é o efeito da interação -ésimo nível de \\(\\alpha\\) com o j-ésimo nível de \\(\\tau\\); e \\(\\mathop \\varepsilon \\nolimits_{ijk}\\) é o erro aleatório associado \\({y_{ijk}}\\), assumindo \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits^2 )\\).Basicamente, estes fatores podem ser divididos em dois tipos: qualitativos  e quantitativos . Um fator qualitativo é, como o nome já diz, relacionado qualidade, ou seja, diferentes em tipo mas não em quantidade. Como exemplo, podemos citar cultivares, defensivos agrícolas, práticas de manejo, etc. Um fator quantitativo, por outro lado, é caracterizado pela quantidade utilizada experimento. Podemos citar, por exemplo, doses de adubação. Cabe ressaltar que o termo fatorial não indica um delineamento experimental, mas uma forma de arranjo de tratamentos na área parcela. Estes experimentos podem ser conduzidos tanto em DIC  quanto DBC . Assim, em cada repetição/bloco, o tratamento ser aplicado é combinação dos níveis dos dois fatores. combinação de diferentes tipos de fatores não influenciará análise de variância dos dados, entanto resultará em algumas particularidades nas análises complementares, como será visto ao longo desta seção.","code":""},{"path":"analdata.html","id":"download-dos-dados","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.1 Download dos dados","text":"Nesta seção, serão analisados dados de experimentos bifatoriais com diferentes combinações de fatores qualitativos e quantitativos na presença de interação significativa e não significativa. Em todos os exemplos, será considerado o delineamento de blocos completos casualizados, tendo como variável resposta, o rendimento de grãos (RG). Para isto, utilizaremos cinco conjuntos de dados que serão detalhados seguir.Nesta seção, utilizaremos funções dos pacotes metan34 e ExpDes.pt  (Ferreira, Cavalcanti, Nogueira 2018) para análise estatistica dos dados. O pacote ExpDes.pt contém funções úteis para análise de variância de experimentos nos delineamentos experimentais e arranjo de tratamentos mais conhecidos. função pacote ExpDes.pt utilizada neste exemplo será fat2.dbc()  qual analisa dados de experimentos bifatoriais em delineamento de blocos completos casualizados. Uma explicação detalhada será dada agora ao passo em que nos outros experimentos serão apresentadas apenas linhas de comandos e uma breve discussão.delcaração dos argumentos na função fat2.dbc() é simples. Basta informarmos o nome das colunas nosso arquivo correspondentes aos argumentos requeridos pela função. Por exemplo fator1 e fator2, represetam os fatores em em estudo, que, neste caso, são híbridos e fontes de nitrogênio. argumento bloco informamos o nome da coluna de nossos dados correspondente aos blocos. O mesmo para o argumento resp que é variável resposta, caso, o rendimento de grãos.Definidos entrada de dados, precisamos declarar quais análises complementares serem realizadas, em caso de significancia estatística. argumento quali, informamos qual o tipo de fator em estudo. declaração é um vetor lógico de comprimento 2. Em nosso caso, ao declararmos quali = c(TRUE, TRUE) (padrão) estamos informando que o ambos os fatores HÍBRIDO e FONTEN são qualitativos. O argumento mcomp é utilizado para informar o teste de comparação (ou agrupamento ) de médias utilizado. Em nosso exemplo, vamos utilizar o teste Tukey (padrão). argumento fac.names é possivel informar nomes específicos para os fatores. Os argumentos sigF e sigT são utilizados para informar probabilidade de erro assumida na análise de variância e nas analises de comparação de médias, respectivamente. Ambos tem padrão 5%.O resultado da função fat2.dbc() irá depender da significância das fontes de variação experimento. Em caso de interação não significativa  função realiza uma comparação de médias para efeitos qualitativos e ajusta modelos de regressão polinomial para fatores quantitativos (quando significativos). Em experimentos com dois fatores qualitativos, em caso de interação significativa, função realiza o desdobramento das médias dos fatores. Em caso de um fator qualitativo e outro quantitativo, em caso de interação significativa função realiza comparação das médias fator qualitativo para cada nível fator quantitativo, bem como ajusta uma regressão para cada nível fator qualitativo.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nFAT1_CI <- import(url, sheet = \"FAT1_CI\")\nFAT1_SI <- import(url, sheet = \"FAT1_SI\")\nFAT2_CI <- import(url, sheet = \"FAT2_CI\")\nFAT2_SI <- import(url, sheet = \"FAT2_SI\")\nFAT3 <- import(url, sheet = \"FAT3\")"},{"path":"analdata.html","id":"qualitativo-vs-qualitativo","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.2 Qualitativo vs qualitativo","text":"","code":""},{"path":"analdata.html","id":"sem-interação-significativa","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.2.1 Sem interação significativa","text":"Como exemplo de análise de um experimento bifatorial com dois fatores qualitativos sem interação  significativa, utilizaremos o conjunto de dados FAT1_SI. Já sabemos que interação não será significativa neste exemplo. Os dois próximos gráficos nos ajudam compreender porque.\r\nFigure 10.16: Característica da produção em um experimento bifatorial sem interação significativa\r\nÉ possível identificar que o magnitude da variável resposta de um fator não é alterada pelo nível outro fator. Por exemplo, nota-se que o híbrido NUPEC_2 parece ter uma média maior que os outros híbridos, independentemente da fonte de nitrogênio utilizada. mesmo modo, fonte de nitrogênio AmonioA parece proporcionar maior produtividade, independentemente híbrido testado. Estas afirmações, entanto, só poderão ser confirmadas pela análise de variância e posterior comparação de médias.Gráficos iterativos podem ser obtidos utilizando função ggplotly() pacote plotly. Este pacote converte um objeto ggplot2 em um gráfico iterativo que é apresentado na aba viewer ambiente de trabalho RStudio.Análise de variânciaPlotagem das médias considerando os efeitos principaisConforme já sabido, interação HIBRIDO x FONTEN não foi significativa 5% de probabilidade de erro. Assim, o procedimento ser realizado é comparação das médias para os fatores principais apenas. Os dois gráficos abaixo mostram médias dos fatores principais. função plot_bars() utilizada para confeccionar os gráficos desta seção retorna análise descritiva para os fatores considerados se o argumento verbose =  TRUE adicionado na função.","code":"\nhf3 <- plot_factbars(FAT1_SI,\n                     FONTEN,\n                     HIBRIDO,\n                     resp = RG)\nhf4 <- plot_factbars(FAT1_SI,\n                     FONTEN,\n                     HIBRIDO,\n                     resp = RG,\n                     invert = TRUE)\n\nplot_grid(hf3, hf4, labels = c(\"hf3\", \"hf4\"))\nwith(FAT1_SI,\n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  FONTEN,\n              bloco = BLOCO,\n              resp =  RG,\n              fac.names = c(\"HIBRIDO\", \"FONTEN\")))\nhf5 <- plot_bars(FAT1_SI, HIBRIDO, RG, lab.bar = c(\"c\", \"a\", \"b\"), values = TRUE)\nhf6 <- plot_bars(FAT1_SI, FONTEN, RG, lab.bar = c(\"a\", \"b\", \"b\"), values = TRUE)\nplot_grid(hf5, hf6, labels = c(\"hf5\", \"hf6\"))"},{"path":"analdata.html","id":"com-interação-significativa","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.2.2 Com interação significativa","text":"O conjunto de dados utilizado neste exemplo será o FAT1_CI. análise de variância é realizada utilizando mesma função anterior.Análise de variânciaComo interação  foi significativa, o próximo passo é comparação das médias considerando os efeitos da interação. Os valores das médias fixando os níveis de cada fator são, por padrão, calculados pela função fat2.dbc(). apresentação destas médias em um trabalho científico, por exemplo, pode ser por meio de tabelas, ou gráficos. função plot_factbars() é usada aqui para confeccionar um gráfico de barras mostrando interação entre os fatores.Plotagem das médias considerando interaçãoExercício 11\r\n- Confeccione um gráfico semelhante ao acima considerando o fator FONTEN mapeado em escala de cinza.Resposta","code":"\nwith(FAT1_CI,\n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  FONTEN,\n              bloco = BLOCO,\n              resp =  RG,\n              fac.names = c(\"HIBRIDO\", \"FONTEN\")))\n\nplot_factbars(FAT1_CI, FONTEN, HIBRIDO, resp = RG,\n              values = TRUE,\n              lab.bar = c(\"bB\", \"cC\", \"aB\", # AmonioA\n                          \"bC\", \"bD\", \"aC\", # NitratoA\n                          \"aA\", \"aA\", \"aA\", # SulfatoA\n                          \"cC\", \"aB\", \"bC\")) # Ureia"},{"path":"analdata.html","id":"qualitativo-vs-quantitativo","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.3 Qualitativo vs quantitativo","text":"","code":""},{"path":"analdata.html","id":"sem-interação-significativa-1","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.3.1 Sem interação significativa","text":"O conjunto de dados utilizado neste exemplo será o FAT2_SI. mesmo modo exemplo anterior, iremos confeccionar um gráfico prévio para visualização dos dados.Visualização dos dados\r\nFigure 10.17: Característica de produção em um experimento bifatorial sem interação significativa\r\nanálise estatistica dos dadosA função fat2.rdb()  será novamente utilizada neste exemplo. declaração dos argumentos é idêntica ao exemplo anterior. O que mudará aqui, é que será necessário informar que o fator DOSEN é quantitativo. Para isso, usa-se o argumento quali.Como interação não foi significativa, proceder-se-comparação de médias dos dois híbridos considerando média de todas doses de nitrogênio, e o ajuste de apenas uma regressão para os dois híbridos. Como o grau polinômio significativo foi quadrático, declararmos fit = 2 na função plot_lines().","code":"\nggplot(FAT2_SI, aes(x = DOSEN, y = RG)) +\n       geom_point(aes(colour = factor(HIBRIDO)), size = 1.5) +\n       geom_smooth(aes(colour = factor(HIBRIDO)), method = \"loess\")\nwith(FAT2_SI, \n     fat2.dbc(fator1 =  HIBRIDO,\n              fator2 =  DOSEN,\n              bloco = BLOCO,\n              resp =  RG,\n              quali = c(TRUE, FALSE),\n              fac.names = c(\"HIBRIDO\", \"DOSE\")))\nh <- plot_bars(FAT2_SI, HIBRIDO, RG,\n                 width.bar = 0.5,\n                 lab.bar = c(\"a\", \"b\"))\nd <- plot_lines(FAT2_SI, DOSEN, RG,\n                fit = 2,\n                col = FALSE,\n                xlab = \"Doses de nitrogênio\",\n                ylab = \"Rendimento de grãos (Mg/ha)\") +\n     geom_text(aes(0, 6.5, label=(paste(expression(\"y = 6,8326 + 0,0012x - 0,0003x\"^2*\"  R\" ^2*\" = 0,99 \")))),\n               hjust = 0,\n               col = \"black\",\n               parse = TRUE) \nplot_grid(h, d, labels = c(\"h\", \"d\"), rel_widths = c(1, 3))"},{"path":"analdata.html","id":"com-interação-significativa-1","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.3.2 Com interação significativa","text":"O conjunto de dados utilizado neste exemplo será o FAT2_CI. Por se tratar de um experimento fatorial com um fator qualitativo (hibrido) e outro quantitativo (dose), convém confeccionar um gráfico com o rendimento observado de cada híbrido em cada dose. Este gráfico, além de servir como ferramenta para identificar possíveis outliers, também nos permite identificar resposta de cada híbrido. Cabe salientar que este é um gráfico meramente ilustrativo. análise estatistica dos dados será realizada posteriormente.Visualização dos dados \r\nFigure 10.18: Rendimento observado de cada híbrido em cada dose de nitrogênio\r\nPara evitar uma longa saída neste documento devido interação significativa, impressão dos resultados foi suprimida. Rode programação em seu console para observar os resultados.%%análise de indicou efeitos significativos tanto para os efeitos principais, quanto para interação. Assim, análises complementares realizadas foram () comparação das médias pelo teste Tukey em cada nível da dose de N; e (ii) uma regressão polinomial ajustada para cada híbrido. Por padrão, o máximo grau polinômio ajustado é 3 (modelo cúbico).Comparação das médias dos híbridos em cada dose de nitrogênio.comparações de médias são apresentadas como saída da função fat2.dbc() após análise de variância. Neste momento, utilizaremos função plot_factbars()  pacote metan** para plotar médias dos híbridos em cada dose de nitrogênio. apresentação gráfica de resultados, mesmo considerando médias, é uma alternativa interessante à tabela, pois permite uma interpretação mais clara e intuitiva dos resultados.\r\nFigure 10.19: Gráfico das médias dos híbridos em cada dose de nitrogênio.\r\nAjuste de regressão para cada híbridoNo exemplo anterior, apresentamos médias dos híbridos em cada dose de nitrogênio. Agora, criaremos um gráfico com o grau polinômio significativo ajustado de cada híbrido. O grau ser ajustado deve ser identificado na saída da ANOVA . Para fins didáticos um resumo é fornecido.Utilizando uma equação, é possível estimar produtividade para uma dose de nitrogênio específica não testada, desde que ela esteja dentro intervalo estudado. Para isto, basta substituir o x na equação pela dose ser testada. Por exemplo, para estimar qual seria produtividade híbrido NUPEC_1 se tivéssemos aplicado 60 kg de N ha\\(^{-1}\\) basta resolver: \\(y = 9,2054 + 0,0209\\times 60\\), resultando em \\(y \\approx 10.5\\) Mg ha\\(^{-1}\\). interpretação deste resultado, entanto, deve ser cautelosa. Inconscientemente, concluiríamos que produtividade híbrido aumentaria 0,0209 Mg ha\\(^{-1}\\) cada kg de nitrogênio aplicado por hectare. Este fato, entanto, não é observado na prática. Por exemplo, produtividade não irá aumentar infinitamente medida em que se aumenta dose de nitrogênio aplicado. única conclusão válida, neste caso, é que produtividade aumenta linearmente até 100 kg de N ha\\(^{-1}\\). Este resultado se deu em virtude de doses testadas não terem sido o suficiente para identificar um outro comportamento na variável testada. Nestes casos, indica-se para estudos futuros aumentar o número de doses. Quando não se conhece o intervalo de dose em que variável em estudo apresenta uma resposta explicável, estudos pilotos podem ser realizados. Neste caso, testar-se-iam o mesmo número de tratamentos (número de doses), entanto com um intervalo maior entre doses (por exemplo, 0, 100, 200, 300 e 400 kg de N ha\\(^{-1}\\). Possivelmente, nesta amplitude, o comportamento da produtividade não seria linear, pois em uma determinada dose, produtividade estabilizaria.Semelhante ao exemplo das médias nas doses de nitrogênio, utilizaremos função plot_factlines()  para plotar, agora, uma regressão  ajustada para cada híbrido. Os argumentos serem informados são os seguintes: .data, o conjunto de dados (neste caso FAT2_CI); x e y, colunas dos dados correspondentes aos eixos x e y gráfico, respectivamente; group coluna que contém os níveis dos fatores em que regressões serão ajustadas; fit um vetor de comprimento igual ao número de níveis da coluna informada em group. O número indicado em cada posição vetor, corresponde ao grau polinômio ajustado (máximo grau ajustado = 4). Em nosso exemplo, utilizaremos fit = c(2, 1) para ajustar uma regressão quadrática para o híbrido NUPEC_1 e uma regressão linear para o híbrido NUPEC_2.Observando-se figura acima, é possível identificar o comportamento quadrático da variável resposta híbrido NUPEC_1. Para estes híbridos, houve um incremento positivo na produtividade até um ponto, posteriormente observa-se que produtividade tendeu reduzir. Uma explicação biológica para esta redução seria que o excesso de nitrogênio aplicado proporcionou um alto vigor vegetativo plantas, podendo ter ocorrido competição entre plantas por água, luz e outros nutrientes, ou até mesmo tombamento das plantas. O ponto em X (dose) em que produtividade é máxima é chamado de máxima eficiência técnica (MET) e pode ser estimado por: \\[\r\nMET = \\frac{{ - {\\beta _1}}}{{2 \\times {\\beta _2}}}\r\n\\]Substituindo com os parâmetros estimados, temos: \\[\r\nMET = \\frac{{ - 0,05575}}{{2 \\times  -0,0005574}} = 50\r\n\\]Logo, dose que proporciona máxima produtividade para o híbrido NUPEC_1 é aproximadamente 50 kg de N ha\\(^{-1}\\). Assim para sabermos qual é esta produtividade estimada, basta substituir o x da equação por 50, resultando em \\(y_{máx}\\) = 12,949 Mg ha\\(^{-1}\\).Outro ponto importante que é possível de estimar utilizando uma equação de segundo grau, é máxima eficiência econômica (MEE), ou seja, dose máxima, neste caso de nitrogênio, em que é possível aplicar obtendo-se lucro. Este ponto é importante, pois partir de uma certa dose, os incrementos em produtividade não compensariam o preço pago pelo nitrogênio aplicado. Este ponto pode ser facilmente estimado por:\\[\r\nMEE = MET + \\frac{u}{{2 \\times \\beta_2 \\times m}}\r\n\\]onde u e m são os preços nitrogênio e milho em grão, respectivamente, na mesma unidade utilizada para estimativa da equação (neste caso, preço nitrogênio por kg e preço milho por tonelada). Considerando o preço de custo nitrogênio como R 1,35 por kg e o preço de venda milho 600,00 por tonelada, substituindo-se na formula obtém-se:\\[\r\nMEE = 50 + \\frac{{1.35}}{{2 \\times (-0,0005574) \\times 600}} \\approx 48\r\n\\]Assim, dose máxima de nitrogênio que em que os incrementos de produtividade são lucrativos é de \\(\\approx 48\\) Kg ha\\(^{-1}\\).Exercício 12Utilize funções pacote dplyr para selecionar apenas o híbrido NUPEC_1 exemplo anterior.Utilize funções pacote dplyr para selecionar apenas o híbrido NUPEC_1 exemplo anterior.Confeccione um gráfico com uma linha ajustada considerando um modelo polinomial de segundo grau.Confeccione um gráfico com uma linha ajustada considerando um modelo polinomial de segundo grau.Insira uma linha vertical tracejada e cinza que intercepta o eixo x na dose de máxima eficiência técnica.Insira uma linha vertical tracejada e cinza que intercepta o eixo x na dose de máxima eficiência técnica.Insira uma linha vertical sólida e cinza que intercepta o eixo x na dose de máxima eficiência econômica.Insira uma linha vertical sólida e cinza que intercepta o eixo x na dose de máxima eficiência econômica.Resposta","code":"\nggplot(FAT2_CI, aes(x = DOSEN, y = RG)) + # cria um objeto ggplot\n       geom_point(aes(colour = factor(HIBRIDO)), size = 1.5) + # adiciona pontos\n       geom_smooth(aes(colour = factor(HIBRIDO)), method = \"loess\") # adiciona banda\nwith(FAT2_CI, \n     fat2.dbc(fator1 =  HIBRIDO,\n     fator2 =  DOSEN,\n     bloco = BLOCO,\n     resp =  RG,\n     quali = c(TRUE, FALSE),\n     fac.names = c(\"HIBRIDO\", \"DOSE\")))\nplot_factbars(FAT2_CI, DOSEN, HIBRIDO,\n              resp = RG,\n              xlab = \"Doses de nitrogênio\",\n              ylab = expression(paste(\"Rendimento de grãos (Mg ha\"^-1,\")\")),\n              palette = \"Greys\",\n              lab.bar = c(\"a\", \"b\", # 0\n                          \"a\", \"b\", # 25\n                          \"a\", \"b\", # 50\n                          \"a\", \"b\", # 75\n                          \"a\", \"a\")) # 100\nplot_factlines(FAT2_CI, DOSEN, RG,\n               group = HIBRIDO,\n               fit = c(2, 1))"},{"path":"analdata.html","id":"quantitativo-vs-quantitativo","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.4 Quantitativo vs quantitativo","text":"Neste exemplo, iremos avaliar dados de um experimento que testou dois fatores quantitativos. O conjunto de dados utilizado é o FAT3. análise de variância, neste caso, é realizada da mesma maneira que os exemplos anteriores, o que muda agora é que análise complementar, em caso de interação significativa será diferente. Por se tratar de dois fatores quantitativos, neste caso, doses de nitrogênio e de potássio na cultura milho, é de se esperar que, em caso de interação  significativa, haja uma combinação de doses ótimas que proporcione maior magnitude da variável resposta (rendimento de grãos). Assim, uma análise de superfície de resposta é mais indicada para este tipo de experimento.Para análise neste exemplo, utilizaremos função resp_surf(). Nesta função estão implementadas seguintes rotinas: análise de variância, ajuste da equação de superfície de resposta, determinação dos pontos críticos, estatisticas de ajuste e análise residual. O procedimento para análise é simples. Os seguintes argumentos precisam ser declarados: .data, o conjunto de dados; fator1 o nome da coluna com o primeiro fator; fator2 o nome da coluna com o segundo fator; rep o nome da coluna com os blocos; e resp o nome da coluna com variável resposta que se deseja analizar. Neste exemplo, vamos armazenar os resultados objeto sresp.Análise estatísticaPodemos observar que interação DOSEN x DOSEK foi significativa, assim metodologia de superficie de resposta foi corretamente utilizada. equação de superfícide considerada é um modelo com termos de segunda ordem, onde variável resposta é estimada considerando dois preditores, neste caso doses de nitrogênio e doses de potássio. Considerando estes fatores como e D o seguinte modelo é ajustado: \\(Y_i = \\beta_0 + \\beta_1A_i+\\beta_2D_i+\\beta_3A_i^2+\\beta_4D_i^2 +\\beta_5A_iD_i+\\epsilon_i\\). Os parametros estimados estão em Parâmetros estimados.doses ótimas são estimadas pela seguinte equação: \\[\r\n- 0.5 \\times ({{\\boldsymbol{}}^{ - 1}}{\\boldsymbol{X}})\r\n\\]Onde\\[\r\n{\\boldsymbol{}} = \\left( {\\begin{array}{*{20}{c}}{{\\beta _3}}&{{\\beta _5}/2}\\\\{{\\beta _5}/2}&{{\\beta _4}}\\end{array}} \\right)\r\n\\]e\\[\r\n{\\boldsymbol{X}} = \\left( \\begin{array}{l}{\\beta _1}\\\\{\\beta _2}\\end{array} \\right)\r\n\\]Em nosso exemplo,\\[\r\n{\\boldsymbol{}} = \\left( {\\begin{array}{*{20}{c}}{ - 0.11213}&{9.865e - 05}\\\\{9.865e - 05}&{ - 0.00763}\\end{array}} \\right){\\rm{; }}{{\\boldsymbol{}}^{ - 1}} = \\left( {\\begin{array}{*{20}{c}}{ - 8.91796}&{ - 0.1152}\\\\{ - 0.11527}&{ - 131.009}\\end{array}} \\right)\r\n\\]e\\[\r\n{\\boldsymbol{X}} = \\left( \\begin{array}{l}14.7502\\\\1.00569\\end{array} \\right)\r\n\\]Assim\\[\r\n- 0.5 \\times \\left[ {\\left( {\\begin{array}{*{20}{c}}{ - 8.91796}&{ - 0.1152}\\\\{ - 0.11527}&{ - 131.009}\\end{array}} \\right) \\times \\left( \\begin{array}{l}14.7502\\\\1.00569\\end{array} \\right)} \\right] = \\left( \\begin{array}{l}65.8292\\\\66.7277\\end{array} \\right)\r\n\\]","code":"\nsrmod =  resp_surf(FAT3,\n                   factor1 = DOSEN,\n                   factor2 = DOSEK,\n                   rep = BLOCO,\n                   resp = RG)\n# -----------------------------------------------------------------\n# Result for the analysis of variance \n# Model: Y = m + bk + Ai + Dj + (AD)ij + eijk \n# -----------------------------------------------------------------\n#             Df Sum Sq Mean Sq  F value   Pr(>F)    \n# BLOCO        3    158      53    3.621   0.0183 *  \n# DOSEN        3  65978   21993 1515.063  < 2e-16 ***\n# DOSEK        4  11817    2954  203.513  < 2e-16 ***\n# DOSEN:DOSEK 12   2363     197   13.563 1.21e-12 ***\n# Residuals   57    827      15                      \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# -----------------------------------------------------------------\n# Shapiro-Wilk's test for normality of residuals: \n# -----------------------------------------------------------------\n# W =  0.946194 p-valor =  0.002100403 \n# -----------------------------------------------------------------\n# Anova table for the response surface model \n# -----------------------------------------------------------------\n# Analysis of Variance Table\n# \n# Response: RG\n#             Df Sum Sq Mean Sq  F value    Pr(>F)    \n# DOSEN        1   3215    3215  15.4854  0.000186 ***\n# DOSEK        1   6538    6538  31.4899 3.301e-07 ***\n# I(DOSEN^2)   1  50925   50925 245.2693 < 2.2e-16 ***\n# I(DOSEK^2)   1   5098    5098  24.5541 4.440e-06 ***\n# DOSEN:DOSEK  1      1       1   0.0053  0.942298    \n# Residuals   74  15365     208                       \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# -----------------------------------------------------------------\n# Model equation for response surface model \n# Y = B0 + B1*A + B2*D + B3*A^2 + B4*D^2 + B5*A*D \n# -----------------------------------------------------------------\n# Estimated parameters \n# B0: -317.8340786\n# B1: 14.7502633\n# B2: 1.0056943\n# B3: -0.1121344\n# B4: -0.0076331\n# B5: 0.0001973\n# -----------------------------------------------------------------\n# Matrix of parameters (A) \n# -----------------------------------------------------------------\n# -0.1121344    9.87e-05 \n# 9.87e-05    -0.0076331 \n# -----------------------------------------------------------------\n# Inverse of the matrix A (invA) \n# -8.9179679    -0.1152744 \n# -0.1152744    -131.0091259 \n# -----------------------------------------------------------------\n# Vetor of parameters B1 e B2 (X) \n# -----------------------------------------------------------------\n# B1: 14.7502633\n# B2: 1.0056943\n# -----------------------------------------------------------------\n# Equation for the optimal points (A and D) \n# -----------------------------------------------------------------\n# -0.5*(invA*X)\n# Eigenvalue 1: -0.007633\n# Eigenvalue 2: -0.112135\n# Stacionary point is maximum!\n# -----------------------------------------------------------------\n# Stacionary point obtained with the following original units: \n# -----------------------------------------------------------------\n# Optimal dose (DOSEN): 65.8292\n# Optimal dose (DOSEK): 66.7277\n# Predicted: 201.2184\n# -----------------------------------------------------------------\n# Fitted model \n# -----------------------------------------------------------------\n# A = DOSEN\n# D = DOSEK\n# y = -317.83408+14.75026A+1.00569D+-0.11213A^2+-0.00763D^2+2e-04A*D\n# -----------------------------------------------------------------\n# Shapiro-Wilk normality test\n# p-value:  0.4522241 \n# According to Shapiro-Wilk normality test at 5% of significance, residuals can be considered normal. \n# ------------------------------------------------------------------\nP1 = plot(srmod)\nP2 = plot(srmod, cut = 9,\n          colorkey = list(space = \"top\", width = 1),\n          xlab = \"Dose de Nitrogênio (Kg/ha)\",\n          ylab= \"Dose de Potássio (Kg/ha)\")\n\ngridExtra::grid.arrange(P1, P2, ncol = 2)"},{"path":"analdata.html","id":"experimentos-em-parcelas-subdivididas","chapter":"Capítulo 10 Análise de dados experimentais","heading":"10.6.5 Experimentos em parcelas subdivididas","text":"Experimentos fatoriais são úteis devido possibilidade de se testar dois ou mais fatores em um mesmo experimento. Uma desvantagem deste tipo de experimento é que cada bloco deve receber todos os tratamentos, ou seja, todas combinações dos níveis dos dois fatores. Assim, o número de parcelas experimento e consequentemente o tamanho da área experimental crese drastricamente na medida em que são incluídos fatores ou níveis de fatores experimento. Uma maneira de se contornar isto, é condução de experimentos em parcelas subdivididas.Parcelas subdivididas  são um caso especial de estrutura de tratamentos fatorial em que um fator é alocado na parcela principal e outro fator é alocado na subparcela. Este tipo de estrutura de tratamentos pode ser utilizada quando um fator é de dificil instalação em pequenas parcelas, como por exemplo, semeadura mecanizada ou um sistema de irrigação, e o segundo fator pode ser alocado em parcelas mais pequenas, como um doses de nitrogênio, por exemplo.Diferentemente modelo fatorial tradicional, o modelo estatístico para análise de experimentos em parcelas subdivididas conta com mais uma fonte de variação. Vamos considerar como exemplo, um experimento que avaliou influencia de dois fatores, digamos \\(\\alpha\\) e \\(\\tau\\), em uma determinada variável resposta, agora, conduzido em parcelas subivididas, onde o fator \\(\\alpha\\) foi alocado na parcela principal e o fator \\(\\tau\\) alocado na subparcela. O modelo estatístico considerado neste tipo de experimento é:\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + {\\rm{ }}\\mathop \\alpha \\nolimits_i + \\mathop \\beta \\nolimits_{k} + \\mathop \\eta \\nolimits_{ik}  +\\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) é variável resposta observada; \\(\\mu\\) é média geral; \\(\\mathop \\alpha \\nolimits_i\\) é o efeito -ésimo nível de \\(\\alpha\\) ; \\(\\mathop \\beta \\nolimits_{k}\\) é o efeito bloco k; \\(\\mathop \\eta \\nolimits_{ik}\\) é o erro de parcela, mais conhecido como erro ; assumido \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits_\\eta^2 )\\); \\(\\mathop \\tau \\nolimits_j\\) é o efeito j-ésimo nível de \\(\\tau\\) ; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) é o efeito da interação -ésimo nível de \\(\\alpha\\) com o j-ésimo nível de \\(\\tau\\); e \\(\\mathop \\varepsilon \\nolimits_{ijk}\\) é o erro da subparcela, mais conhecido como erro b, assumindo \\(\\mathop \\varepsilon \\nolimits_{ijk} \\mathop \\cap \\limits^{iid} N(0,\\mathop \\sigma \\nolimits^2 )\\).","code":"\nwith(FAT1_SI,\npsub2.dbc(fator1 =  HIBRIDO,\n          fator2 =  FONTEN,\n          bloco = BLOCO,\n          resp =  RG,\n          fac.names = c(\"HIBRIDO\", \"FONTEN\")))\n# ------------------------------------------------------------------------\n# Legenda:\n# FATOR 1 (parcela):  HIBRIDO \n# FATOR 2 (subparcela):  FONTEN \n# ------------------------------------------------------------------------\n# \n# ------------------------------------------------------------------------\n# $`Quadro da analise de variancia\\n------------------------------------------------------------------------\\n`\n#                GL      SQ     QM      Fc Pr(>Fc)    \n# HIBRIDO         2  98.051 49.026 209.972   3e-06 ***\n# Bloco           3   0.128  0.043   0.183  0.9041    \n# Erro a          6   1.401  0.233                    \n# FONTEN          2  83.048 41.524  71.117  <2e-16 ***\n# HIBRIDO*FONTEN  4   1.350  0.337   0.578  0.6824    \n# Erro b         18  10.510  0.584                    \n# Total          35 194.488                           \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# ------------------------------------------------------------------------\n# CV 1 = 4.31183 %\n# CV 2 = 6.818585 %\n# \n# Interacao nao significativa: analisando os efeitos simples\n# ------------------------------------------------------------------------\n# HIBRIDO\n# Teste de Tukey\n# ------------------------------------------------------------------------\n# Grupos Tratamentos Medias\n# a      NUPEC_2     13.33417 \n#  b     NUPEC_3     10.97337 \n#   c    NUPEC_1     9.311875 \n# ------------------------------------------------------------------------\n# \n# FONTEN\n# Teste de Tukey\n# ------------------------------------------------------------------------\n# Grupos Tratamentos Medias\n# a      AmonioA     13.33304 \n#  b     Ureia   10.40515 \n#  b     NitratoA    9.881223 \n# ------------------------------------------------------------------------"},{"path":"reg.html","id":"reg","chapter":"Capítulo 11 Análise de regressão","heading":"Capítulo 11 Análise de regressão","text":"","code":""},{"path":"reg.html","id":"regressão-linear","chapter":"Capítulo 11 Análise de regressão","heading":"11.1 Regressão Linear","text":"análise de regressão  tem como objetivo verificar como uma variável independente influencia resposta de uma variável dependente. análise de regressão é amplamente utilizada em ciências agrárias e pode ser dividida em simples ou múltipla. Na regressão simples, apenas uma variável independente é declarada modelo:\\[\r\nY_i = {\\beta _0} + {\\beta _1}x + \\varepsilon_i  \r\n\\]Onde \\(Y_i\\) é variável dependente, \\(x\\) é variável independente, \\(\\beta_0\\) é o intercepto, \\(\\beta_1\\) é inclinação da reta e \\(\\varepsilon\\) é o erro. Na regressão linear múltipla, mais de uma variável independente é declarada modelo:\\[\r\nY_i = {\\beta _0} + {\\beta _1}x_1 + {\\beta _2}x_2 + ... + {\\beta _k}x_k + \\varepsilon_i  \r\n\\]Onde \\(Y_i\\) é variável dependente, \\(x_1\\), \\(x_2\\),…,\\(x_k\\) são variáveis independentes, \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\),…,\\(\\beta_2\\) são os parâmetros da regressão e \\(\\varepsilon\\) é o erro. Então, uma regressão por ser descrita genericamente por (Draper Smith 1998)\\[\r\nY_i = f(x)+\\varepsilon_i \r\n\\]Onde \\(Y_i\\) é variável dependente, \\(f(x)\\) é função resposta modelo e \\(\\varepsilon\\) é o erro.","code":""},{"path":"reg.html","id":"estimativa","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.1 Estimativa","text":"Uma das formas de estimar os parâmetros em regressão é minimizando os erros. Os erros são diferença entre o valor estimado pela função resposta \\(f(x)\\) e o valor observado (\\(Y_i\\)), representados gráfico abaixo por pontos vermelhos. Então, devemos encontrar valores para \\({\\boldsymbol{\\beta}}\\) que minimizem estes erros, representados pela distância entre reta estimada e os valores observados. \r\nFigure 11.1: Gráfico de dispersão de alguns dados com uma linha representando tendência geral. linhas verticais representam diferenças (ou residuais) entre linha e os dados observados.\r\nNa figura acima, uma reta é traçada de modo que distâncias entre ela e os pontos seja mínimo. Essa distância é obtida, pelo método dos mínimos quadrados, minimizando soma de quadrados dos resíduos  (uma vez que soma dos resíduos é igual zero).\\[\r\nS = {\\boldsymbol{\\varepsilon '\\varepsilon }} = \\sum\\limits_{= 1}^n {{{\\left( {{Y_i} - {{\\hat Y}_i}} \\right)}^2}} = 0\r\n\\]Para encontrar os valores dos parâmetros que minimiza essa soma de quadrados basta resolver o sistema de equações normais, obtida após derivar \\(S\\) em relação aos parâmetros. resolução deste sistema fornece estimativas não viesadas dos parâmetros \\({\\boldsymbol\\hat\\beta}\\).\\[\r\n\\begin{array}{c}{\\boldsymbol{X'X\\beta = X'Y}}\\\\{\\boldsymbol{\\hat\\beta}} = {\\left( {{\\boldsymbol{X'X}}}\\right)^{- 1}}{\\boldsymbol{X'Y}}\\end{array}\r\n\\]Percebe-se que estimativas dos parâmetros não tem nenhuma relação com os pressupostos  de normalidade, homocedasticidade e independência dos resíduios. Porém, cumprir pressupostos é importante para testar hipóteses e construir intervalos de confiança. Outro aspecto importante na estimação é necessidade da matriz \\(\\boldsymbol{X'X}\\) ser não singular, pois assim é possível inverter essa matriz e resolver o sistema de equações normais  obtendo parâmetros únicos. O sistema de equações normais também pode ser resolvido utilizando inversa generalizada. Neste último caso, os valores dos parâmetros que resolvem o sistema de equações não são únicos. Para maiores detalhes sobre como estimar os parâmetros de regressões lineares, ver Draper Smith (1998), Kutner et al. (2005) e Rencher Schaalje (2008).Vimos que uma matriz \\(\\boldsymbol{X'X}\\) não singular é necessária para obtermos os parâmetros da nossa regressão. não singularidade da matriz está relacionada com quanto variáveis independentes estão correlacionadas. Quando variáveis independentes estão aproximadamente (ou perfeitamente, o que é praticamente impossível) relacionadas dizemos que há elevada multicolinearidade. O principal problema da multicolinearidade está relacionado com estimativas dos parâmetros. Quando ela é elevada, um conjunto de funções resposta minimiza os erros e prediz, com precisão, os valores observados.Quando há multicolinearidade é possível resolver o sistema de equações normais  utilizando inversa generaliazada de Moore-Penrose, utilizando função ginv()  pacote MASS. Utilizando inversa generalizada minimiza-se soma dos quadrados, porém não garante-se que os parâmetros sejam únicos. Então, qualquer inferências sobre como variáveis se relacionam passa ser duvidosa (Kutner et al. 2005).Para demonstrar como multicolinearidade afeta estimativa dos parâmetros, utilizamos um exemplo hipotético de Kutner et al. (2005). Execute programação em casa e veja os resultados.Quando há multicolinearidade, conjuntos de diferentes parâmetros resolvem o sitema de equações normais e minimizam soma de quadrados. Por isso, relacionar resposta da variável dependente em função das variáveis independentes passa ser impossível. É por isso também que multicolinearidade é importante na análise de trilha. relação entre variáveis na análise de trilha é determinada com base valor dos coeficientes de trilha, que nada mais são que parâmetros de uma regressão múltipla estimados com variáveis padronizadas.","code":"\ndata_error <- data.frame(x = seq(0, 20, 2),\n                         y = c(3,9,4,10,12,9,14,16,18,16,14))\nmod <- lm(y ~ x, data = data_error)\nggplot(data_error, aes(x, y)) + \n       geom_segment(aes(x = x, y = y, xend = x, yend = fitted(mod))) +\n       geom_point(color = \"red\") + \n       geom_smooth(se = FALSE, method = \"lm\")\nX1 <- c(2,8,6,10)\nX2 <- c(6,9,8,10)\ncor(X1, X2) # correlação entre as variávies independentes\n\n## Minimizando a soma de quadrados\nrequire (nls2)\nresultados <- data.frame(matrix(ncol = 4,nrow = 20))\nnames(resultados) <- c(\"b0\",\"b1\",\"b2\",\"sigma\")\n\nfor(i in 1:20){\nY <- c(23,83,63,103)\nX1 <- c(2,8,6,10)\nX2 <- c(6,9,8,10)\ngrid <- expand.grid(list(\nb0 <- seq(-i,i, by = 0.1),\nb1 <- seq(-i, i, by = 0.1),\nb2 <- seq(-i, i, by = 0.1)\n)) # Armazenando um conjunto de valores que quero dar aos parâmetros\n\nResp <- nls2(Y~b0+b1*X1+b2*X2,\nstart <- grid,\nalgorithm <- \"brute-force\")\n\nb0 <- summary(Resp)$coefficients[1,1]\nb1 <- summary(Resp)$coefficients[2,1]\nb2 <- summary(Resp)$coefficients[3,1]\nsigma <- summary(Resp)$sigma\n;\nresultados$b0[i] <- b0\nresultados$b1[i] <- b1\nresultados$b2[i] <- b2\nresultados$sigma[i] <- sigma\n}\nresultados"},{"path":"reg.html","id":"ajustando-regressões-com-a-função-lm","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.2 Ajustando regressões com a função lm()","text":"função lm()  é utilizada para ajustar regresões lineares simples e múltipla. Os argumentos mais importantes desta função são formula, onde indicamos função resposta; e data, onde indicamos o banco de dados. Vamos utiliza um exemplo simples retirado de Schenider, Schenider, Souza (2009):Através da função anova(mod7.1, mod7)  pode-se verificar se o modelo é ou não significativo. hipótese \\(H_0 = 0\\) é rejeitada e conclui-se que o modelo explica o comportamento da variável resposta. Através da função summary()  obtém-se o resultado teste t para os parâmetros modelo. hipótese testada neste caso é \\(H_0:\\boldsymbol{\\beta} = 0\\) vs \\(H_A:\\boldsymbol{\\beta}\\ne 0\\). Por fim, função anova() retorna um teste F que possibilita verificar contribuição de cada parâmetro em explicar variabilidade da variável resposta.Entre os parâmetros modelo, apenas \\(\\hat{\\beta_3}\\) não foi significativo, indicando que não há necessidade dele ser incluido modelo. Através teste F é possível verificar qual o modelo (com ou sem \\(\\beta_3\\)) é o mais parcimonioso. O teste F é dado por:\\[\r\nF_{calc} = \\frac{S{Q_{Erro}}(\\Omega)- S{Q_{Erro}}(\\omega)/G{L_{Erro}}(\\Omega)- G{L_{Erro}}(\\omega)} {Q{M_{Erro}}(\\omega )}\\\r\n\\]Onde, \\(SQ_{Erro}(\\Omega)\\) e \\(SQ_{Erro}(\\omega)\\) são somas de quadrados dos resíduos  nos modelos completo e reduzido, respectivamente; \\(G{L_{Erro}}(\\omega)\\) e \\(G{L_{Erro}}(\\Omega)\\) são os graus de liberdade resíduo modelo completo e reduzido, respectivamente; e \\(Q{M_{Erro}}(\\omega )\\) é o quadrado médio resíduo modelo completo. Podemos realizar o teste F utilizando função anova():Como ambos modelos são estatisticamente iguais, opta-se pelo modelo reduzido. O modelo que melhor se ajustou aos dados foi \\(Y = 6.726 + 2.759X_1 - 1.937X_2\\), \\(R^2_{Aj} = 0.93\\) superior 90%.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nreg <- import(url, sheet = \"REG\")\nmod7 <- lm(Y ~ X1 + X2 + X3, data = reg)\nmod7.1 <- lm(Y ~ 1, data = reg)\nanova(mod7.1, mod7) # Verificar a significância do modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     12 1834.00                                  \n# 2      9   97.47  3    1736.5 53.449 4.653e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(mod7)\n# \n# Call:\n# lm(formula = Y ~ X1 + X2 + X3, data = reg)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -5.4062 -2.2378 -0.3066  1.6321  4.8468 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  7.86871    3.32628   2.366   0.0422 *  \n# X1           2.72881    0.25198  10.830 1.84e-06 ***\n# X2          -1.92182    0.25540  -7.525 3.60e-05 ***\n# X3          -0.09752    0.15686  -0.622   0.5496    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.291 on 9 degrees of freedom\n# Multiple R-squared:  0.9469,  Adjusted R-squared:  0.9291 \n# F-statistic: 53.45 on 3 and 9 DF,  p-value: 4.653e-06\nmod8 <- lm(Y ~ X1 + X2, data = reg)\nanova(mod8, mod7)\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\ncoefficients(mod8)\n# (Intercept)          X1          X2 \n#    6.726606    2.759633   -1.937615"},{"path":"reg.html","id":"seleção-de-variáveis","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.3 Seleção de variáveis","text":"Foi mostrado brevemente um exemplo de como ajustar e selecionar variáveis. Porém, exemplo apresentado, utilizou-se somente três variáveis. entanto, quando há um elevado número de variáveis, selecioná-las torna-se um trabalho um pouco mais complexo. Nestes casos é necessário utilizar algoritimos de seleção. Os mais comuns são o Forward, Backward e Stepwise.Forward método forward parte-se de um modelo com uma variável independente, que é aquela que possui maior correlação  amostral com variável dependente. Posteriormente, realiza-se o teste F para verificar se ela é realmente é significativa. segunda variável independente com maior correlação amostral com variável dependente é adicionada ao modelo, e um teste F parcial verifica significância. variávies são adicionadas enquanto o F parcial significativo.Backward método backward parte-se modelo completo. variávies candidatas serem eliminadas são determinadas através teste F parcial, como se elas fossem (hipoteticamente) últimas serem incluidas modelo.Stepwise O método stepwise utiliza características dos métodos forward e backward. Neste método parte-se de um modelo composto pela variável com maior correlação  amostral. cada variável adicionada por forward, é relizado um backward para retirar uma das variáveis previamente adicionadas.análises acima foram demonstradas apenas para detalhar o funcionamento dos algoritimos de seleção, utilizando F parcial. O F parcial é muito rigoroso, e por isso muitas vezes o pesquisador usa valores de \\(\\alpha\\) maiores que 5%. Além disso, várias funções R estão disponíveis para selecionar variáveis utilizando diferentes diferentes critérios. funções ols_step_forward_p(), ols_step_backward_p() e ols_step_both_p() pacote olsrr35 selecionam variávies utilizando utilizado forward, backward ou stepwise, respectivamente, considerando p-valores para critério de decisão de inclusão/remoção de variáveis., \r\n","code":"\ncor(reg[1], reg[2:4]) # Correlação\n#          X1         X2         X3\n# Y 0.7754301 -0.4558018 -0.2460537\nmod_for1 <- lm(Y ~ 1, data = reg)\nmod_for2 <- lm(Y ~ X1, data = reg) # Adiciona X1 \nmod_for3 <- lm(Y ~ X1 + X2, data = reg)  # Adiciona X2 \nmod_for4 <- lm(Y ~ X1 + X2 + X3, data = reg)  # Adiciona X3 \nanova(mod_for1, mod_for2, mod_for3, mod_for4)  # Seleciona o modelo \n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1\n# Model 3: Y ~ X1 + X2\n# Model 4: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq        F    Pr(>F)    \n# 1     12 1834.00                                    \n# 2     11  731.23  1   1102.77 101.8264 3.318e-06 ***\n# 3     10  101.66  1    629.58  58.1331 3.243e-05 ***\n# 4      9   97.47  1      4.19   0.3865    0.5496    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# F parcial para X3\nmod_back.x3 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x3.1 <- lm(Y~X1+X2,data = reg)\nanova(mod_back.x3.1,mod_back.x3) # Menor F parcial\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\n\n# F parcial para X2\nmod_back.x2 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x2.1 <- lm(Y~X1+X3,data = reg)\nanova(mod_back.x2.1,mod_back.x2)\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X3\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     10 710.70                                  \n# 2      9  97.47  1    613.23 56.624 3.598e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# F parcial para X1\nmod_back.x1 <- lm(Y~X1+X2+X3,data = reg)\nmod_back.x1.1 <- lm(Y~X2+X3,data = reg)\nanova(mod_back.x1.1,mod_back.x1) # Maior F parcial \n# Analysis of Variance Table\n# \n# Model 1: Y ~ X2 + X3\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     10 1367.60                                  \n# 2      9   97.47  1    1270.1 117.28 1.836e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Elimina a 3, depois a 2 e depois a 1\nmod_back1 <- lm(Y~X1+X2+X3,data = reg)\nmod_back2 <- lm(Y~X1+X2,data = reg)\nanova(mod_back2,mod_back1) # elimina X3\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\n\nmod_back2 <- lm(Y~X1+X2,data = reg)\nmod_back3 <- lm(Y~X1,data = reg)\nanova(mod_back3,mod_back2) # não elimina X2 e seleciona\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1\n# Model 2: Y ~ X1 + X2\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 731.23                                  \n# 2     10 101.66  1    629.58 61.933 1.359e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Passo 1\nmod_step1.0 <- lm(Y ~ 1, data = reg)\nmod_step1 <- lm(Y ~ X1, data = reg)\nanova(mod_step1.0, mod_step1) # adiciona X1\n# Analysis of Variance Table\n# \n# Model 1: Y ~ 1\n# Model 2: Y ~ X1\n#   Res.Df     RSS Df Sum of Sq      F   Pr(>F)   \n# 1     12 1834.00                                \n# 2     11  731.23  1    1102.8 16.589 0.001842 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Passo 2\nmod_step2 <- lm(Y ~ X1, data = reg)\nmod_step2.1 <- lm(Y ~ X1 + X2, data = reg)\nanova(mod_step2, mod_step2.1) # adiciona X2\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1\n# Model 2: Y ~ X1 + X2\n#   Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 731.23                                  \n# 2     10 101.66  1    629.58 61.933 1.359e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nmod_step2.2 <- lm(Y ~ X2, data = reg)\nanova(mod_step2.2, mod_step2.1) # mantém X1 no modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X2\n# Model 2: Y ~ X1 + X2\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     11 1452.98                                  \n# 2     10  101.66  1    1351.3 132.93 4.251e-07 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Passo 3\nmod_step3 <- lm(Y ~ X1 + X2, data = reg)\nmod_step3.1 <- lm(Y ~ X1 + X2 + X3,data = reg)\nanova(mod_step3, mod_step3.1) # não adiciona X3, seleciona o modelo\n# Analysis of Variance Table\n# \n# Model 1: Y ~ X1 + X2\n# Model 2: Y ~ X1 + X2 + X3\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     10 101.655                           \n# 2      9  97.469  1     4.186 0.3865 0.5496\nolsrr::ols_step_forward_p(mod_for4)\nolsrr::ols_step_backward_p(mod_for4)\nolsrr::ols_step_both_p(mod_for4)"},{"path":"reg.html","id":"falta-de-ajuste","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.4 Falta de ajuste","text":"Quando várias observações são realizadas para cada variável independente (experimentos com repetição, por exemplo), é necessário verificar falta de ajuste. Nestes casos, o erro é dividido em duas partes: ) o erro puro,  que consiste na diferença entre média e observações em cada variável; b) falta de ajuste,  que é diferença entre média da variável independente e o valor ajustado pela regressão.\\[\r\n{Y_{ij}} - {\\hat Y_i} = \\left( {{Y_{ij}} - {{\\bar Y}{.}}} \\right) + \\left( {{{\\bar Y}{.}} - {{\\hat Y}_i}} \\right)\r\n\\]Onde \\(\\hat{Y_{ij}}-Y_{ij}\\) é o erro modelo, \\((Y_{ij}-\\bar{Y_j})\\) é o erro puro e \\((\\hat{Y_{ij}}-\\bar{Y_j})\\) é falta de ajuste.significância teste F indica que o modelo linear não é adequado para representar relação entre variávies dependentes e independentes. Isso indica que o modelo ajustado “não se aproxima” satisfatoriamente da média das variávies independentes, e que falta de ajuste  é elevada quando comparado ao erro puro. Agora, vamos ajustar um modelo quadrático:não significância teste F indica que o modelo quadrático é adequado para representar relação entre variávies dependentes e independentes. Aqui o exemplo é apresentado para um ajuste de polinômios, mas sua aplicação se estende qualquer regressão (linear simples, múltipla ou regressão não linear).","code":"\nmod10 <- lm(RG ~ DOSEN, data = quantitativo) # Regressão linear\nmod10.1 <- lm(RG ~  factor(DOSEN), data = quantitativo) #falta de ajuste\nanova(mod10, mod10.1) # Erro puro\n# Analysis of Variance Table\n# \n# Model 1: RG ~ DOSEN\n# Model 2: RG ~ factor(DOSEN)\n#   Res.Df     RSS Df Sum of Sq      F    Pr(>F)    \n# 1     18 2.74295                                  \n# 2     15 0.50276  3    2.2402 22.279 8.831e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmod11 <- lm(RG ~  DOSEN +I(DOSEN^2), data = quantitativo) #Regressão quadrática\nanova(mod11, mod10.1)\n# Analysis of Variance Table\n# \n# Model 1: RG ~ DOSEN + I(DOSEN^2)\n# Model 2: RG ~ factor(DOSEN)\n#   Res.Df     RSS Df Sum of Sq      F Pr(>F)\n# 1     17 0.58616                           \n# 2     15 0.50276  2  0.083408 1.2443 0.3163"},{"path":"reg.html","id":"análise-dos-resíduos","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.5 Análise dos resíduos","text":"Na análise de regressão, os resíduos  devem ser normalmente distribuídos, homocedásticos e independentes. O diagnóstico é realizado por testes estatísticos ou através de análise gráfica.análise dos resíduos  também pode indicar necessidade de adicionar variávies explicativas ao modelo. Por exemplo, vamos analisar os resíduos de um modelo linear ajustados dados que tem (conhecidamente) comportamento quadrático.Percebe-se, pelo gráfico residuals vs fitted, que os resíduos não são aleatoriamente distribuídos em torno de zero. distribuição sistemática dos resíduos indica que uma variável que explica consideravelmente variabilidade dos dados não foi incluída modelo (caso o termo quadrático polinômio).","code":"\n\nresiduos <- residuals(mod11)\nshapiro.test(residuos) # Normalidade\n# \n#   Shapiro-Wilk normality test\n# \n# data:  residuos\n# W = 0.96509, p-value = 0.6497\nbartlett.test(residuos ~ DOSEN, data = quantitativo) # Homocedasticidade\n# \n#   Bartlett test of homogeneity of variances\n# \n# data:  residuos by DOSEN\n# Bartlett's K-squared = 5.7732, df = 4, p-value = 0.2167\nautoplot(mod11)# análise gráfica dos resíduos\nresiduos <- residuals(mod10)\nautoplot(mod10) # análise gráfica dos resíduos"},{"path":"reg.html","id":"pontos-influentes","chapter":"Capítulo 11 Análise de regressão","heading":"11.1.6 Pontos influentes","text":"observações influentes podem ser mensuradas através da distância de Cook, DFBETA e DFFITS. O DFFITS e distância de Cook medem influência das observações sobre predição das variávies; e o DFBETA mede influência destas observações sobre estimativas dos parâmetros.Vamos utilizar funções gráficas pacote olsrr para fazer o diagnóstico dos pontos infuentes.\r\nFigure 11.2: Distância de Cook representando influencia dos pontos na predição das variávies\r\nobservações 8 e 12 são que mais influenciam os valores preditos e estimativas dos parâmetros. Os limites pela distância de Cook, DFBETA e DFFITS para realizar o diagnóstico são \\(\\frac{4}{n} = \\frac{4}{20} = 0,20\\), \\(\\frac{2}{\\sqrt{n}} = \\frac{2}{\\sqrt{2}} = 0,45\\) e \\(2 \\times \\sqrt {\\frac{p}{n}} = 2 \\times \\sqrt {\\frac{3}{{20}}} = 0,77\\), respectivamente.","code":"\nolsrr::ols_plot_cooksd_bar(mod11) # Distância de Cook\nolsrr::ols_plot_dfbetas(mod11) # DFBetas\nolsrr::ols_plot_dffits(mod11) # DFFits"},{"path":"reg.html","id":"regressão-não-linear","chapter":"Capítulo 11 Análise de regressão","heading":"11.2 Regressão não linear","text":"Uma regressão é dita não linear quando os parâmetros não encontram-se de forma aditiva modelo. Devido isso, o sistema de equações normais \\({\\left( {{\\boldsymbol{X'X}}} \\right)^{{\\boldsymbol{ - 1}}}}{\\boldsymbol{\\beta = X'Y}}\\) não pode ser resolvido analiticamente, e os parâmetros precisam ser estimados utilizando métodos iterativos. ","code":""},{"path":"reg.html","id":"estimativa-1","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.1 Estimativa","text":"estimação dos parâmetros é realizado pelo método dos mínimos quadrados . O método iterativo utilizado nos softwares R e SAS, por exemplo, é o de Gauss-Newton. Este método utiliza aproximações lineares de Taylor de primeira ordem da função resposta, dada por\\[\r\nf\\left( {x,{\\boldsymbol{\\theta }}} \\right) = f\\left( {x,{{\\boldsymbol{\\theta }}^0}} \\right) + \\frac{{\\partial f\\left( {x,{{\\boldsymbol{\\theta }}^0}} \\right)}}{{\\partial {\\boldsymbol{\\theta }}}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)   \r\n\\]Essa aproximação linear de primeira ordem pode ser simplificada por \\(f\\left( {\\boldsymbol{\\theta }} \\right) = f\\left( {{{\\boldsymbol{\\theta }}^0}} \\right) + {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)\\). Substituindo-na função que minimiza soma de quadrados, temos:\\[\r\n\\begin{array}{c}S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {y - f\\left( {{\\boldsymbol{\\hat \\theta }}} \\right)} \\right)}^2}} \\\\S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {y - f\\left( {{{\\boldsymbol{\\theta }}^0}} \\right) - {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)} \\right)}^2}} \\\\S\\left( {\\boldsymbol{\\theta }} \\right) = \\sum\\limits_{= 1}^n {{{\\left( {\\varepsilon  - {\\boldsymbol{F}}\\left( {{\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^0}} \\right)} \\right)}^2}} \\end{array}\r\n\\]Percebe-se que matriz \\(\\boldsymbol{F}\\), que substitui \\(\\boldsymbol{X}\\), é sempre dependente de um dos parâmetros modelo, e por isso o sistema de equações não tem resolução analítica. O sistema de equações não linear acima é resolvido por \\({\\boldsymbol{\\theta }} - {{\\boldsymbol{\\theta }}^{\\boldsymbol{0}}} = {\\left( {{\\boldsymbol{F}}'{\\boldsymbol{F}}} \\right)^{ - 1}}{\\boldsymbol{F}}'{\\boldsymbol{\\varepsilon }}\\), que reorganizada como \\({\\boldsymbol{\\theta }} = {{\\boldsymbol{\\theta }}^{\\boldsymbol{0}}} + {\\left( {{\\boldsymbol{F}}'{\\boldsymbol{F}}} \\right)^{ - 1}}{\\boldsymbol{F}}'{\\boldsymbol{\\varepsilon }}\\), corresponde ao primeiro passo método iterativo de Gauss-Newton. Para algoritimo seja iniciado, um valor inicial para os parâmetros deve ser declarado. O processo é repetido até obter convergência, que ocorre quando os valores estimados em cada passo são próximos um dos outros.","code":""},{"path":"reg.html","id":"ajustando-o-modelo-com-a-função-nls","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.2 Ajustando o modelo com a função nls()","text":"função nls()  pode ser utilizada para ajustar modelos não lineares. Os principais argumentos da função são: ) formula, onde o modelo é declarado; ii) data, onde os dados são declarados e iii) start , que é uma lista com os valores iniciais dos parâmetros.Valores iniciais dos parâmetrosO primeiro passo da análise é encontrar os valores iniciais dos parâmetros. O método gráfico é útil para cumprir esse objetivo. Valores dos parâmetros são declarados até o ponto em que curva gerada se aproxime dos valores observados. programação que será apresentada foi obtida blog Ridículas, mantido pelo LEG da UFPR. Utilizaremos como exemplo o modelo logístico (uma de suas parametrizações), dado por\\[\r\n{Y_i} = \\frac{{{\\beta _1}}}{{1 + {e^{\\left( {{\\beta _2} - {\\beta _3}{t_i}} \\right)}}}} + {\\varepsilon _i}\r\n\\]Manipulando os valores dos parâmetrosAjustando o modeloOs valores iniciais serão armazenados na lista start, e esta será declarada argumento start da função nls()  . função summary()  retorna o teste de Wald para os parâmetros.","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nnls_tomato <- import(url, sheet = \"TOMATE\")\nnls_tomato_cord <- subset(nls_tomato,Genotipo  ==  \"Cordillera\") \n# Modelo logístico\nlogi <- function(x, b1, b2, b3){\n  b1 / (1 + exp(b2 - b3 * x))\n}\nstart=list()\nmanipulate({\n  plot(num~DAT,data = nls_tomato_cord)\n  curve(logi(x, b1=b1,b2=b2,b3=b3),add=TRUE)\n  start<<-list(b1=b1,b2=b2,b3=b3)},\n  b1=slider(0,50,initial=10),\n  b2=slider(0, 20,initial=5),\n  b3=slider(0, 1,initial=0)\n)\nnls1 <- nls(num~b1/(1+exp(b2-b3*DAT)), \n         data = nls_tomato_cord, # indica os dados\n         start = start) # indica os valores iniciais\nsummary(nls1)\n# \n# Formula: num ~ b1/(1 + exp(b2 - b3 * DAT))\n# \n# Parameters:\n#    Estimate Std. Error t value Pr(>|t|)    \n# b1 39.68149    1.50505   26.37 1.96e-07 ***\n# b2 13.53980    0.94913   14.27 7.42e-06 ***\n# b3  0.13390    0.01018   13.15 1.19e-05 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.9108 on 6 degrees of freedom\n# \n# Number of iterations to convergence: 7 \n# Achieved convergence tolerance: 8.502e-06"},{"path":"reg.html","id":"análise-dos-resíduos-1","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.3 Análise dos resíduos","text":"Os resíduos  dos modelos não lineares também deve ser normalmente distribuídos, homocedásticos e independentes. Os testes estatísticos e análise dos resíduos seguem os mesmos princípios dos modelos lineares.O cumprimento dos pressupostos  dos resíduos não afeta estimativa dos parâmetros, mas é de extrema importância para construir intervalos de confiança e testar hipóteses. Percebe-se, nosso exemplo, que os pressuposto foram cumpridos (p-valor>0,05). Para realizar análise gráfica dos resíduos pode-se utilizar função nlsResiduals()  pacote nlstools.exemplo acima, apenas uma observação foi realizada para cada variável independente (dias após o transplante, caso). Por isso optou-se por utilizar o teste de Breusch-Pagan para realizar o diagnóstico de homocedasticidade dos resíduos. Quando mais de uma observação é realizada em cada variável independente, pode-se utilizar os testes de Bartlett ou Levene.","code":"\nrequire(lmtest) # pacote para carregar teste de Breusch-Pagan (homogeneidade)\nrequire (car) # pacote para carregar teste de DW (independência)\n### Normalidade\nres_nls1 <- residuals(nls1)\nshapiro.test(res_nls1)\n# \n#   Shapiro-Wilk normality test\n# \n# data:  res_nls1\n# W = 0.92624, p-value = 0.4464\nnls1_grad <- attr(nls1$m$fitted(), \"gradient\") # obtem matriz gradiente\nnls1_lm <- lm(num~-1+nls1_grad, data = nls_tomato_cord) \nbptest(nls1_lm) # teste de Breusch-Pagan (homogeneidade)\n# \n#   studentized Breusch-Pagan test\n# \n# data:  nls1_lm\n# BP = 1.5734, df = 2, p-value = 0.4554\ndurbinWatsonTest(nls1_lm) # teste de DW (independência)\n#  lag Autocorrelation D-W Statistic p-value\n#    1      0.08163678      1.763777   0.248\n#  Alternative hypothesis: rho != 0\nrequire(nlstools)\nres1_nls1 = nlsResiduals(nls1)\nplot(res1_nls1)\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nnls_eggplant <- import(url, sheet = \"EGGPLANT\")\nnls_eggplant <- subset(nls_eggplant, ESTUFA  ==  \"E1\") \nstart <- list(b1 = 10,b2 = 6.7,b3 = 0.073)\nnls2 <- nls(NUMERO ~ b1/(1+exp(b2-b3*DAT)), \n            data = nls_eggplant,\n            start = start)"},{"path":"reg.html","id":"medidas-de-não-linearidade","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.4 Medidas de não linearidade","text":"Conforme vimos acima, estimação dos parâmetros é realizada pelo método dos mínimos quadrados  utilizando aproximações lineares de Taylor da função resposta. É esta aproximação linear que garante que os parâmetros estimados sejam proximos de não viesados (só serão não viesados assintoticamente). Modelos com aproximação linear pobre tendem ter parâmetros muito viesados, o que impede que eles sejam utilizados para explicar determinado fenomeno biológico (eles tem ineterpretação biológica). medidas de curvatura de Bates e Watts são amplamente utilizadas para avaliar o grau de não linearidade da função resposta. Para maiores detalhes, ver Bates Watts (1988), cap. 7 e Seber Wild (2003), cap. 4. Essas medidas são facilmente implementadas através da função rms.curv()  pacote MASS. Para isto, utilizaremos o modelo ajustado nls1Os valores que função rms.curv() retorna são \\({c^\\theta } \\times \\sqrt {{F_{\\alpha ;p,n - p}}}\\) e \\({c^\\iota } \\times \\sqrt {{F_{\\alpha ;p,n - p}}}\\). Valores baixos destas duas medidas indicam que função tem boa aproximação linear e, consequentemente, os parâmetros são próximos de ser não viesados.","code":"\n\nstart <- list(b1 = 30, b2 = 19, b3 = 0.2)\n## Medidas de não linearidade\nnls1_hess <- deriv3(~b1/(1 + exp(b2 - b3 * DAT)), c(\"b1\",\"b2\",\"b3\"),\n                   function(DAT,b1,b2,b3)NULL) ## hessiana\nnls1_hess.1 <- nls(num ~ nls1_hess(DAT,b1,b2,b3),\n                   data = nls_tomato_cord,\n                   start = start)\nrms.curv(nls1_hess.1)\n# Parameter effects: c^theta x sqrt(F) = 0.806 \n#         Intrinsic: c^iota  x sqrt(F) = 0.1235"},{"path":"reg.html","id":"comparação-de-parâmetros","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.5 Comparação de parâmetros","text":"Os parâmetros em um modelo podem ser comparados utilizando variávies dummy. Utilizando esta técnica, ocorrência de um determinado fator é associado um nova variável. Como os modelos são aninhados, pode-se utilizar o teste F para verificar significância parâmetro (e, consequentemente, fator) associado esta variável.Vamos ao exemplo. Suponha que queiramos comparar produção e taxa de producão de frutos de dois genótipos de tomate. Sabemos que quando acumuladas, podução de olerícolas tem comportamento sigmoide (Lúcio, Nunes, Rego 2015; Lucio, Nunes, Rego 2016), o que permite deteminar essas caracteristicas através dos parâmetros de um modelo logístico:\\[\r\n{Y_i} = \\frac{{{\\beta _1}}}{{1 + {e^{\\left( {{\\beta _2} - {\\beta _3}{t_i}} \\right)}}}} + {\\varepsilon _i}\r\n\\]modelo acima, \\(\\beta_1\\) representa assíntota, e está associada produção total dos genótipos; \\(\\beta_3\\) é taxa de produção de frutos, e está associada precocidade produtiva dos genótipos (Sari 2018). Vamos testar seguintes hipóteses:\\[\r\n    \\begin{array}{*{20}{c}}{{H_0} = {\\beta _{11}} = {\\beta _{12}}}&{{H_0} = {\\beta _{31}} = {\\beta _{32}}}\\\\{{H_A} = {\\beta _{11}} \\ne {\\beta _{12}}}&{{H_A} = {\\beta _{31}} \\ne {\\beta _{32}}}\\end{array}\r\n\\]Testando hipótese \\(H_0:\\beta_{11} = \\beta_{12}\\)Associamos os fatores novas variávies através de uma nova coluna banco de dados. nosso caso, coluna “Completo” associa o fator aos parâmetros modelo logístico. Então, como o modelo logístico possui 3 parâmetros, ao associarmos variávies dummys ao fator genótipo (são dois genótipos), o modelo completo passa ter 6 parâmetros.Para tertar esta hipótese, associamos variáveis dummy s apenas aos parâmetros \\(\\beta_1\\) e \\(\\beta_3\\). Neste caso, o modelo passa ter 5 parâmetros (1 \\(\\beta_1\\), 2 \\(\\beta_2\\) e 2 \\(\\beta_3\\)). Podemos comparar esse modelo reduzido com o modelo completo de 6 parâmetros. Se o teste F der significativo, concluímos que há influência fator genótipo.Os valores da assintota diferem estatisticamente entre si. Percebe-se claramente que o genótipo Cordillera foi mais produtivo que o genótipo Gaúcho.Testando hipótese \\(H_0:\\beta_{31} = \\beta_{32}\\)Os valores da taxa de produção de frutos não diferem estatisticamente entre si.","code":"\nnls_dummy <- \n  nls_tomato %>%\n  as_factor(Genotipo, Completo, Reduzido)\n## Modelo completo\ntomato_completo <- \n  nls(num ~ b1[Completo] / (1 + exp(b2[Completo] - b3[Completo] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94, 39.68),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.17, 0.13)))\n## Modelo reduzido (beta 1)\ntomato_reduzido.b1 <- \n  nls(num ~ b1[Reduzido] / (1 + exp(b2[Completo] - b3[Completo] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.17, 0.13)))\nanova(tomato_reduzido.b1, tomato_completo)\n# Analysis of Variance Table\n# \n# Model 1: num ~ b1[Reduzido]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n# Model 2: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n#   Res.Df Res.Sum Sq Df Sum Sq F value    Pr(>F)    \n# 1     13     55.636                                \n# 2     12      7.833  1 47.802  73.228 1.875e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Modelo reduzido (beta 3)\ntomato_reduzido.b3 <- \n  nls(num ~ b1[Completo] / (1 + exp(b2[Completo] - b3[Reduzido] * DAT)), \n      data = nls_dummy,\n      start = list(b1 = c(18.94, 39.68),\n                   b2 = c(16.04, 13.54),\n                   b3 = c(0.16)))\nanova(tomato_reduzido.b3, tomato_completo)\n# Analysis of Variance Table\n# \n# Model 1: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Reduzido] * DAT))\n# Model 2: num ~ b1[Completo]/(1 + exp(b2[Completo] - b3[Completo] * DAT))\n#   Res.Df Res.Sum Sq Df Sum Sq F value  Pr(>F)  \n# 1     13    10.1479                            \n# 2     12     7.8335  1 2.3144  3.5454 0.08417 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"reg.html","id":"representação-gráfica-dos-modelos","chapter":"Capítulo 11 Análise de regressão","heading":"11.2.6 Representação gráfica dos modelos","text":"Percebe-se claramente que um modelo comum aos dois genótipos não é possível (assintota diferem entre si). Por isso, optou-se por gerar um modelo em separado para cada genótipo. Podemos representar isso graficamente utilizando função ggplot()  .\r\nFigure 11.3: Representação gráfica em modelos não lineares\r\n","code":"\nformula <- as.formula(\"y ~ b1/(1 + exp(b2-b3*x))\")\nstart <- list(b1 = 10.224, b2 = 6.765, b3 = 0.0725)\n\nggplot(nls_tomato, aes(x = DAT, y = num, colour = Genotipo)) + \ngeom_point() +\ngeom_smooth(method = \"nls\", \n            method.args = list(formula = formula, \n                                start = start),\n                                se = F,\n                                data = subset(nls_tomato, Genotipo == \"Cordillera\")) +\ngeom_smooth(method = \"nls\", \n            method.args = list(formula = formula, \n                                start = start),\n                                se = F,\n                                data = subset(nls_tomato, Genotipo == \"Gaucho\"))+\ntheme(legend.position = \"bottom\")+\nlabs(x = \"Dias após o transplante\", y = \"Número de frutos\")"},{"path":"reg.html","id":"regressão-bisegmentada-com-platô","chapter":"Capítulo 11 Análise de regressão","heading":"11.3 Regressão bisegmentada com platô","text":"Continuaremos tomando como exemplo produção de olerícolas de múltiplas colheitas para exemplificar o uso deste tipo de regressão. Os modelos com platô são modelos bi-segmentados cujo primeiro segmento descreve o crescimento da produção até determinado ponto, e um segundo segmento que descreve estabilização da produção (platô). Podemos representar um modelo linear-platô por:\\[\r\n{Y_i} = \\left\\{ \\begin{array}{l}{\\beta _0} + {\\beta _1}x + {\\varepsilon _i}{\\rm{, se }}{X_i}{\\rm{  <  }}{X_0}\\\\P{\\rm{ se }}{X_i}{\\rm{  >  }}{X_0}{\\rm{ }}\\end{array} \\right.\r\n\\]O genótipo Cordillera produz uma taxa de 1,33 frutos dia\\(^-1\\) até os ~9 dias após o início das colheitas, quando começa estabilizar produção (17,19 frutos). Já o genótipo Santa Clara produz uma taxa de 0,71 frutos \\(^-1\\) até os ~14 dias após o início das colheitas, quando começa estabilizar produção (12,69 frutos). Podemos verificar taxa de produção e o ponto de estabilização através de variáveis dummy .Percebe-se que taxa de produção genótipo Cordillera é significativamente superior taxa de produção genótipo Santa Clara. Como consequência, o momento de estabilização da produção ocorre mais precocemente genótipo Cordillera. O exemplo foi realizado com dados de produção de olerícolas, mas pdoe ser adaptado para qualquer estudo (desde que tenha este comportamento).Representação gráfica dos modelos\r\nFigure 11.4: Representação gráfica de regressão bisegmentada com platô\r\n","code":"\nurl <- \"https://github.com/TiagoOlivoto/e-bookr/raw/master/data/data_R.xlsx\"\nplato_tomato <- import(url, sheet = \"PLATO\")\nplato_cordillera <- \n  nls(num ~ (b0 + b1 * DAT * (DAT <= P)) + (b1 * P * (DAT > P)),\n      data = subset(plato_tomato, Genotipo == \"Cordillera\"),\n      start = list(b0 = 5, b1 = 11/8, P = 15))\nplato_SantaCl <- \n  nls(num ~ (b0 + b1 * DAT * (DAT <= P)) + (b1 * P * (DAT > P)),\n      data = subset(plato_tomato, Genotipo == \"Santa.Clara\"),\n      start = list(b0 = 5, b1 = 11/8, P = 15))\nplato_dummy <- \n  plato_tomato %>%\n  as_factor(Genotipo, Completo, Reduzido)\n# Modelo completo\nplato_completo <- \n  nls(num ~ (b0[Completo] + b1[Completo] * DAT * (DAT <= P[Completo]))+\n        (b1[Completo] * P[Completo] * (DAT > P[Completo])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8, 2.5), \n                   b1 = c(1.33, 0.71), \n                   P = c(9.28, 14.36)))\n\n# Modelo reduzido (taxa de produção)\nplato_reduzido.b1 <- \n  nls(num ~ (b0[Completo] + b1[Reduzido] * DAT * (DAT <= P[Completo]))+\n        (b1[Reduzido] * P[Completo] * (DAT > P[Completo])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8,2.5), \n                   b1 = c(1), \n                   P = c(9.28,14.36)))\nanova(plato_reduzido.b1, plato_completo)\n\n# Modelo reduzido (Platô)\nplato_reduzido.P <- \n  nls(num ~ (b0[Completo] + b1[Completo] * DAT * (DAT <= P[Reduzido])) + \n        (b1[Completo] * P[Reduzido] * (DAT > P[Reduzido])),\n      data = plato_dummy,\n      start = list(b0 = c(4.8, 2.5), \n                   b1 = c(1.33, 0.71), \n                   P = c(12)))\nanova(plato_reduzido.P, plato_completo)\nplot(num ~ DAT,\n     data = subset(plato_tomato,Genotipo == \"Cordillera\"),ylim = c(0,20),\n     xlab = \"Dias após o transplante (DAT)\",\n     ylab = \"Número de frutos por planta\",pch = 1,lwd = 2)\npoints(num~DAT,data = subset(plato_tomato,Genotipo == \"Santa.Clara\"),pch = 2,lwd = 2)\nsegments(y0 = 4.8385, x0 = 0, y1 = 17.2083, x1 = 9.28641,lty = 1,lwd = 2)\nsegments(y0 = 17.2083, x0 = 9.28641, y1 = 17.2083, x1 = 20,lty = 1,lwd = 2)\nsegments(y0 = 2.5000, x0 = 0, y1 = 12.7655, x1 = 14.3606,lty = 2,lwd = 2)\nsegments(y0 = 12.7655, x0 = 14.3606, y1 = 12.7655, x1 = 20,lty = 2,lwd = 2)\n\n\nlegend(\"bottomright\", legend = c(\"Cordillera\", \"Santa Clara\"), lty = c(1,2),\n       pch = c(1,2), lwd = c(2,2),bty = \"n\")"},{"path":"relations.html","id":"relations","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"Capítulo 12 Relações lineares entre variáveis","text":"Conhecer o grau de associação linear entre caracteres é de fundamental importância em um programa de melhoramento genético vegetal. Esta importância aumenta, principalmente se algum caractere desejável é de difícil mensuração, ou apresenta baixa herdabilidade. O coeficiente de correlação  produto-momento de Pearson (1920), r, vem sendo amplamente utilizado para este fim. Embora o mérito desta análise seja atribuído à Karl Pearson, o método foi originalmente concebido por Francis Galton, que definiu o termo correlação como como o seguinte: duas variáveis são ditas correlacionadas quando variação de uma é acompanhada na média, mais ou menos variação da outra, e mesmo sentido (Galton 1888).","code":""},{"path":"relations.html","id":"dados","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.1 Dados","text":"Nesta sessão, e na sessão de análise multivariada iremos utilizar o conjunto de dados datage_2 pacote metan. Para maiores informações veja ?data_ge2","code":"\nmaize <- data_ge2\nnumeric_var <- maize %>% select_numeric_cols()\ndatacor <- maize %>% select_cols(CD, CL, CW, PH, EH, EP, EL, ED)"},{"path":"relations.html","id":"correlação-linear","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2 Correlação linear","text":"estimativa r leva em consideração covariância entre duas variáveis, representadas aqui por XY dividia pelo produto dos respectivos desvios padrões de X e de Y, conforme o seguinte modelo:\\[\r\n{\\rm{r  =  }}\\frac{{\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{\\rm{[ (}}{{\\rm{X}}_{\\rm{}}}{\\rm{ - \\bar X)(}}{{\\rm{Y}}_{\\rm{}}}{\\rm{ - \\bar Y)] }}} }}{{\\sqrt {\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{{\\rm{(}}{{\\rm{X}}_{\\rm{}}}{\\rm{ - \\bar X)}}}^{\\rm{2}}}} } \\sqrt {\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{{\\rm{(}}{{\\rm{Y}}_{\\rm{}}}{\\rm{ - \\bar Y)}}}^{\\rm{2}}}} } }}\r\n\\]onde \\({\\rm{\\bar X = }}\\frac{{\\rm{1}}}{{\\rm{n}}}\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{\\rm{X}}_{\\rm{}}}}\\) e \\({\\rm{\\bar Y = }}\\frac{{\\rm{1}}}{{\\rm{n}}}\\sum\\limits_{{\\rm{= 1}}}^{\\rm{n}} {{{\\rm{Y}}_{\\rm{}}}}\\).Esta sessão é focada em apresentar funções básicas e avançadas para visualização gráfica de associações e estimativas coeficiente de correlação. Para este fim, utilizaremos o conjunto de dados datacor, criado anteriormente.","code":""},{"path":"relations.html","id":"visualização-gráfica","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.1 Visualização gráfica","text":"seguinte função proporciona uma visualização gráfica de todos os pares de correlação possíveis (scatter-plot)\r\n","code":"\npairs(datacor)"},{"path":"relations.html","id":"estimativa-dos-coeficientes-de-correlação","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.2 Estimativa dos coeficientes de correlação","text":"","code":"\ncorr <- corr_coef(datacor)\nprint(corr)\n# ---------------------------------------------------------------------------\n# Pearson's correlation coefficient\n# ---------------------------------------------------------------------------\n#       CD    CL    CW    PH    EH    EP    EL    ED\n# CD 1.000 0.300 0.484 0.315 0.281 0.175 0.912 0.390\n# CL 0.300 1.000 0.738 0.325 0.397 0.391 0.255 0.697\n# CW 0.484 0.738 1.000 0.505 0.519 0.425 0.458 0.737\n# PH 0.315 0.325 0.505 1.000 0.932 0.638 0.380 0.661\n# EH 0.281 0.397 0.519 0.932 1.000 0.870 0.363 0.630\n# EP 0.175 0.391 0.425 0.638 0.870 1.000 0.263 0.458\n# EL 0.912 0.255 0.458 0.380 0.363 0.263 1.000 0.385\n# ED 0.390 0.697 0.737 0.661 0.630 0.458 0.385 1.000\n# ---------------------------------------------------------------------------\n# p-values for the correlation coefficients\n# ---------------------------------------------------------------------------\n#          CD       CL       CW       PH       EH       EP       EL       ED\n# CD 0.00e+00 1.39e-04 1.54e-10 6.06e-05 3.90e-04 2.88e-02 1.97e-61 4.94e-07\n# CL 1.39e-04 0.00e+00 3.92e-28 3.45e-05 2.84e-07 4.55e-07 1.29e-03 4.74e-24\n# CW 1.54e-10 3.92e-28 0.00e+00 1.83e-11 3.76e-12 3.25e-08 1.81e-09 5.31e-28\n# PH 6.06e-05 3.45e-05 1.83e-11 0.00e+00 1.11e-69 3.10e-19 9.80e-07 5.66e-21\n# EH 3.90e-04 2.84e-07 3.76e-12 1.11e-69 0.00e+00 4.80e-49 3.28e-06 1.19e-18\n# EP 2.88e-02 4.55e-07 3.25e-08 3.10e-19 4.80e-49 0.00e+00 8.92e-04 1.83e-09\n# EL 1.97e-61 1.29e-03 1.81e-09 9.80e-07 3.28e-06 8.92e-04 0.00e+00 6.88e-07\n# ED 4.94e-07 4.74e-24 5.31e-28 5.66e-21 1.19e-18 1.83e-09 6.88e-07 0.00e+00"},{"path":"relations.html","id":"combinando-visualização-gráfica-e-numérica-i","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.3 Combinando visualização gráfica e numérica (I)","text":"Na figura acima, os pontos observados são plotados na diagonal inferior. Na diagonal, é apresentada estimativa da densidade Kernel (linha preta) e um histgrama de cada variável. diagonal superior contém os coeficientes de correlação.","code":"\n\npairs.panels(datacor)"},{"path":"relations.html","id":"combinando-visualização-gráfica-e-numérica-ii","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.4 Combinando visualização gráfica e numérica (II)","text":"função corr_plot()pacote metan retorna um gráfico semelhante ao anterior, entanto possui diversas opções, tais como mudança tamanho da letra dependendo da magnitude da correlação e indicação de cores para correlações significativas.\r\nFigure 12.1: Scatter plot de uma matriz de correlação de Pearson\r\nfunção corr_plot() pode ser utilizada com o operador %>%. Em adição, é possível escolher variáveis serem plotadas. Para isto, basta digitar o nome das variáveis.\r\nFigure 12.2: Scatter plot de uma matriz de correlação de Pearson\r\n\r\nFigure 12.3: Scatter plot de uma matriz de correlação de Pearson\r\n","code":"\ncorr_plot(datacor)\nmaize %>%\n  corr_plot(CD, EL, PERK, NKR, CW,\n            shape.point = 19,\n            size.point = 2,\n            alpha.point = 0.5,\n            alpha.diag = 0,\n            pan.spacing = 0,\n            col.sign = \"gray\",\n            alpha.sign = 0.3,\n            axis.labels = TRUE,\n            progress = FALSE)\n\nmaize %>%\n  corr_plot(CD, EL, PERK, NKR, CW,\n            shape.point = 21,\n            col.point = \"black\",\n            fill.point = \"orange\",\n            size.point = 2,\n            alpha.point = 0.6,\n            maxsize = 4,\n            minsize = 2,\n            smooth = TRUE,\n            col.smooth = \"black\",\n            col.sign = \"cyan\",\n            upper = \"scatter\",\n            lower = \"corr\",\n            diag.type = \"density\",\n            col.diag = \"cyan\",\n            pan.spacing = 0,\n            lab.position = \"bl\")"},{"path":"relations.html","id":"combinando-visualização-gráfica-e-numérica-iii","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.5 Combinando visualização gráfica e numérica (III)","text":"função corrplot.mixed()  pacote corrplot36 também é uma boa opção para visualização gráfica, principalmente quando um grande número de combinações está disponível.Criando matrix de correlação utilizando o conjunto de dados datacor.8 variáveis | 28 combinações","code":"\n\ncor1 <- cor(datacor)\ncorrplot.mixed(cor1,\n               upper = \"ellipse\",\n               lower = \"number\",\n               number.digits = 2)"},{"path":"relations.html","id":"combinando-visualização-gráfica-e-numérica-iv","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.2.6 Combinando visualização gráfica e numérica (IV)","text":"Criando matrix de correlação utilizando o conjunto de dados dataset.14 variáveis | 91 combinaçõesA função corrplot()  pacote corrplot permite uma poderosa personalização. Esta função tem vantagem de apresentar um elevado número de combinações em um gráfico claro e intuitivo.\r\nFigure 12.4: Gráfico de pizza de uma matriz de correlação de Pearson\r\n","code":"\ncor2 <- cor(numeric_var)\npval <- cor.mtest(cor2)$p\ncorrplot(cor2,\n         method = \"pie\",\n         p.mat = pval,\n         sig.level = 0.05,\n         insig = \"blank\",\n         type = \"lower\",\n         diag = F,\n         tl.col = \"black\",\n         tl.srt = 45)\ncorrplot(cor2,\n         method = \"ellipse\",\n         p.mat = pval,\n         sig.level = 0.05,\n         insig = \"blank\",\n         type = \"upper\",\n         diag = F,\n         tl.col = \"black\",\n         tl.srt = 45)"},{"path":"relations.html","id":"correlações-genéticas","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.3 Correlações genéticas","text":"função covcor_design() pode ser usada para calcular matrizes de correlação/(co) variância genéticas, fenotípicas e residuais através método de Análise de Variância (ANOVA) usando delineamento de blocos completos casualizados (DBC) ou delineamento inteiramente casualizado (DIC).correlações fenotípicas \\(r^p_{xy}\\), genotípicas \\(r^g_{xy}\\) e residuais \\(r^r_{xy}\\) entre duas variáveis x e y são calculadas conforme segue.\\[\r\nr^p_{xy} = \\frac{cov^p_{xy}}{\\sqrt{var^p_{x}var^p_{y}}} \\\r\nr^g_{xy} = \\frac{cov^g_{xy}}{\\sqrt{var^g_{x}var^g_{y}}} \\\r\nr^r_{xy} = \\frac{cov^r_{xy}}{\\sqrt{var^r_{x}var^r_{y}}}\r\n\\]Utilizando os quadrados médios (QM) obtidos da ANOVA, variâncias (var) e covariâncias (cov) são calculadas da seguinte forma:\\[\r\ncov^p_{xy} = [(QMT_{x+y} -QMT_x -QMT_y)/2]/r \\\\\r\nvar^p_x =QMT_x / r \\\\\r\nvar^p_y =QMT_y / r\r\n\\]\\[\r\ncov^r_{xy} = (QME_{x+y} - QME_x - QME_y)/2 \\\\\r\nvar^r_x = QME_x \\\\\r\nvar^r_y = QME_y\r\n\\]\\[\r\ncov^g_{xy} = [(cov^p_{xy} \\times r) - cov^r_{xy}]/r \\\\\r\nvar^g_x = (MST_x - MSE_x)/r \\\\\r\nvar^g_y = (MST_x - MSE_y)/r\r\n\\]onde QMT é o quadrado médio para tratamento, QME é o quadrado médio erro e r é o número de repetições/blocos. função covcor_design()  retorna uma lista com matrizes de (co)variâncias e correlações. Matrizes específicas podem ser retornadas usando o tipo de argumento type. exemplo abaixo, o coeficiente de correlação genotípico entre variáveis PH, EH, NKE e TKW será computado para o ambiente A1, considerando um DBC.Resposta","code":"\nmaize %>%\n  filter(ENV == \"A1\") %>%\n  covcor_design(gen = GEN,\n                rep = REP,\n                resp = c(PH, EH, NKE, TKW),\n                type = \"gcor\")\n#               PH           EH        NKE        TKW\n# PH   1.000000000 -0.006544623  0.2801806  0.2459377\n# EH  -0.006544623  1.000000000 -0.7752497  0.7247684\n# NKE  0.280180560 -0.775249741  1.0000000 -0.5117645\n# TKW  0.245937657  0.724768430 -0.5117645  1.0000000"},{"path":"relations.html","id":"intervalo-de-confiança","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.4 Intervalo de confiança","text":"","code":""},{"path":"relations.html","id":"paramétrico","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.4.1 Paramétrico","text":"O intervalo de confiança para o coeficiente de correlação pode ser obtido utilizando função cor.mtest() pacote corrplot, conforme o seguinte exemplo.","code":"\nci_corr <- cor.mtest(datacor)"},{"path":"relations.html","id":"não-paramétrico","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.4.2 Não paramétrico","text":"Um estimador não paramétrico intervalo de confiança coeficiente de correlação de Pearson foi proposto por Olivoto et al. (2018). Este estimador é baseado tamanho da amostra e força de associações e pode ser estimado usando função corr_ci() pacote metan.  É possível estimar o intervalo de confiança declarando o tamanho da amostra (n) e o coeficiente de correlação (r), ou usando um dataframe. O código seguir calcula o intervalo de confiança para os possíveis pares de correlação entre variáveis que contém E nome. Note o benefício operador %>% neste caso.","code":"\nmaize %>% \n  select(contains(\"E\")) %>%\n  corr_ci(verbose = FALSE) %>%\n  plot_ci()"},{"path":"relations.html","id":"tamanho-da-amostra","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.5 Tamanho da amostra","text":"Baseado modelo proposto por Olivoto et al. (2018), o tamanho da amostra suficiente para estimativa coeficiente de correlação considerando um intervalo de confiança desejado é obtido pela função corr_ss(). Neste exemplo, vamos calcular o tamanho da amostra necessário para que uma correlação de 0.6 apresente uma semi-amplitude intervalo de confiança igual 0.1 ","code":"\ncorr_ss(r = 0.6, CI = 0.1)\n# ------------------------------------------------- \n# Sample size planning for correlation coefficient \n# ------------------------------------------------- \n# Level of significance: 5%\n# Correlation coefficient: 0.6\n# 95% half-width CI: 0.1\n# Required sample size: 194\n# -------------------------------------------------"},{"path":"relations.html","id":"correlação-parcial","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.6 Correlação parcial","text":"Em certos casos, o coeficiente de correlação linear simples pode nos levar equívocos na interpretação da associação entre duas variáveis, pois este não considera influência das demais variáveis contidas conjunto de dados. O coeficiente de correlação parcial  é uma técnica baseada em operações matriciais que nos permite identificar associação entre duas variáveis retirando-se os efeitos das demais variáveis presentes (Anderson 2003) Uma maneira generalizada para estimativa coeficiente de correlação parcial entre duas variáveis (e j) é por meio da matriz de correlação simples que envolve estas duas variáveis e m outras variáveis das quais queremos retirar o efeito. estimativa coeficiente de correlação parcial entre e j excluído o efeito de m outras variáveis é dado por:\\[\r\n{r_{ij.m} = \\frac{{- {a_{ij}}}}{{\\sqrt {{a_{ii}}{a_{jj}}}}}}\r\n\\]onde \\(r_{ij.m}\\) é o coeficiente de correlação parcial  entre variável e j, sem o efeito das outras m outras variáveis; \\(a_{ij}\\) é o elemento da ordem ij da inversa da matriz de correlação simples; \\(a_{ii}\\) e \\(a_{jj}\\) são os elementos de ordens ii e jj, respectivamente, da inversa da matriz de correlação simples.matrizes de coeficientes de correlação linear e parcial podem ser facilmente obtida utilizando função lpcor()  pacote metan. entrada dos dados pode ser realizada de duas maneiras. () utilizando os dados com observações de cada variável; ou (ii) utilizando uma matriz de correlação linear simples pré estimada. Em nosso exemplo, vamos utilziar os mesmos dados utilizados nas funções anteriores (datacor).função retorna 3 objetos: linear.mat que contém matriz de correlação linear simples; partial.mat que contém matriz de correlações parciais, results que contém todas combinações de correlação com seus respectivos testes de hipótese.","code":"\n\npartial <- lpcor(datacor)\nprint(partial)\n# # A tibble: 28 x 5\n#    Pairs   linear partial      t          prob\n#    <chr>    <dbl>   <dbl>  <dbl>         <dbl>\n#  1 CD x CL  0.300  0.0767  0.936 0.351        \n#  2 CD x CW  0.484  0.125   1.53  0.129        \n#  3 CD x PH  0.315  0.0196  0.238 0.812        \n#  4 CD x EH  0.281 -0.0258 -0.314 0.754        \n#  5 CD x EP  0.175 -0.0121 -0.147 0.884        \n#  6 CD x EL  0.912  0.893  24.1   0            \n#  7 CD x ED  0.390  0.0108  0.131 0.896        \n#  8 CL x CW  0.738  0.465   6.40  0.00000000198\n#  9 CL x PH  0.325 -0.309  -3.95  0.000121     \n# 10 CL x EH  0.397  0.256   3.22  0.00158      \n# # ... with 18 more rows"},{"path":"relations.html","id":"análise-de-trilha","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7 Análise de trilha","text":"Nesta sessão, primeiramente uma breve introdução à análise de trilha é apresentada. Algumas dificuldades, como , por exemplo, presença de multicolinearidade  e possíveis soluções para contorná-la serão discutidas. Posteriormente exemplos numéricos serão realizados utilizando funções pacote metan.","code":""},{"path":"relations.html","id":"introdução-1","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.1 Introdução","text":"análise de trilha vem se destacando na área melhoramento genético, pois seleção para melhoria de um caractere desejável que possui difícil mensuração e baixa herdabilidade, pode ser realizada indiretamente por outro caractere, diretamente associado este, mas que apresente alta herdabilidade e seja de fácil mensuração. Esta técnica é baseada em ideias originalmente desenvolvidas por Sewall Wright (Wright 1921), entanto desde sua concepção até consolidação método, algumas divergências quanto fidedignidade método matemático que explica relações de causa e efeito foram observadas. Em 1922, Henry E. Niles, em um artigo37 publicado na revista Genetics intitulado Correlation, Causation Wright’s theory path coefficients, fez uma crítica ao método proposto por Wright, afirmando que base filosófica método dos coeficientes de trilha era falha. Niles, testando o método de Wright, evidenciou em alguns de seus resultados coeficientes superiores |1|, afirmando […]estes resultados são ridículos[…] e que Wright teria de fornecer provas bem mais convincentes que ele estava apresentando Niles (1922).ano seguinte, 1923, Sewall Wright em seu artigo38 entitulado theory path coefficients: reply Niles’s criticism, publicado na mesma revista Genetics, consolida seu método ao concluir que Niles pareceu se basear em conceitos matemáticos incorretos, resultado de uma falha em reconhecer que um coeficiente de trilha não é uma função simétrica de duas variáveis, mas que ele necessariamente tem direção. Este autor conclui seu trabalho afirmando que análise de trilha não fornece uma fórmula geral para deduzir relações causais partir conhecimento das correlações. Ela é, entanto, dentro de certas limitações, um método de avaliar consequências lógicas de uma hipótese de relação causal em um sistema de variáveis correlacionadas. Acrescenta ainda que críticas oferecidas por Niles em nada invalidam teoria método ou sua aplicação (Wright 1923). Atualmente, o método estatístico é consolidado, e utilizado mundialmente em diversas áreas da ciência.","code":""},{"path":"relations.html","id":"estimativa-2","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.2 Estimativa","text":"decomposição das correlações lineares em efeitos diretos e indiretos de um conjunto de p-variáveis explicativas é realizada o sistema de equações normais \\[\r\nX'X\\hat \\beta = X'Y\r\n\\]que tem como resolução\\[\r\n\\hat \\beta = X'X^{-1} X'Y\r\n\\]onde \\(\\hat \\beta\\) é o vetor dos coeficiente de regressão parcial (\\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\), \\(\\hat \\beta_3\\),…,\\(\\hat \\beta_p\\)) para p + 1; \\(X'X^{-1}\\) é inversa da matriz de correlação linear entre variáveis explicativas e \\(X'Y\\) é matriz de correlação de cada variável explicativa, com variável dependente.Após estimativa dos coeficientes de regressão (\\(\\hat \\beta_p\\)), os efeitos diretos e indiretos conjunto de p-variáveis explicativas podem ser estimados. Considere o seguinte exemplo, onde um conjunto de variáveis explicativas (, b, c) são utilizadas para explicar relações de causa e efeito  na resposta de uma variável dependente (y). Após estimativas dos coeficientes de regressão parcial (\\(\\hat \\beta_1\\), \\(\\hat \\beta_2\\) e \\(\\hat \\beta_3\\)), os efeitos diretos e indiretos de sobre y são dados por:\\[\r\nr_{:y} = \\hat \\beta_1 + \\hat \\beta_{2_{ra:b}} + \\hat \\beta_{3_{ra:c}}\r\n\\]onde \\(r_{:y}\\) é correlação linear entre e y, \\(\\hat \\beta_1\\) é o efeito direto de em y; \\(\\hat \\beta_{2_{ra:b}}\\) é o efeito indireto de em y via b e \\(\\hat \\beta_{3_{ra:c}}\\) é o efeito indireto de em y via c. Regressões semelhantes são utilizadas para estimativa dos efeitos de b e c, conforme segue:\\[\\begin{gather*}\r\nr_{b:y} = \\hat \\beta_{1_{rb:}} + \\hat \\beta_2 + \\hat \\beta_{3_{rb:c}}\\\\\r\nr_{c:y} = \\hat \\beta_{1_{rc:}} + \\hat \\beta_{2_{rc:b}} + \\hat \\beta_3\r\n\\end{gather*}\\]Como exemplo numérico, vamos utilizar variáveis PERK, EH, CDED como variáveis explicativas e variável KW como dependente, conjunto de dados maize. Para seleção destas variáveis, função select() é utilizada.Utilizando os conhecimentos acumulados até agora, estes mesmos coeficientes podem ser estimados de maneira mais “elegantemente”, utilizando o código abaixo.","code":"\nx <- maize %>% select(PERK, EH, CDED)\ny <- maize %>% select(KW)\n\nxx <- cor(x) # Correlação entre as variáveis explicativas\nxy <- cor(x, y) # Correlação das explicativas com a dependente\nb <- solve(xx) %*% xy # Estimativa dos coeficientes\n\nPERK_KW_DIR <- b[1] # Direto de PERK em KW\nPERK_KW_EH <- b[2] * xx[1,2] # Indireto de PERK em KW via EH\nPERK_KW_CDED <- b[3] * xx[1,3] # Indireto de PERK em KW via CDED\n\nEH_KW_PERK <- b[1] * xx[2,1] # Indireto de EH em KW via PERK\nEH_KW_DIR <- b[2] # Direto de EH em KW\nEH_KW_CDED <- b[3] * xx[2,3] # Indireto de EH em KW via CDED\n\nCDED_KW_PERK <- b[1] * xx[3,1] # Indireto de CDED em KW via PERK\nCDED_KW_EH <- b[2] * xx[3,2] # Indireto de CDED em KW via EH\nCDED_KW_DIR <- b[3] # Direto de CDED em KW\n\n# Coeficientes de trilha (direto na diagonal, indireto fora da diagonal)\ncoeff <- matrix(c(PERK_KW_DIR, PERK_KW_EH, PERK_KW_CDED,\n                  EH_KW_PERK, EH_KW_DIR, EH_KW_CDED,\n                  CDED_KW_PERK, CDED_KW_EH, CDED_KW_DIR),\n                ncol = 3)\nrownames(coeff) <- colnames(coeff) <- c(\"PERK\", \"EH\", \"CDED\")\ncoeff\n#             PERK          EH        CDED\n# PERK -0.10410823 0.002222624  0.05948542\n# EH   -0.01473329 0.690110850 -0.04548514\n# CDED  0.09200901 0.010613425 -0.16102929\nn <- ncol(xx)\nCoeff <- data.frame(xx)\nfor (i in 1:n) {\n  for (j in 1:n) {\n    Coeff[j, i] <- b[j] * xx[j, i]\n  }\n}\nrownames(coeff) <- colnames(coeff) <- c(\"PERK\", \"EH\", \"CDED\")\nCoeff\n#             PERK          EH        CDED\n# PERK -0.10410823 0.002222624  0.05948542\n# EH   -0.01473329 0.690110850 -0.04548514\n# CDED  0.09200901 0.010613425 -0.16102929"},{"path":"relations.html","id":"multicolinearidade","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.3 Multicolinearidade","text":"Embora esta análise revele associações de causa e efeito, sua estimativa é baseada em princípios de regressão múltipla. Assim, estimativas dos parâmetros podem ser enviesadas devido natureza complexa dos dados, em que resposta da variável dependente está ligada um grande número de variáveis explicativas, que são muitas vezes correlacionadas ou multicolineares entre si (Graham 2003). Assim, sempre que duas supostas variáveis explicativas se apresentam altamente associadas, é difícil estimar relações de cada variável explicativa individualmente, uma vez que vários parâmetros resolvem o sistema de equações normais. esta particularidade é atribuída o nome de multicolinearidade  (Blalock 1963).Os principais meios utilizados para identificar o grau de multicolinearidade em uma matriz de variáveis explicativas são os seguintes:Número de condição (CN):  O número de condição é calculado pela razão entre o maior e menor autovalor (\\(\\lambda\\)) da matriz de correlação \\(X'X\\), de acordo com expressão\\[\r\n {\\rm{NC = }}\\frac{{\\lambda_{\\rm{Max}}}}{{\\lambda_{\\rm{Min}}}}\r\n\\]O grau de multicolinearidade é considerado fraco se NC \\(\\leq\\) 100, moderado se 100 \\(\\leq\\) NC \\(\\leq\\) 1000 e severo quando NC > 1000.Determinante da matriz \\(X'X\\) (D):  O determinante da cada matriz de correlação é estimado pelo produto de seus respectivos autovalores, para \\(\\lambda_j > 0\\), de acordo com expressão\\[\r\n\\mathop D\\nolimits_{{\\boldsymbol{X'X}}} {\\rm{  = }}\\prod\\limits_{j = 1}^p {\\lambda j}\r\n\\]Um determinantes muito próximo zero indica dependência linear entre características explicativas, indicando problemas graves de multicolinearidade.Fator de inflação de variância (VIF):  Os (VIFs) são utilizados para medir o quanto variância dos coeficientes de regressão estimados (\\(\\hat \\beta_k\\)) foi inflada em comparação quando os caracteres explicativos não são linearmente associados. estimativa VIF k-ésimo elemento de \\(\\hat \\beta\\) é dada pela soma dos quocientes quadrado de cada componente autovetor pelo seu respectivo autovalor associado, de acordo com expressão\\[\r\n\\mathop {{\\rm{VIF}}}\\nolimits_{\\mathop \\beta \\nolimits_k }  = \\left( {\\frac{{\\mathop {{\\rm{(AV}}}\\nolimits_{KC1} {)^2}}}{{\\lambda 1}} + \\frac{{{{(\\mathop {{\\rm{AV}}}\\nolimits_{KC2} )}^2}}}{{\\lambda 2}} + ... + \\frac{{{{(\\mathop {{\\rm{AV}}}\\nolimits_{KCp} )}^2}}}{{\\lambda p}}} \\right)\r\n \\]onde \\(\\mathop {{\\rm{VIF}}}\\nolimits_{\\mathop \\beta \\nolimits_k }\\) é o fator de inflação de variância o k-ésimo elemento de \\(\\beta\\) para k = 1, 2, …, p; \\(\\mathop {{\\rm{EV}}}\\nolimits_{KC{\\rm{1}}}\\) é o componente k-ésimo autovetor para k = 1, 2, …, p e C = 1, 2, …, p; e \\(\\lambda\\) é o autovalor associado ao respectivo autovetor para \\(\\lambda\\) = 1, 2, …, p. Os VIFs também podem ser considerados como os elementos da diagonal da inversa da matriz \\(X'X\\). Considera-se que presença de VIFs maiores que 10 é um indicativo de multicolinearidade .","code":""},{"path":"relations.html","id":"diagnóstico-da-multicolinearidade","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.3.1 Diagnóstico da multicolinearidade","text":"função colindiag()  pacote metan é utilizada para realizar o diagnóstico da multicolinearidade de uma matriz de correlação. Os códigos seguir calculam um diagnóstico completo de colinearidade de uma matriz de correlação de características preditivas. Vários indicadores, como fator de inflação de variação, número de condição e determinante de matriz são considerados (T. Olivoto, Souza, et al. 2017; Tiago Olivoto, Nardino, et al. 2017) O diagnóstico pode ser realizado usando: () matrizes de correlação; (ii) quadros de dados ou (iii) um objeto agrupado passado de group_by().Usando uma matriz de correlação, estimada anteriormenteUsando um dataframeDiagnóstico para cada nível fator ENV","code":"\ncor_data <- cor(datacor)\nn <- nrow(datacor)\ncold1 <- colindiag(cor_data, n = n)\nprint(cold1)\n# Severe multicollinearity in the matrix! Pay attention on the variables listed bellow\n# CN = 1830.678\n# Matrix determinant: 2.32e-05 \n# Largest correlation: PH x EH = 0.932 \n# Smallest correlation: CD x EP = 0.175 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 3 \n# Variables with largest weight in the last eigenvalues: \n# EH > PH > EP > CL > ED > CW > CD > EL\ncold2 <- colindiag(datacor)\nprint(cold2)\n# Severe multicollinearity in the matrix! Pay attention on the variables listed bellow\n# CN = 1830.678\n# Matrix determinant: 2.32e-05 \n# Largest correlation: PH x EH = 0.932 \n# Smallest correlation: CD x EP = 0.175 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 3 \n# Variables with largest weight in the last eigenvalues: \n# EH > PH > EP > CL > ED > CW > CD > EL\ncold3 <- colindiag(data_ge2, by = ENV)\nprint(cold3)\n# # A tibble: 4 x 2\n#   ENV   data      \n#   <fct> <list>    \n# 1 A1    <colindig>\n# 2 A2    <colindig>\n# 3 A3    <colindig>\n# 4 A4    <colindig>"},{"path":"relations.html","id":"seleção-de-preditores-com-mínima-multicolinearidade","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.3.2 Seleção de preditores com mínima multicolinearidade","text":"função non_collinear_vars() seleciona um conjunto de preditores com multicolinearidade mínima usando o fator de inflação de variação (VIF) como critério para remover variáveis colineares. O algoritmo irá: () calcular o valor VIF da matriz de correlação que contém variáveis originais; (ii) ordenar os valores VIF e excluir variável com maior VIF; e (iii) iterar etapa ii até que o valor VIF seja menor ou igual um valor pré-estabelecido.Em análise de trilha, o diagnóstico da multicolinearidade deve ser realizado na matriz de correlação das variáveis explicativas. exemplo acima, supondo que análise de trilha fosse realizada considerando variável PH como dependente, o seguinte comando deveria ter sido utilizado para o diagnóstico da multicolinearidade.multicol <- datacor %>% select(-PH) %>% colindiag()","code":"\nnon_collinear_vars(data_ge2)\n#          Parameter                                       values\n# 1       Predictors                                           10\n# 2              VIF                                         7.16\n# 3 Condition Number                                       56.797\n# 4      Determinant                                 0.0008810515\n# 5         Selected PERK, EP, CDED, NKR, PH, NR, TKW, EL, CD, ED\n# 6          Removed                          EH, CL, CW, KW, NKE\nnon_collinear_vars(data_ge2, EH, CL, CW, KW, NKE, max_vif = 5)\n#          Parameter          values\n# 1       Predictors               4\n# 2              VIF           2.934\n# 3 Condition Number          11.248\n# 4      Determinant    0.2400583901\n# 5         Selected NKE, EH, CL, CW\n# 6          Removed              KW"},{"path":"relations.html","id":"métodos-para-ajustar-a-multicolinearidade","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.4 Métodos para ajustar a multicolinearidade","text":"Embora os problemas relacionados multicolinearidade  se apresente como uma dificuldade na estimativa de coeficientes de trilha, algumas medidas podem ser tomadas visando mitigar seus efeitos indesejáveis, quando esta detectada pelos métodos acima citados. Sabe-se atualmente, que exclusão das variáveis responsáveis por inflar variância de um coeficiente de regressão é um dos métodos mais indicados para reduzir multicolinearidade em matrizes de variáveis explicativas (T. Olivoto, Souza, et al. 2017). identificação destas variáveis, entanto, pode ser uma tarefa difícil. Recentemente, T. Olivoto, Nardino, et al. (2017) propuseram utilização de procedimentos stepwise  juntamente com análise de trilha sequencial visando identificar um conjunto de variáveis com alto poder explicativo, mas que não se apresentem altamente correlacionadas. Quando exclusão das variáveis responsáveis não é um procedimento considerado pelo pesquisador, por exemplo, devido um número reduzido de variáveis explicativa, ou pela importância em conhecer seus efeitos, uma terceira opção é realizar análise de trilha com todas variáveis explicativas, porém com inclusão de um pequeno valor nos elementos da diagonal \\(X'X\\), conhecida como regressão em crista39. Este procedimento, entanto superestima os efeitos diretos, principalmente daquelas variáveis com alto VIF. (T. Olivoto, Souza, et al. 2017).","code":""},{"path":"relations.html","id":"análise-tradicional","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.5 Análise tradicional","text":"Esta sessão está focada principalmente em três objetivos: () diagnóstico da multicolinearidade; (ii) seleção de variáveis  preditoras; e (iii) estimação dos coeficientes de trilha. Embora esta seja sequência correta ser seguida para estimativa dos coeficientes de trilha, utilizaremos somente função path_coeff(),  que possibilita todas estas abordagens. Para isto, o conjunto de dados maize será utilizado.Com função acima, os coeficientes de trilha foram estimados considerando variável KW (massa de grãos por espiga) como dependente, e todas outras variáveis numéricas conjunto de dados como explicativas. função summary() pode ser utilizada para resumir os resultados da análise. Note que os coeficientes foram estimados considerando todos os níveis dos fatores ENV, GEN, e REP.mensagem de aviso gerada pela função acima pode ser suprimidada utilizando o argumento verbose = FALSEDe acordo com o NC, VIF e determinante da matriz, multicolinearidade  na matriz das variáveis explicativa é severa. Por exemplo, foram observados oito VIFs > 10 e o determinante da matriz foi de \\(8.619 \\times 10^{-12}\\). análise dos autovalores-autovetores (pathtodas$weightvar) indicou que, em ordem de importância, variáveis que mais contribuem para multicolinearidadeque são: CL > ED > CDED > EH > CW > PH > NKE > EP > TKW > PERK > NR > EL > NKR > CD. Conforme discutido, temos basicamente duas opções para contornar os problemas da elevada multicolineridade em nossos dados. Excluir variáveis responsáveis pela multicolinearidade, ou manter todas variáveis e incluir um fator de correção na diagonal da matrix . Vamos começar pela última opção.","code":"\npathtodas <- \n  maize %>%\n  path_coeff(KW, verbose = FALSE)"},{"path":"relations.html","id":"incluindo-um-fator-de-correção-k","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.6 Incluindo um fator de correção (k)","text":"variância dos coeficientes de regressão é reduzida quando quando um valor k para \\(0 < k \\leq 1\\) é incluído na diagonal da matriz de correlação \\(X'X\\). Com esta técnica, estimativa dos coeficientes de regressão é dado por:\\[\r\n\\hat \\beta = (X'X+Ik)^{-1} X'Y\r\n\\]escolha da magnitude de k, entanto, deve ser cautelosa. Sugere-se que o valor ser incluído seja aquele menor possível que estabilize os coeficientes de regressão (\\(\\beta_p\\)). Felizmente, grande parte deste trabalho já foi realizado quando utilizamos função path_coeff()  na sessão anterior. Um conjunto de estimativas de \\(\\beta_p\\) foi estimado com 101 valores de k, (\\(k = 0, 0.01, ..., 1\\)) e representado graficamente.\r\nFigure 12.5: Valores de beta obtidos com 101 valores de k\r\nprint()O gráfico acima nos proporciona uma interpretação sobre qual é o valor de k mais indicado ser utilizado. Para fins didátidos, escolheremos, por enquanto, o valor de k igual 0.04, valor qual os coeficientes de regressão da maioria das variávies se estabiliza. Para incluir este valor de correção, utilizaremos novamente função path_coeff(), entanto, agora, incluiremos o argumento correction = 0.04.Com inclusão fator de correção (k = 0.04) na diagonal de \\(X'X\\), multicolinearidade foi classificada como moderada (NC = 141). Inevitavelmente, temos duas opções para obtermos menores níveis de multicollinearidade. primeira é aumentar o valor de k, digamos, para 0.1. Isto iria reduzir ainda mais o nível de multicolinearidade  em nossa matriz, entanto, o viés na estimativa dos coeficientes aumentaria. segunda (e mais razoável) opção, é exclusão das variáveis que mais causam problemas de multicolinearidade. Por exemplo, podemos considerar variáveis com maior peso nos últimos autovalores, ou aquelas com maior VIF. variáveis CL e EL apresentam alta correlação (veja seção estimativas dos coeficientes de correlação), assim poderíamos manter apenas uma destas variáveis. mesma interpretação pode ser considerada para CDED. Esta é uma co-variável (razão diâmetro sabugo e diâmetro da espiga, CDED = CD/ED). Vamos considerar então exclusão destas variáveis.","code":"\npathtodas$plot\npathtodas_k <- \n  maize %>% \n  path_coeff(KW, correction = 0.04, verbose = FALSE)"},{"path":"relations.html","id":"excluindo-variáveis","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.7 Excluindo variáveis","text":"O ajuste novo modelo excluindo estas variáveis é facilmente realizado. Para isto, iremos utilizar dois argumentos da função path_coeff()  não vistos até agora: pred e exclude. variáveis informadas em pred podem ser variáveis preditoras (default) ou variáveis serem excluídas, se exclude = TRUE. Vamos ao exemplo.Abaixo, um resumo das três abordagens realizadas até agora, utilizando o resumo apresentado na tabela abaixo.Conforme também observado por Hoerl Kennard (1970) e T. Olivoto, Souza, et al. (2017), exclusão de variáveis responsáveis pela multicolinearidade  foi mais eficiente que inclusão k, proporcionando menores níveis de multicolinearidade e maior precisão modelo (maior \\(R^2\\) e menor residual). Os níveis de multicolinearidade ao excluir variáveis ainda preocupam. Vimos que tanto identificação das variáveis responsáveis pela multicolinearidae quanto o ajuste modelo declarando preditores específicos é um procedimento relativamente simples utilização função path_coeff(). Mas, e se algum procedimento estatistico-computacional facilitasse ainda mais essa tarefa? Vamos, partir de agora, considerar isso.T. Olivoto, Nardino, et al. (2017) sugeriram utilização de regressões stepwise para seleção de um conjunto de preditores com minima multicolinearidade em análise de trilha. Esta opção está disponível na função path_coeff(). Baseado em um algoritmo heurístico iterativo executado pelo argumento brutstep = TRUE, um conjunto de preditores com mínima multicolienaridade é selecionado com base nos valores de VIF. Posteriormente, uma série de regressões stepwise são ajustadas. primeira regressão stepwise é ajustada considerando \\(p-1\\) variáveis preditoras selecionadas, sendo p o número de variáveis selecionadas processo iterativo. O segundo modelo ajusta uma regressão considerando \\(p-2\\) variávies selecionadas, e assim por diante até o último modelo, que considera apenas duas variáveis selecionadas. Vamos ao exemplo.","code":"\npath_exclude <- \n  maize %>% \n  path_coeff(resp = KW,\n             pred = c(PERK, EH, CDED),\n             exclude = TRUE,\n             verbose = FALSE)"},{"path":"relations.html","id":"seleção-de-variáveis-em-análise-de-trilha","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.8 Seleção de variáveis em análise de trilha","text":" Três objetos são criados por esta função: Summary, Models e Selectedpred. O objeto Summary contém um resumo procedimento, listando o número modelo, o valor AIC, o diagnóstico da multicolinearidade  e os valores de R2 e residual. O objeto Models, contém todos os modelos ajustados, e o objeto Selectedpred, contém o nome das variávies preditoras selecionadas processo iterativo. Podemos notar que o algorítmo selecionou um conjunto com 10 preditores (PERK, EP, CDED, NKR, PH, NR, TKW, EL, CD, ED) que apresenta multicolinearidade em niveis aceitáveis. Assim, qualquer um destes modelos poderia ser utilizado sem maiores problemas em relação à isto. O procedimento stepwise realizado com diferentes números de variáveis selecionados também permite seleção de um modelo mais parcimonio, tarefa que ficará critério pesquisador.","code":"\npath_step <- \n  maize %>% \n  path_coeff(resp = KW,\n             brutstep = TRUE)\n# --------------------------------------------------------------------------\n# The algorithm has selected a set of 10 predictors with largest VIF = 7.16. \n# Selected predictors: PERK EP CDED NKR PH NR TKW EL CD ED \n# A forward stepwise-based selection procedure will fit 8 models.\n# --------------------------------------------------------------------------\n# Adjusting the model 1 with 9 predictors (12.5% concluded)\n# Adjusting the model 2 with 8 predictors (25% concluded)\n# Adjusting the model 3 with 7 predictors (37.5% concluded)\n# Adjusting the model 4 with 6 predictors (50% concluded)\n# Adjusting the model 5 with 5 predictors (62.5% concluded)\n# Adjusting the model 6 with 4 predictors (75% concluded)\n# Adjusting the model 7 with 3 predictors (87.5% concluded)\n# Adjusting the model 8 with 2 predictors (100% concluded)\n# Done!\n# --------------------------------------------------------------------------\n# Summary of the adjusted models \n# --------------------------------------------------------------------------\n#    Model  AIC Numpred    CN Determinant    R2 Residual maxVIF\n#  MODEL_1 1099       9 50.99     0.00216 0.945   0.0545   6.96\n#  MODEL_2 1098       8 45.48     0.00396 0.945   0.0550   6.80\n#  MODEL_3 1097       7 37.46     0.02618 0.944   0.0555   6.42\n#  MODEL_4 1103       6 35.70     0.03481 0.942   0.0582   5.71\n#  MODEL_5 1116       5 26.39     0.08049 0.936   0.0642   5.71\n#  MODEL_6 1129       4 21.07     0.17146 0.930   0.0705   5.71\n#  MODEL_7 1170       3  4.97     0.55639 0.907   0.0933   1.78\n#  MODEL_8 1232       2  1.57     0.95068 0.860   0.1402   1.05\n# --------------------------------------------------------------------------"},{"path":"relations.html","id":"análise-de-trilha-para-cada-nível-de-um-fator","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.7.9 Análise de trilha para cada nível de um fator","text":"Em alguns casos, é de interesse estimar os coeficientes de trilha para cada nível de um fator, por exemplo, para cada ambiente em um ensaio multi-ambiente. Utilizando o argumento , isto é facilmente realizado. . Para fins didáticos vamos estimar os coeficientes para cada ambiente conjunto de dados maize. ","code":"\npath_levels <- \n  maize %>%\n  group_by(ENV) %>%\n  path_coeff(resp = KW,\n             pred = c(TKW, NKE, PERK))\n# Weak multicollinearity. \n# Condition Number = 4.697\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 2.296\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 4.244\n# You will probably have path coefficients close to being unbiased. \n# Weak multicollinearity. \n# Condition Number = 1.884\n# You will probably have path coefficients close to being unbiased."},{"path":"relations.html","id":"correlação-canônica","chapter":"Capítulo 12 Relações lineares entre variáveis","heading":"12.8 Correlação canônica","text":"Correlações canônicas podem ser implementadas com função can_corr(). Primeiro, renomearemos variáveis relacionadas à planta PH EH e EP com o sufixo _PLA para mostrar usabilidade select helper contains().$$","code":"\ndata_cc <- \n  rename(data_ge2,\n         PH_PLA = PH,\n         EH_PLA = EH,\n         EP_PLA = EP)\n\n# Digitar nome das variáveis\ncc1 <- \n  can_corr(data_cc,\n           FG = c(PH_PLA, EH_PLA, EP_PLA),\n           SG = c(EL, ED, CL, CD, CW, KW))\n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of first group (FG)\n# ---------------------------------------------------------------------------\n#           PH_PLA    EH_PLA    EP_PLA\n# PH_PLA 1.0000000 0.9318282 0.6384123\n# EH_PLA 0.9318282 1.0000000 0.8695460\n# EP_PLA 0.6384123 0.8695460 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within first group \n# ---------------------------------------------------------------------------\n# The multicollinearity in the matrix should be investigated.\n# CN = 977.586\n# Largest VIF = 229.164618380199\n# Matrix determinant: 0.0025852 \n# Largest correlation: PH_PLA x EH_PLA = 0.932 \n# Smallest correlation: PH_PLA x EP_PLA = 0.638 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# EH_PLA > PH_PLA > EP_PLA \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of second group (SG)\n# ---------------------------------------------------------------------------\n#           EL        ED        CL        CD        CW        KW\n# EL 1.0000000 0.3851451 0.2554068 0.9118653 0.4581728 0.6685601\n# ED 0.3851451 1.0000000 0.6974629 0.3897128 0.7371305 0.8241426\n# CL 0.2554068 0.6974629 1.0000000 0.3003636 0.7383379 0.4709310\n# CD 0.9118653 0.3897128 0.3003636 1.0000000 0.4840299 0.6259806\n# CW 0.4581728 0.7371305 0.7383379 0.4840299 1.0000000 0.7348622\n# KW 0.6685601 0.8241426 0.4709310 0.6259806 0.7348622 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within second group \n# ---------------------------------------------------------------------------\n# Weak multicollinearity in the matrix\n# CN = 66.084\n# Matrix determinant: 0.0028626 \n# Largest correlation: EL x CD = 0.912 \n# Smallest correlation: EL x CL = 0.255 \n# Number of VIFs > 10: 0 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# KW > EL > ED > CD > CL > CW \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between FG and SG\n# ---------------------------------------------------------------------------\n#               EL        ED        CL        CD        CW        KW\n# PH_PLA 0.3801960 0.6613148 0.3251648 0.3153910 0.5047388 0.7534439\n# EH_PLA 0.3626537 0.6302561 0.3971935 0.2805118 0.5193136 0.7029469\n# EP_PLA 0.2634237 0.4580196 0.3908239 0.1750448 0.4248098 0.4974193\n# ---------------------------------------------------------------------------\n# Correlation of the canonical pairs and hypothesis testing \n# ---------------------------------------------------------------------------\n#              Var   Percent       Sum      Corr  Lambda     Chisq DF   p_val\n# U1V1 0.630438540 78.617161  78.61716 0.7940016 0.30668 177.29224 18 0.00000\n# U2V2 0.163384310 20.374406  98.99157 0.4042083 0.82985  27.97651 10 0.00182\n# U3V3 0.008086721  1.008433 100.00000 0.0899262 0.99191   1.21794  4 0.87514\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the first group \n# ---------------------------------------------------------------------------\n#               U1        U2         U3\n# PH_PLA  2.609792  5.490798   7.575090\n# EH_PLA -2.559005 -7.646096 -12.812234\n# EP_PLA  1.191023  2.428742   6.604968\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the second group \n# ---------------------------------------------------------------------------\n#             V1         V2          V3\n# EL -0.01008726 -1.0481893  0.60553720\n# ED  0.14629899  0.7853469 -1.30457763\n# CL -0.09112023 -1.2989864 -0.07497186\n# CD -0.29105227  1.1513083 -1.50589651\n# CW -0.12527616 -0.0361706  0.21180796\n# KW  1.16041981 -0.1022916  1.34278026\n# ---------------------------------------------------------------------------\n# Canonical loads of the first group \n# ---------------------------------------------------------------------------\n#               U1          U2         U3\n# PH_PLA 0.9856022 -0.08351129 -0.1470178\n# EH_PLA 0.9085216 -0.41771278 -0.0102277\n# EP_PLA 0.6319736 -0.71449671  0.3001730\n# ---------------------------------------------------------------------------\n# Canonical loads of the second group \n# ---------------------------------------------------------------------------\n#           V1          V2         V3\n# EL 0.4759982 -0.11260907 -0.2944636\n# ED 0.8294407 -0.18663860 -0.4477426\n# CL 0.3749015 -0.74801793 -0.4937819\n# CD 0.3951578  0.02985218 -0.5415818\n# CW 0.6225367 -0.41451273 -0.2698904\n# KW 0.9570820 -0.07344796 -0.1498587\n\n# Use select helpers\ncc2 <- \n  can_corr(data_cc,\n           FG = contains(\"_PLA\"),\n           SG = c(EL, ED, CL, CD, CW, KW))\n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of first group (FG)\n# ---------------------------------------------------------------------------\n#           PH_PLA    EH_PLA    EP_PLA\n# PH_PLA 1.0000000 0.9318282 0.6384123\n# EH_PLA 0.9318282 1.0000000 0.8695460\n# EP_PLA 0.6384123 0.8695460 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within first group \n# ---------------------------------------------------------------------------\n# The multicollinearity in the matrix should be investigated.\n# CN = 977.586\n# Largest VIF = 229.164618380199\n# Matrix determinant: 0.0025852 \n# Largest correlation: PH_PLA x EH_PLA = 0.932 \n# Smallest correlation: PH_PLA x EP_PLA = 0.638 \n# Number of VIFs > 10: 3 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# EH_PLA > PH_PLA > EP_PLA \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between variables of second group (SG)\n# ---------------------------------------------------------------------------\n#           EL        ED        CL        CD        CW        KW\n# EL 1.0000000 0.3851451 0.2554068 0.9118653 0.4581728 0.6685601\n# ED 0.3851451 1.0000000 0.6974629 0.3897128 0.7371305 0.8241426\n# CL 0.2554068 0.6974629 1.0000000 0.3003636 0.7383379 0.4709310\n# CD 0.9118653 0.3897128 0.3003636 1.0000000 0.4840299 0.6259806\n# CW 0.4581728 0.7371305 0.7383379 0.4840299 1.0000000 0.7348622\n# KW 0.6685601 0.8241426 0.4709310 0.6259806 0.7348622 1.0000000\n# ---------------------------------------------------------------------------\n# Collinearity within second group \n# ---------------------------------------------------------------------------\n# Weak multicollinearity in the matrix\n# CN = 66.084\n# Matrix determinant: 0.0028626 \n# Largest correlation: EL x CD = 0.912 \n# Smallest correlation: EL x CL = 0.255 \n# Number of VIFs > 10: 0 \n# Number of correlations with r >= |0.8|: 2 \n# Variables with largest weight in the last eigenvalues: \n# KW > EL > ED > CD > CL > CW \n# ---------------------------------------------------------------------------\n# Matrix (correlation/covariance) between FG and SG\n# ---------------------------------------------------------------------------\n#               EL        ED        CL        CD        CW        KW\n# PH_PLA 0.3801960 0.6613148 0.3251648 0.3153910 0.5047388 0.7534439\n# EH_PLA 0.3626537 0.6302561 0.3971935 0.2805118 0.5193136 0.7029469\n# EP_PLA 0.2634237 0.4580196 0.3908239 0.1750448 0.4248098 0.4974193\n# ---------------------------------------------------------------------------\n# Correlation of the canonical pairs and hypothesis testing \n# ---------------------------------------------------------------------------\n#              Var   Percent       Sum      Corr  Lambda     Chisq DF   p_val\n# U1V1 0.630438540 78.617161  78.61716 0.7940016 0.30668 177.29224 18 0.00000\n# U2V2 0.163384310 20.374406  98.99157 0.4042083 0.82985  27.97651 10 0.00182\n# U3V3 0.008086721  1.008433 100.00000 0.0899262 0.99191   1.21794  4 0.87514\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the first group \n# ---------------------------------------------------------------------------\n#               U1        U2         U3\n# PH_PLA  2.609792  5.490798   7.575090\n# EH_PLA -2.559005 -7.646096 -12.812234\n# EP_PLA  1.191023  2.428742   6.604968\n# ---------------------------------------------------------------------------\n# Canonical coefficients of the second group \n# ---------------------------------------------------------------------------\n#             V1         V2          V3\n# EL -0.01008726 -1.0481893  0.60553720\n# ED  0.14629899  0.7853469 -1.30457763\n# CL -0.09112023 -1.2989864 -0.07497186\n# CD -0.29105227  1.1513083 -1.50589651\n# CW -0.12527616 -0.0361706  0.21180796\n# KW  1.16041981 -0.1022916  1.34278026\n# ---------------------------------------------------------------------------\n# Canonical loads of the first group \n# ---------------------------------------------------------------------------\n#               U1          U2         U3\n# PH_PLA 0.9856022 -0.08351129 -0.1470178\n# EH_PLA 0.9085216 -0.41771278 -0.0102277\n# EP_PLA 0.6319736 -0.71449671  0.3001730\n# ---------------------------------------------------------------------------\n# Canonical loads of the second group \n# ---------------------------------------------------------------------------\n#           V1          V2         V3\n# EL 0.4759982 -0.11260907 -0.2944636\n# ED 0.8294407 -0.18663860 -0.4477426\n# CL 0.3749015 -0.74801793 -0.4937819\n# CD 0.3951578  0.02985218 -0.5415818\n# CW 0.6225367 -0.41451273 -0.2698904\n# KW 0.9570820 -0.07344796 -0.1498587"},{"path":"multivariate.html","id":"multivariate","chapter":"Capítulo 13 Análise multivariada","heading":"Capítulo 13 Análise multivariada","text":"melhoramento genético de plantas, diversas variáveis são mensuradas em cada genótipo, visando maior segurança na distinção de tais genótipos. Embora em alguns casos possa fazer sentido isolar cada variável e estudá-la separadamente, geralmente, uma análise que englobe todas variáveis fornece um maior número de informações. Como todo o conjunto de variáveis é medido em cada genótipo, variáveis serão relacionadas em maior ou menor grau. Consequentemente, se cada variável é analisada isoladamente, estrutura completa dos dados pode não ser revelada. análise multivariada é análise estatística simultânea de uma coleção de variáveis que utilzia informações sobre relações entre estas. É muito provável que análise de cada variável separadamente não revele padrões interessantes que análise multivariada proporciona.concepção da análise multivariada  é provavelmente o trabalho realizado por Francis Galton e Karl Pearson final século XIX sobre quantificação da relação entre descendentes e características parentais e o desenvolvimento coeficiente de correlação (Galton 1888). Naquele tempo, o processamento computacional era muito limitado para suportar o peso das vastas quantidades de aritmética envolvidas na aplicação dos métodos multivariados que estavam sendo propostos. Assim, os desenvolvimentos eram principalmente matemáticos e pesquisa multivariada era, na época, em grande parte, um ramo de álgebra linear. entanto, chegada e rápida expansão uso de computadores eletrônicos na segunda metade século XX, levou à crescente aplicação prática dos métodos existentes de análise multivariada, renovando o interesse desenvolvimento de novas técnicas.Nos primeiros anos século XXI, ampla disponibilidade de computadores pessoais e laptops relativamente baratos e extremamente poderosos, aliados softwares estatísticos flexíveis fez com que todos os métodos de análise multivariada pudessem ser aplicados rotineiramente, mesmo para grandes conjuntos de dados, como os gerados em um programa de melhoramento genético –por exemplo, dados de marcadores moleculares e sequenciamento gênico.","code":""},{"path":"multivariate.html","id":"correlações-canônicas","chapter":"Capítulo 13 Análise multivariada","heading":"13.1 Correlações canônicas","text":"Correlações canonicas podem ser computadas utilizando função can_corr() pacote metan. O primeiro argumento da função é o conjunto de dados (opcional) que deve conter variáveis numéricas que serão usadas na estimativa das correlações canônicas. Os grupos de variáveis são definidos pelos argumentos FG (primeiro/menor grupo) e SG (segundo/maior grupo). Por padrão, um diagnótico da multicolinearidade é realizado em cada grupo de variável. exemplo abaixo, os coeficientes foram armazenados objeto cc1. Note que o argumento verbose = FALSE foi utilizado para previnir uma longa saída.Na função can_corr(), os dados também podem ser passados diretamente pelos argumentos FG e SG, por exemplo, FG = maize[, 4:6]. Alternativamente, dados podem ser passados da função split_factors(). Nesse caso, correlações canônicas serão estimadas para cada nível da variável de agrupamento nessa função.","code":"\ncc1 = can_corr(maize,\n               FG = c(PH, EH, EP),\n               SG = c(EL, ED, CL, CD, CW, KW, NR),\n               verbose = FALSE)\nprint(cc1$Sigtest, digits = 2)\n#        Var Percent Sum Corr Lambda Chisq DF  p_val\n# U1V1 0.632    76.2  76 0.79   0.30 181.8 21 0.0000\n# U2V2 0.187    22.5  99 0.43   0.80  32.5 12 0.0012\n# U3V3 0.011     1.3 100 0.10   0.99   1.6  5 0.9015"},{"path":"multivariate.html","id":"análise-de-agrupamento","chapter":"Capítulo 13 Análise multivariada","heading":"13.2 Análise de agrupamento","text":"análise de agrupamento  é um procedimento multivariado muito útil melhoramento de plantas. O princípio básico é agrupar indivíduos (genótipos) de acordo com suas semelhanças (variáveis analizadas). Esta sessão é focada na estimativa de matrizes de distâncias e na implementação de algorítimos aglomerativos de agrupamento hierárquicos para confecção de dendrogramas.  função clustering()  pacote metan será utilizada para este fim.Existem muitos métodos para calcular informações de (di)similaridade. opções incluidas na função são: “euclidean” (padrão), “maximum”, “manhattan”, “canberra”, “binary”, “minkowski”, “pearson”, “kendall” e “spearman”. Estas três últimas são distâncias baseadas em correlação. Para maiores informações veja ?clustering.","code":""},{"path":"multivariate.html","id":"todas-as-linhas-e-todas-as-variáveis-numéricas","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.1 Todas as linhas e todas as variáveis numéricas","text":"Por padrão, função computa distancias para cada combinação de linhas conjunto de dados, utilizando todas variáveis numéricas conjunto. Isto significa que, considerando o conjunto de dados maize, com 156 observações, 12090 distancias serão computadas baseadas nas 15 variáveis numéricas conjunto.","code":"\nd1 <- clustering(maize)"},{"path":"multivariate.html","id":"com-base-na-média-de-cada-genótipo","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.2 Com base na média de cada genótipo","text":"Supondo que o pesquisador deseja computar distancias entre cada genótipo (o que é lógico em melhoramento genético vegetal) e que esta distancia deve ser computada apenas com algumas variáveis numéricas conjunto de dados, o seguinte código deverá ser utilizado. Para selecionar variáveis serem utilziadas, basta apenas fornecer uma lista de nomes separadas por vírgula e sem o uso conhecido (e ultrapassado) \"\". Em adição, para que distância seja computada entre os genótipos, basta passar os dados médios de cada genótipo computados com means_by() . Neste caso, média de cada genótipo é calculada internamente para cada variável numérica e distãncia é computada utilizando estas médias. função plot() pode ser usada para plotar um dendrograma. Uma linha é desenhada ponto de corte sugerido de acordo com Mojena (1977).dendrograma exibido acima, cada folha corresponde um genótipo. À medida que subimos na árvore, genótipos que são semelhantes uns aos outros são combinados em ramos, que vão sendo fundidos uma altura cada vez maior maior. altura da fusão, fornecida eixo vertical, indica (di)similaridade/distância entre dois genótipos. Quanto maior altura da fusão, menos semelhantes são os genótipos.Após confecção dendrograma , convém avaliar se distâncias (ou seja, alturas) na árvore refletem distâncias originais com precisão. Uma maneira de medir o quão bem o dendrograma gerado reflete seus dados é calcular correlação entre distâncias cofenéticas e matriz de de distância originais. procedimento anterior o dendrograma não foi mostrado, entanto, o coeficiente de correlação cofenético foi calculado. Para isto basta incluir o seguinte comando:Quanto mais próximo o valor coeficiente de correlação de 1, mais precisamente o dendrograma  refletirá distâncias originais. Valores acima de 0,75 são considerados bons. O método de ligação “average”, ou UPGMA (padrão na função clustering() parece produzir altos valores dessa estatística. Esta pode ser uma das razões por que ele é tão popular.","code":"\nmgen <- \n  maize %>% \n  means_by(GEN) %>% \n  column_to_rownames(\"GEN\")\nd2 <- clustering(mgen, NKR, TKW, NKE)\nplot(d2, horiz = FALSE, ylab = \"Distância euclidiana\")\nd2$cophenetic\n# [1] 0.8640355"},{"path":"multivariate.html","id":"seleção-de-variáveis-1","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.3 Seleção de variáveis","text":"função clustering()  também conta com um algorítimo de seleção de variáveis . O objetivo é selecionar um grupo de variáveis que mais contribuam para explicar variabilidade dos dados originais. Digamos que se algumas poucas variáveis pudessem ser utilizadas para agrupar os genótipos sem que haja perda de informação, recursos humanos e financeiros poderiam ser poupados. Assim ao envés de avaliarmos 15 variáveis (em nosso exemplo), poderiamos avaliar somente aquelas que realmente contribuiem para distinção dos genótipos.O algoritmo de seleção de variáveis é executado quando o argumento selvar = TRUE é incluído na função. seleção das variáveis é baseada na solução de autovalores/autovetores baseada nos seguintes passos: 1: calcular matriz de distância e correlação cofenética com variáveis originais (todas variáveis numéricas conjunto de dados); 2: calcular os autovalores e autovetores da matriz de correlação entre variáveis; 3: deletar variável com o maior peso (maior autovetor menor autovalor); 4: calcular matriz de distância e correlação cofenética com variáveis que restaram; 5: calcular correlação de Mantel entre matriz de distâncias obtidas e matriz de distância original; 6: iterar os passos 2 5 p - 2 vezes, onde p é o número de variáveis originais. final das iterações, um resumo dos modelos é retornado. distância é calculada com variáveis que geraram o modelo com maior correlação copenética. Sugerimos uma avaliação criteriosa com o objetivo de escolher um modelo parcimonioso, ou seja, aquele com menor número de variáveis, que apresente correlação cofenética aceitável e alta similaridade com distâncias originais.saída acima nos mostra o progresso algorítmo, indicando qual foi variável excluída em cada passo. Isto pode ser omitido, indicando verbose = FALSE na função. O resumo modelo (Summary adjusted models) nos fornece duas informações muito úteis. Primeira: ao reduzir o número de variáveis utilziadas na estimativa das distâncias, o coeficiente de correlação cofenético não reduziu significativamente, apresentando variação apenas na terceira casa decimal. segunda, e talvez mais importante, é correlação de mantel realizada com matriz de distâncias em cada passo da análise com matriz de distâncias inicial, que foi computada com todas variáveis. Percebe-se que utilizando apenas duas variáveis, distâncias calculadas foram praticamente idênticas (r = 0.992) às distâncias calculadas com todas variáveis. Por padrão, o algorítmo estima matriz de distâncias considerando variáveis modelo com maior coeficiente de correlação cofenética. Em nosso exemplo, variáveis utilizadas foram ED, CW, KW, NKR, TKW, e NKE. Veja que reduzimos em um terço o número de variáveis necessárias para diferenciar os tratamentos, sem perda de informação. Salienta-se, entanto, que este resultado pode não ser reproduzido em um conjunto de dados diferente, cabendo usuário decidir qual é o melhor modelo ser utilizado.","code":"\nd3 <- clustering(mgen, selvar = TRUE)\n# Model 1 with 15 variables. 'EH' excluded in this step (7.1%).\n# Model 2 with 14 variables. 'EP' excluded in this step (14.3%).\n# Model 3 with 13 variables. 'CDED' excluded in this step (21.4%).\n# Model 4 with 12 variables. 'PH' excluded in this step (28.6%).\n# Model 5 with 11 variables. 'CL' excluded in this step (35.7%).\n# Model 6 with 10 variables. 'NR' excluded in this step (42.9%).\n# Model 7 with 9 variables. 'PERK' excluded in this step (50%).\n# Model 8 with 8 variables. 'EL' excluded in this step (57.1%).\n# Model 9 with 7 variables. 'CD' excluded in this step (64.3%).\n# Model 10 with 6 variables. 'ED' excluded in this step (71.4%).\n# Model 11 with 5 variables. 'KW' excluded in this step (78.6%).\n# Model 12 with 4 variables. 'CW' excluded in this step (85.7%).\n# Model 13 with 3 variables. 'NKR' excluded in this step (92.9%).\n# Model 14 with 2 variables. 'TKW' excluded in this step (100%).\n# Done! \n# -------------------------------------------------------------------------- \n# \n# Summary of the adjusted models \n# -------------------------------------------------------------------------- \n#     Model excluded cophenetic remaining cormantel    pvmantel\n#   Model 1        -  0.8656190        15 1.0000000 0.000999001\n#   Model 2       EH  0.8656191        14 1.0000000 0.000999001\n#   Model 3       EP  0.8656191        13 1.0000000 0.000999001\n#   Model 4     CDED  0.8656191        12 1.0000000 0.000999001\n#   Model 5       PH  0.8656189        11 1.0000000 0.000999001\n#   Model 6       CL  0.8655939        10 0.9999996 0.000999001\n#   Model 7       NR  0.8656719         9 0.9999982 0.000999001\n#   Model 8     PERK  0.8657259         8 0.9999977 0.000999001\n#   Model 9       EL  0.8657904         7 0.9999972 0.000999001\n#  Model 10       CD  0.8658997         6 0.9999964 0.000999001\n#  Model 11       ED  0.8658274         5 0.9999931 0.000999001\n#  Model 12       KW  0.8643556         4 0.9929266 0.000999001\n#  Model 13       CW  0.8640355         3 0.9927593 0.000999001\n#  Model 14      NKR  0.8648384         2 0.9925396 0.000999001\n# --------------------------------------------------------------------------\n# Suggested variables to be used in the analysis \n# -------------------------------------------------------------------------- \n# The clustering was calculated with the  Model 10 \n# The variables included in this model were...\n#  ED CW KW NKR TKW NKE \n# --------------------------------------------------------------------------"},{"path":"multivariate.html","id":"escolha-do-número-de-clusters","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.4 Escolha do número de clusters","text":"Um dos problemas com o agrupamento  hierárquico é que ele não nos informa quantos clusters existem ou onde cortar o dendrograma  para formar os clusters. Você pode cortar árvore hierárquica uma determinada altura, digamos, na média das distâncias, entanto esta decisão é puramente impirica. Por exemplo, se o ponto de corte muito alto, tendemos agrupar genótipos que podem, de fato, não ser semelhantes. Um ponto de corte muito baixo, por outro lado, pode resultar em fracassos na seleção, pois consideramos que os genótipos são distintos, podendo não o serem. Procedimentos estatisticos à exemplo de Milligan Cooper (1985), Scott Symons (1971), Krzanowski Lai (1988), Halkidi, Batistakis, Vazirgiannis (2001) e Hubert Arabie (1985) são recomendados para esta escolha. Estes algorítimos estão implementados pacote NbClust (Charrad et al. 2014) pode ser utilizado para este fim. Por padrão, função fornece o ponto de corte calculado pelo método de Mojena (1977).Procedimentos baseado em reamostragens bootstrap que calcula probabilidade de erro de cada galho dendrograma  podem ser utilizados. O código abaixo estima p-valores para cada junção dendrograma modelo d3. Para maiore detalhes sobre o métoddo veja Suzuki Shimodaira (2006).O resultado procedimento bootstrap indicou formação de dois clusters, com probabilidade de erro de 5%. Assim, nas próximas funções, serão demostrados os diferentes dendrogramas  que podem ser gerados","code":"\npv_clust <- pvclust(t(d3$data), nboot = 100, method.dist = \"euclidean\")\n# Bootstrap (r = 0.5)... Done.\n# Bootstrap (r = 0.67)... Done.\n# Bootstrap (r = 0.83)... Done.\n# Bootstrap (r = 1.0)... Done.\n# Bootstrap (r = 1.17)... Done.\n# Bootstrap (r = 1.33)... Done.\nplot(pv_clust, hang = -1, cex = 0.5)\npvrect(pv_clust, alpha = 0.95)"},{"path":"multivariate.html","id":"dendrogramas-personalizados","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.5 Dendrogramas personalizados","text":"O pacote factoextra oferece um conjunto de funções para estender objetos dendrogramas em R. Um exemplo simples é dado abaixo.","code":"\nde1 <- fviz_dend(d3$hc,\n                 k = 2,\n                 xlab = \"Genótipos\",\n                 ylab =  \"Distância euclidiana\",\n                 main = \"\") \nde2 <- fviz_dend(d3$hc,\n                 k = 2,\n                 type = \"circular\")\nde3 <- fviz_dend(d3$hc,\n                 k = 2,\n                 type = \"phylogenic\",\n                 repel = TRUE)\nde4 <- fviz_dend(d1$hc,\n                 k = 5,\n                 xlab = \"Genótipos\",\n                 ylab =  \"Distância euclidiana\",\n                 palette = \"jco\",\n                 show_labels = FALSE,\n                 main = \"\")\ncp1 <- plot_grid(de1, de2, de3, nrow = 1,\n                 labels = c(\"de1\", \"de2\", \"de3\"))\nplot_grid(cp1, de4, rows = 2, labels = c(\"\", \"de4\"))"},{"path":"multivariate.html","id":"distancias-para-cada-ambiente","chapter":"Capítulo 13 Análise multivariada","heading":"13.2.6 Distancias para cada ambiente","text":"O seguinte código computa distâncias para cada nível fator AMB conjunto de dados maize. função group_by()  é utilizada para criar uma lista onde cada elemento conterá os dados de cada ambiente. Note que o argumento keep_factors = TRUE foi utilizado para manter colunas de fatores. Assim é possível computar média para cada genótipo quando informado o argumento means_by = GEN na função clustering() . O resultado desta função é então passado para função pairs_mantel().  Esta função é utilizada para avaliar associação entre quatro matrizes de distância.","code":"\nd4 <- \n  maize %>%\n  group_by(ENV) %>%\n  clustering(NKR, TKW, NKE, nclust = 4)\npairs_mantel(d4, names = c(\"A1\", \"A2\", \"A3\", \"A4\"),\n             maxsize = 5, minsize = 3)"},{"path":"multivariate.html","id":"componentes-principais","chapter":"Capítulo 13 Análise multivariada","heading":"13.3 Componentes principais","text":"","code":""},{"path":"multivariate.html","id":"conceito","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.1 Conceito","text":"O objetivo básico da análise de componentes principais (Principal Component Analysis, PCA) é descrever variação em um conjunto de variáveis correlacionadas \\(x^T = (x_1, ..., x_q)\\), em termos de um novo conjunto de variáveis não correlacionadas, \\(y^T = (y_1, ..., y_q)\\), onde cada variável é uma combinação linear das variáveis x. novas variáveis são ordenadas em ordem decrescente de “importância”, sentido de que \\(y_1\\) é responsável pelo máximo possível da variação dos dados originais entre todas combinações lineares de x. Então \\(y_2\\) é escolhido para explicar o máximo possível da variação restante, sujeito ser não correlacionado com \\(y_1\\), e assim por diante. novas variáveis definidas por este processo, \\(y_1, ..., y_q\\), são os componentes principais. principal vantagem da análise de componentes principais é que os primeiros componentes serão responsáveis por uma proporção substancial da variação nas variáveis originais, e podem, conseqüentemente, serem usados para fornecer um resumo de dimensões inferiores dessas variáveis.","code":""},{"path":"multivariate.html","id":"pacotes-r","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.2 Pacotes R","text":"Diversas funções de diferentes pacotes estão disponíveis software R para realização da PCA, entanto, função prcomp() pacote stats (nativa R) será utilizada aqui. Para confecção de biplots e extração dos resultados, funções pacote factoextra serão utilizadas. O primeiro passo é realizar instalação e o carregamento pacote.","code":""},{"path":"multivariate.html","id":"formato-dos-dados","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.3 Formato dos dados","text":"Para realizar aplicação da técnica de componentes principais, os dados em data_ge2 serão utilizados. Precisamos, primeiramente, organizar os dados de modo que tenhamos uma matriz de dupla entrada, contendo os genótipos nas linhas e variáveis nas colunas.Em PCA, variáveis são frequentemente dimensionadas (isto é, padronizadas). Isto é particularmente recomendado quando variáveis são medidas em diferentes escalas (quilogramas, quilômetros, centímetros, …). Caso contrário, os resultados da PCA podem ser severamente afetados. Geralmente variáveis são escalonadas para média zero e variáncia unitária, de acordo com seguinte fórmula.\\[\r\nx_s = \\frac{x_i- mean(x)}{sd(x)}\r\n\\]onde \\(mean(x)\\) é média dos valores de x e \\(sd(x)\\) é o desvio padrão. função nativa R scale() pode ser utilizada para padronizar os dados. Isto pode ser realizado, entanto, utilizando o argumento scale. = TRUE na função prcomp().","code":"\n\ndata_pca <- \n  data_ge2 %>% \n  means_by(GEN) %>% \n  column_to_rownames(\"GEN\")"},{"path":"multivariate.html","id":"código-r","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.4 Código R","text":"Para ajudar na interpretação PCA realizado acima, usaremos funções pacote factoextra. Essas funções incluem:\r\nget_eigenvalue(res.pca): Extrai os autovalores/variâncias dos componentes principais\r\nfviz_eig(res.pca): Para visualizar os autovalores\r\nget_pca_ind(res.pca), get_pca_var(res.pca): Para extrair os resultados dos genótipos e variáveis, respectivamente.\r\nfviz_pca_ind(res.pca), fviz_pca_var(res.pca): Para visualizar os resultados dos genótipos e variáveis, respectivamente.\r\nfviz_pca_biplot(res.pca): Para confecção de biplots.","code":"\nres.pca <- prcomp(data_pca, scale. = TRUE)"},{"path":"multivariate.html","id":"autovalores","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.5 Autovalores","text":"Os autovalores medem quantidade de variância retida por cada componente principal. Autovalores são grandes para os primeiros PCs e pequenos para os PCs subseqüentes. Ou seja, os primeiros PCs correspondem às direções com quantidade máxima de variação conjunto de dados. Examinamos os autovalores para determinar o número de componentes principais serem considerados. Os autovalores e proporção de variâncias (isto é, informação) retidos pelos componentes principais (PCs) são extraídos usando função get_eigenvalue().soma de todos os autovalores resulta em uma variância total de 15 proporção de variação explicada por cada autovalor é dada na segunda coluna. Por exemplo, 7.623 dividido por 15 é igual 0.5082, ou, cerca de 50.82% da variação é explicada por este primeiro autovalor. porcentagem acumulada explicada é obtida adicionando proporções sucessivas de variação explicadas. Por exemplo, 50.82% mais 18.13% é igual 68.95% e assim por diante. Assim, cerca de 59,627% da variação é explicada pelos dois primeiros autovalores.Um autovalor maior que 1 indica que os componentes principais são responsáveis por mais variâncias que contabilizadas por uma das variáveis originais nos dados padronizados. Isso é comumente usado como um ponto de corte para o qual os PCs são mantidos (Kaiser 1961). entanto, isto vale apenas quando os dados são padronizados.Infelizmente, não há maneira objetiva bem aceita de decidir quantos componentes principais são suficientes. Em nossa análise, os três primeiros componentes principais explicam 83.62% da variação. Esta é uma percentagem aceitavelmente grande. Um método alternativo para determinar o número de componentes principais é olhar para um Scree Plot, que é o gráfico de autovalores ordenados maior para o menor. O número de componentes é determinado ponto, além qual os autovalores remanescentes são todos relativamente pequenos.\r\nFigure 13.1: Autovalores e variância acumulada na análise de componentes principais\r\n","code":"\neig.val <- get_eigenvalue(res.pca)\np1 <- fviz_eig(res.pca)\np2 <- fviz_eig(res.pca,\n               addlabels = TRUE,\n               geom = \"bar\",\n               barfill = \"orange\",\n               barcolor = \"black\",\n               xlab = \"Componentes Principais\",\n               ylab = \"Percentagem da variância explicada\",\n               main = \"\")\nplot_grid(p1, p2)"},{"path":"multivariate.html","id":"gráfico-das-variáveis","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.6 Gráfico das variáveis","text":"Um método simples para extrair os resultados, para variáveis, de uma saída da função prcomp()  é usar função get_pca_var() . Esta função fornece uma lista de matrizes contendo todos os seguintes resultados:coord: coordenadas de variáveis para criar um gráfico de dispersão.cos2: representa qualidade da representação para variáveis mapa de fatores. É equivalente ao quadrado das coordenadas.contrib: contém contribuições (em percentagem) das variáveis para os componentes principais.","code":""},{"path":"multivariate.html","id":"círculo-de-correlação","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.6.1 Círculo de correlação","text":"correlação entre uma variável e um componente principal (PC) é usada como coordenadas da variável PC. Um gráfico pode ser obtido utilizando função fviz_pca_var(), O gráfico acima mostra relações entre todas variáveis. Ele pode ser interpretado da seguinte forma:Variáveis positivamente correlacionadas são agrupadas.Variáveis negativamente correlacionadas são posicionadas em lados opostos da origem gráfico (quadrantes opostos).distância entre variáveis e origem mede qualidade das variáveis mapa de fatores. Variáveis que estão longe da origem (próximas ao círculo) estão bem representadas mapa de fatores.","code":"\np1 <- fviz_pca_var(res.pca)\np2 <- fviz_pca_var(res.pca,\n                   col.circle = \"red\",\n                   xlab = \"PCA1 (50,8%)\",\n                   ylab = \"PCA2 (18,1%)\",\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   title = \"Gráfico das variáveis\",\n                   repel = TRUE)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"representação-e-contribuição-das-variáveis","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.6.2 Representação e contribuição das variáveis","text":"qualidade e contribuição das variáveis pode ser exibida mapa de fatores utilizando o argumento col.var = \"cos2\" e col.var = \"contrib\", respectivamente. Note que é possível também colorir variáveis utilizando uma variável categórica. Para fins didáticos criaremos uma variável categórica com três neíveis, baseado algorítmo kmeans aplicado nas coordenadas das variáveis.","code":"\np1 <- fviz_pca_var(res.pca,\n                   col.var = \"cos2\",\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   repel = TRUE)\nres.km <- kmeans(get_pca_var(res.pca)$coord, centers = 3, nstart = 25)\ngrp <- as.factor(res.km$cluster)\np2 <- fviz_pca_var(res.pca,\n                   col.var = grp,\n                   palette = c(\"green\", \"blue\", \"red\"),\n                   geom = c(\"point\", \"arrow\", \"text\"),\n                   repel = TRUE) +\n      theme(legend.position = \"bottom\")\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"gráfico-de-indivíduos","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.7 Gráfico de indivíduos","text":"Os resultados, para indivíduos, podem ser extraídos usando função get_pca_ind(). Similarmente ao get_pca_var(), esta função fornece uma lista de matrizes contendo todos os resultados para os indivíduos (coordenadas, correlação entre indivíduos e eixos, quadrado das distâncias e contribuições). função fviz_pca_ind() é usada para produzir o gráfico de indivíduos.","code":"\np1 <- fviz_pca_ind(res.pca)\np2 <- fviz_pca_ind(res.pca,\n                   pointsize = \"cos2\", \n                   pointshape = 21,\n                   fill = \"#E7B800\",\n                   repel = TRUE)\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"multivariate.html","id":"gráfico-biplot","chapter":"Capítulo 13 Análise multivariada","heading":"13.3.8 Gráfico biplot","text":"Um gráfico biplot combina os marcadores de variáveis e indivíduos em um mesmo gráfico. função fviz_pca_biplot() é utilizada para gerar este tipo de gráfico. O primeiro biplot (p1) é confeccionado com o exemplo em res.pca. Um exemplo um pouco mais complexo será mostrado utilizando o banco de dados R iris. Neste exemplo, os indivíduos são coloridos por grupos (cor discreta) e variáveis por suas contribuições para os componentes principais (cores de gradiente). Além disso, alteraremos transparência das variáveis por suas contribuições usando o argumento alpha.var. ","code":"\np1 <- fviz_pca_biplot(res.pca)\n\niris.pca <- prcomp(iris[,-5], scale. = TRUE)\np2 <- fviz_pca_biplot(iris.pca, \n                # indivíduos\n                geom.ind = \"point\",\n                fill.ind = iris$Species, col.ind = \"black\",\n                pointshape = 21, pointsize = 2,\n                palette = \"jco\",\n                addEllipses = TRUE,\n                repel = TRUE,\n                # variáveis\n                alpha.var =\"contrib\", col.var = \"contrib\",\n                gradient.cols = \"RdYlBu\",\n                legend.title = list(fill = \"Especies\",\n                                    color = \"Contribuição\",\n                                    alpha = \"Contribuição\"))\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"), rel_widths = c(0.4, 0.6))"},{"path":"multivariate.html","id":"k-means","chapter":"Capítulo 13 Análise multivariada","heading":"13.4 K-means","text":"","code":""},{"path":"multivariate.html","id":"conceito-1","chapter":"Capítulo 13 Análise multivariada","heading":"13.4.1 Conceito","text":"idéia básica por trás agrupamento  k-means consiste em definir clusters para que variação total dentro cluster seja minimizada. Existem vários algoritmos k-means disponíveis. O algoritmo padrão e o mais utilizado é o algoritmo de Hartigan-Wong (Hartigan Wong 1979), que define variação total dentro cluster como soma das distâncias Euclidianas quadráticas entre os itens e o centróide correspondente. Os passos básicos para análise são os que seguem:Especifique o número de clusters (k) serem criados;Selecione aleatoriamente k objetos conjunto de dados como os centros de clusters iniciais ou médias;Atribua cada observação ao seu centróide mais próximo, com base na distância euclidiana entre o objeto e o centróide;Para cada um dos k clusters, atualize o centróide cluster calculando os novos valores médios de todos os pontos de dados cluster. O centróide k-ésimo cluster é um vetor de comprimento p (p = número de variáveis) contendo médias de todas variáveis para observações k-ésimo cluster;Iterativamente, minimize soma de quadrados total dentro cluster; isto é, iterar etapas 3 e 4 até que atribuições cluster parem de mudar ou até que o número máximo de iterações sejam atingidas. Por padrão, o software R usa 10 como o valor padrão para o número máximo de iterações.","code":""},{"path":"multivariate.html","id":"pacotes-r-1","chapter":"Capítulo 13 Análise multivariada","heading":"13.4.2 Pacotes R","text":"Para implementar o método k-means, função kmeans() pacote stats (nativa R) será utilizada. Para confecção gráfico, função fviz_cluster()  pacote factoextra será utilizada.","code":""},{"path":"multivariate.html","id":"formato-dos-dados-e-códigos-r","chapter":"Capítulo 13 Análise multivariada","heading":"13.4.3 Formato dos dados e códigos R","text":"O formato dos dados utilizados é o mesmo dos utilizados para realizar análise de componentes principais. Assim, neste exemplo, somente criaremos um novo conjunto renomeando os dados organizados anteriormente (data_pca) para data_km. O próximo passo é ajustar o modelo k-means e armazenar em um objeto, em nosso exemplo, kmres. Posteriormente, função fviz_cluster() é utilizada para confeccionar o gráfico.","code":"\ndata_km <- data_pca\nkm.res <- kmeans(data_km, 2, nstart = 25)\np1 <- fviz_cluster(km.res, data = data_pca)\np2 <- fviz_cluster(km.res,\n                   data = data_pca,\n                   ellipse = TRUE,\n                   main = \"\",\n                   ellipse.type = \"confidence\",\n                   ggtheme = theme_metan())+\n      theme(legend.position = \"bottom\")\nplot_grid(p1, p2, labels = c(\"p1\", \"p2\"))"},{"path":"interaction.html","id":"interaction","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"Capítulo 14 Interação genótipo-vs-ambiente","text":"Uma cultura pode ser vista como um sistema complexo com resultados (por exemplo, rendimento de grãos) que são afetados por informações genéticas, fisiológicas, pedoclimáticas e de manejo. Melhoristas e geneticistas se esforçam continuamente para aumentar produtividade das culturas visando suprir demanda mundial cada vez maior por alimentos. É na fase final de um programa de melhoramento de plantas que muito esforço e recursos precisam ser investidos na avaliação dos genótipos (g) serem selecionados. Geralmente algumas centenas de genótipos precisam ser avaliados em um grande número de ambientes (e). Estes ensaios são conhecidos como ensaios multi-ambientes () e os dados destes experimentos resultam em uma matriz M de dimensões \\(g \\times e\\) . É nesta fase processo que surge um dos maiores desafios da análise de : compreender interação genótipo-vs-ambiente buscando novas formas de explorá-la e utilizá-la favor da seleção de genótipos com estabilidade  produtiva satisfatória.Funções pacote metan, acrônimo para multi environment trial analysis serão utilizadas para análise de dados de ensaios multi-ambientes.  O foi desenvolvido em linguagem R e é distribuído sob licença GPL (General Public Licence) 3.0. Isto significa que qualquer pessoa pode: () utilizar o código sem nenhuma restrição/pagamento; (ii) estudar o código e adaptá-lo às suas necessidades; (iii) sugerir modificações/melhorias código de modo aperfeiçoá-lo para uma comunidade maior de usuários, mantendo, porém, os direitos autor. O pacote metan fornece funções úteis para analisar dados de ensaios multi-ambientes usando métodos paramétricos e não paramétricos, incluindo, mas não limitados :Análise gráfica da interação genótipo-vs-ambiente;Análise de variância individualProcedimentos de validação cruzada para modelos da família AMMI e BLUP;Estimativas usando AMMI com diferentes números de termos multiplicativos;Índices de estabilidade baseados em AMMI;Biplots baseados modelo GGE;Predição baseada em modelos de efeito misto;Índices de estabilidade baseados em BLUP;Componentes de variância e parâmetros genéticos em modelos de efeito misto;Ferramentas gráficas para confecção de biplots.Estatísticas de estabilidade paramétrica e não paramétrica.Nesta seção, usaremos o conjunto de dados data_ge disponível pacote metan. Para mais informações, por favor, consulte ?data_ge. Outros conjuntos de dados podem ser usados desde que seguintes colunas estejam conjunto de dados: ambiente, genótipo, bloco e variável(eis) resposta.","code":""},{"path":"interaction.html","id":"análise-gráfica-da-interação","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.1 Análise gráfica da interação","text":"função ge_plot() pode ser usada para visualizar o desempenho genótipo através dos ambientes. O losango preto mostra média para cada ambiente.Para identificar o genótipo vencedor em cada ambiente, podemos usar função ge_winners().Ou obtenha classificação dos genótipos em cada ambiente.Para mais detalhes sobre os testes, podemos usar ge_details()","code":"\na <- ge_plot (data_ge, ENV, GEN, GY)\nb <- ge_plot (data_ge, ENV, GEN, GY) + ggplot2::coord_flip ()\narrange_ggplot(a, b)\nge_winners(data_ge2, ENV, GEN, resp = everything())\n# # A tibble: 4 x 16\n#   ENV   PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR   CDED \n#   <fct> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n# 1 A1    H3    H1    H1    H6    H6    H8    H6    H6    H6    H2    H4    H8   \n# 2 A2    H2    H1    H1    H6    H2    H2    H6    H2    H2    H2    H6    H13  \n# 3 A3    H13   H13   H6    H4    H13   H6    H2    H7    H13   H13   H4    H6   \n# 4 A4    H5    H5    H10   H7    H11   H5    H7    H5    H7    H11   H9    H10  \n# # ... with 3 more variables: PERK <chr>, TKW <chr>, NKE <chr>\nge_winners(data_ge2, ENV, GEN, resp = everything (), type = \"ranks\")\n# # A tibble: 52 x 16\n#    ENV   PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR   CDED \n#    <fct> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#  1 A1    H3    H1    H1    H6    H6    H8    H6    H6    H6    H2    H4    H8   \n#  2 A1    H9    H4    H10   H11   H13   H9    H11   H8    H13   H13   H5    H9   \n#  3 A1    H4    H9    H4    H10   H10   H6    H9    H9    H9    H3    H6    H7   \n#  4 A1    H5    H10   H7    H4    H9    H10   H10   H7    H2    H7    H11   H12  \n#  5 A1    H2    H7    H12   H5    H8    H13   H5    H5    H1    H12   H3    H11  \n#  6 A1    H10   H5    H11   H9    H2    H7    H4    H13   H4    H8    H2    H10  \n#  7 A1    H7    H3    H6    H3    H3    H12   H8    H4    H3    H6    H1    H6   \n#  8 A1    H13   H11   H9    H7    H1    H11   H3    H12   H8    H10   H13   H13  \n#  9 A1    H6    H13   H13   H1    H7    H5    H7    H10   H5    H4    H8    H5   \n# 10 A1    H11   H6    H5    H12   H4    H1    H1    H3    H10   H1    H12   H1   \n# # ... with 42 more rows, and 3 more variables: PERK <chr>, TKW <chr>, NKE <chr>\nge_details(data_ge2, ENV, GEN, resp = everything())\n# # A tibble: 10 x 16\n#    Parameters PH    EH    EP    EL    ED    CL    CD    CW    KW    NR    NKR  \n#    <chr>      <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n#  1 Mean       \"2.4~ \"1.3~ \"0.5~ \"15.~ \"49.~ \"29.~ \"15.~ \"24.~ \"172~ \"16.~ \"32.~\n#  2 SE         \"0.0~ \"0.0~ \"0\"   \"0.1\" \"0.2~ \"0.1~ \"0.0~ \"0.5\" \"2.6~ \"0.1~ \"0.2~\n#  3 SD         \"0.3~ \"0.2~ \"0.0~ \"1.2~ \"2.7~ \"2.3\" \"1.1~ \"6.2~ \"32.~ \"1.6~ \"3.4~\n#  4 CV         \"13.~ \"21.~ \"10.~ \"8.2~ \"5.5~ \"7.9~ \"7.3~ \"25.~ \"18.~ \"10.~ \"10.~\n#  5 Min        \"1.7~ \"0.7~ \"0.3~ \"11.~ \"43.~ \"23.~ \"12.~ \"11.~ \"105~ \"12.~ \"23.~\n#  6 Max        \"3.0~ \"1.8~ \"0.6~ \"17.~ \"54.~ \"34.~ \"18.~ \"38.~ \"250~ \"21.~ \"42 ~\n#  7 MinENV     \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~ \"A3 ~\n#  8 MaxENV     \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~ \"A1 ~\n#  9 MinGEN     \"H10~ \"H8 ~ \"H8 ~ \"H12~ \"H9 ~ \"H12~ \"H12~ \"H11~ \"H9 ~ \"H9 ~ \"H12~\n# 10 MaxGEN     \"H1 ~ \"H1 ~ \"H1 ~ \"H6 ~ \"H6 ~ \"H6 ~ \"H5 ~ \"H5 ~ \"H6 ~ \"H13~ \"H4 ~\n# # ... with 4 more variables: CDED <chr>, PERK <chr>, TKW <chr>, NKE <chr>"},{"path":"interaction.html","id":"análise-de-variância-individual","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.2 Análise de variância individual","text":"função anova_ind()  pode ser utilziada para realizar uma análise de variância para cada ambiente, conforme o seguinte código.","code":"\nind <- anova_ind(data_ge, ENV, GEN, REP, GY)\nprint(ind$GY$individual)\n# # A tibble: 14 x 12\n#    ENV    MEAN   MSG   FCG     PFG    MSB    FCB     PFB    MSE    CV    h2\n#    <chr> <dbl> <dbl> <dbl>   <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <dbl> <dbl>\n#  1 E1     2.52 0.337  2.34 5.94e-2 0.0652  0.453 6.43e-1 0.144  15.1  0.573\n#  2 E10    2.18 0.296 11.1  1.10e-5 0.654  24.5   7.28e-6 0.0267  7.51 0.910\n#  3 E11    1.37 0.151  1.44 2.44e-1 0.377   3.59  4.86e-2 0.105  23.7  0.304\n#  4 E12    1.61 0.320  5.98 6.47e-4 0.0919  1.72  2.08e-1 0.0535 14.4  0.833\n#  5 E13    2.91 0.713  7.18 2.10e-4 0.0767  0.772 4.77e-1 0.0994 10.8  0.861\n#  6 E14    1.78 0.131  1.73 1.53e-1 0.104   1.37  2.78e-1 0.0753 15.4  0.423\n#  7 E2     3.18 0.207  1.16 3.76e-1 0.698   3.91  3.88e-2 0.179  13.3  0.136\n#  8 E3     4.06 0.335  1.87 1.23e-1 0.489   2.73  9.21e-2 0.179  10.4  0.466\n#  9 E4     3.68 0.531  3.86 7.12e-3 0.116   0.846 4.46e-1 0.138  10.1  0.741\n# 10 E5     3.91 0.526  7.93 1.10e-4 0.219   3.30  6.02e-2 0.0664  6.59 0.874\n# 11 E6     2.66 0.135  2.30 6.35e-2 0.160   2.73  9.22e-2 0.0586  9.09 0.565\n# 12 E7     1.99 0.337  3.70 8.73e-3 0.381   4.19  3.22e-2 0.0910 15.2  0.730\n# 13 E8     2.54 0.215  7.72 1.31e-4 0.817  29.4   2.15e-6 0.0278  6.57 0.870\n# 14 E9     3.06 0.679  6.12 5.62e-4 0.583   5.25  1.60e-2 0.111  10.9  0.837\n# # ... with 1 more variable: AS <dbl>"},{"path":"interaction.html","id":"baseada-em-regressão","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.3 Baseada em regressão","text":"Eberhart Russell (1966) popularizaram análise de estabilidade baseada em regressão. Nesse procedimento, análise de adaptabilidade e estabilidade é realizada por meio de ajustes de equações de regressão onde variável dependente é estimada em função de um índice ambiental, conforme o seguinte modelo:\\[\r\n\\mathop Y\\nolimits_{ij}  = {\\beta _{0i}} + {\\beta _{1i}}{I_j} + {\\delta _{ij}} + {\\bar \\varepsilon _{ij}}\r\n\\]onde \\({\\beta _{0i}}\\) é média geral genótipo (= 1, 2, …, ); \\({\\beta _{1i}}\\) é respota linear genótipo ao índice ambiental; Ij é o índice ambiental (j = 1, 2, …, e), onde \\({I_j} = [(y_{.j}/g)- (y_{..}/ge)]\\), \\({\\delta _{ij}}\\) é o desvio da regressão, e \\({\\bar \\varepsilon _{ij}}\\) é o erro experimental.\r\nO modelo é ajustado com função ge_reg() . Os métodos S3 plot() e summary() podem ser utilizados para explorar os resultados.","code":"\nreg_model <- ge_reg(data_ge, ENV, GEN, REP, GY)\nreg_model$GY$anova\n# # A tibble: 17 x 6\n#    SV                       Df `Sum Sq` `Mean Sq` `F value`  `Pr(>F)`\n#    <chr>                 <dbl>    <dbl>     <dbl>     <dbl>     <dbl>\n#  1 \"Total\"                 139  324.       2.33      NA     NA       \n#  2 \"GEN\"                     9   13.0      1.44       6.28   3.05e- 7\n#  3 \"ENV + (GEN x ENV)\"     130  311.       2.39      NA     NA       \n#  4 \"ENV (linear)\"            1  280.     280.        NA     NA       \n#  5 \" GEN x ENV (linear)\"     9    3.61     0.402      1.75   8.58e- 2\n#  6 \"Pooled deviation\"      120   27.6      0.230     NA     NA       \n#  7 \"G1\"                     12    1.11     0.0924     1.06   3.92e- 1\n#  8 \"G10\"                    12    7.54     0.629      7.22   1.66e-11\n#  9 \"G2\"                     12    2.95     0.246      2.82   1.14e- 3\n# 10 \"G3\"                     12    0.699    0.0582     0.669  7.81e- 1\n# 11 \"G4\"                     12    2.23     0.186      2.14   1.48e- 2\n# 12 \"G5\"                     12    1.49     0.124      1.42   1.55e- 1\n# 13 \"G6\"                     12    1.27     0.106      1.22   2.71e- 1\n# 14 \"G7\"                     12    3.25     0.270      3.11   3.72e- 4\n# 15 \"G8\"                     12    2.54     0.211      2.43   5.15e- 3\n# 16 \"G9\"                     12    4.54     0.378      4.34   2.42e- 6\n# 17 \"Pooled error\"          280   24.4      0.0870    NA     NA\nreg_model$GY$regression\n# # A tibble: 10 x 6\n#    GEN       Y slope deviations  RMSE    R2\n#    <chr> <dbl> <dbl>      <dbl> <dbl> <dbl>\n#  1 G1     2.60 1.06    -0.00142 0.162 0.966\n#  2 G10    2.47 1.12     0.177   0.424 0.823\n#  3 G2     2.74 1.05     0.0497  0.265 0.913\n#  4 G3     2.96 1.03    -0.0128  0.129 0.977\n#  5 G4     2.64 0.937    0.0298  0.231 0.917\n#  6 G5     2.54 0.887    0.00902 0.188 0.937\n#  7 G6     2.53 0.861    0.00304 0.174 0.942\n#  8 G7     2.74 0.819    0.0579  0.278 0.852\n#  9 G8     3.00 1.03     0.0382  0.246 0.922\n# 10 G9     2.51 1.19     0.0938  0.329 0.897\nplot(reg_model)"},{"path":"interaction.html","id":"índice-de-confiança-genotípico","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.4 Índice de confiança genotípico","text":"Annicchiarico (1992) propôs um método de estabilidade em que o parâmetro de estabilidade é medido pela superioridade genótipo em relação à média de cada ambiente, de acordo com o seguinte modelo:\\[\r\n{Z_ {ij}} = \\frac{{{Y_ {}}}} {{{{\\bar Y} _ {. J}}}} \\times 100\r\n\\]O índice de confiança genotípico genótipo (\\(W_i\\)) é então estimado da seguinte forma:\\[\r\nW_i = Z_{.} / E - \\alpha \\times sd (Z_{.})\r\n\\]Onde \\(\\alpha\\) é o quantil da distribuição normal padrão uma dada probabilidade de erro (\\(\\alpha \\approx 1.64\\) 0.05). O método é implementado usando função Annicchiarico() . O índice de confiança é estimado considerando todos os ambientes, os ambientes favoráveis (índice positivo) e os ambientes desfavoráveis (índice negativo), como segue:","code":"\nann <- Annicchiarico(data_ge, ENV, GEN, REP, GY)\nann$GY$general\n# # A tibble: 10 x 6\n#    GEN       Y Mean_rp Sd_rp    Wi  rank\n#    <chr> <dbl>   <dbl> <dbl> <dbl> <dbl>\n#  1 G1     2.60    96.5  7.38  91.5     6\n#  2 G10    2.47    90.3 18.9   77.5    10\n#  3 G2     2.74   103.  12.1   94.5     4\n#  4 G3     2.96   111.   4.59 108.      1\n#  5 G4     2.64    99.1  8.03  93.7     5\n#  6 G5     2.54    95.5  7.74  90.2     8\n#  7 G6     2.53    95.5  7.61  90.4     7\n#  8 G7     2.74   105.  12.5   96.0     3\n#  9 G8     3.00   113.   8.87 107.      2\n# 10 G9     2.51    91.6 13.8   82.3     9"},{"path":"interaction.html","id":"índice-de-superioridade-genotípico","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.5 Índice de superioridade genotípico","text":"função superiority() implementa o método não-paramétrico proposto por Lin Binns (1988), que considera que medida de superioridade geral da cultivar para dados de cultivar x localização é definida como quadrado médio da distância entre resposta da cultivar e média de resposta máxima em todas localidades, de acordo com o seguinte modelo.\\[\r\nP_i = \\sum \\limits_{j = 1} ^ n {(y_ {ij} - y _ {. J}) ^ 2 / (2n)}\r\n\\]\r\nonde n é o número de ambientes. Da mesma forma que o índice de confiança genotípico, o índice de superioridade é calculado por todos os ambientes, para os favoráveis e para os desfavoráveis.","code":"\nsuper <- superiority(data_ge, ENV, GEN, GY)\nsuper$GY$index\n# # A tibble: 10 x 8\n#    GEN       Y   Pi_a   R_a   Pi_f   R_f    Pi_u   R_u\n#    <chr> <dbl>  <dbl> <dbl>  <dbl> <dbl>   <dbl> <dbl>\n#  1 G1     2.60 0.169      5 0.228      4 0.125       6\n#  2 G10    2.47 0.344     10 0.475     10 0.245      10\n#  3 G2     2.74 0.126      3 0.149      3 0.108       5\n#  4 G3     2.96 0.0410     1 0.0723     1 0.0175      2\n#  5 G4     2.64 0.173      6 0.289      5 0.0853      4\n#  6 G5     2.54 0.240      8 0.382      8 0.133       7\n#  7 G6     2.53 0.238      7 0.377      7 0.134       8\n#  8 G7     2.74 0.149      4 0.318      6 0.0214      3\n#  9 G8     3.00 0.0412     2 0.0882     2 0.00588     1\n# 10 G9     2.51 0.291      9 0.390      9 0.217       9"},{"path":"interaction.html","id":"estratificação-ambiental","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.6 Estratificação ambiental","text":"Um método que combina análise de estabilidade e estratificação ambiental usando análise fatorial foi proposto por Murakami Cruz (2004). Este método é implementado com função ge_factanal(), como segue:maneira mais fácil de calcular os índices de estabilidade acima mencionados é usando função ge_stats(). Se você deseja exportar um resumo dos resultados, maneira mais simples é usando função summary().Este comando criará um arquivo de texto chamado ge_stats summary.txt  diretório de trabalho atual.","code":"\nfato <- ge_factanal(data_ge, ENV, GEN, REP, GY)\nplot(fato)\nprint(fato$GY$PCA)\n# # A tibble: 14 x 4\n#    PCA   Eigenvalues  Variance Cumul_var\n#    <chr>       <dbl>     <dbl>     <dbl>\n#  1 PC1      5.60e+ 0  4.00e+ 1      40.0\n#  2 PC2      2.51e+ 0  1.79e+ 1      58.0\n#  3 PC3      2.41e+ 0  1.72e+ 1      75.1\n#  4 PC4      1.37e+ 0  9.80e+ 0      84.9\n#  5 PC5      1.13e+ 0  8.05e+ 0      93.0\n#  6 PC6      4.87e- 1  3.48e+ 0      96.5\n#  7 PC7      3.03e- 1  2.16e+ 0      98.6\n#  8 PC8      1.29e- 1  9.25e- 1      99.6\n#  9 PC9      6.24e- 2  4.46e- 1     100  \n# 10 PC10     1.72e-16  1.23e-15     100  \n# 11 PC11     1.41e-16  1.01e-15     100  \n# 12 PC12    -1.74e-16 -1.25e-15     100  \n# 13 PC13    -2.67e-16 -1.91e-15     100  \n# 14 PC14    -3.01e-16 -2.15e-15     100\nprint(fato$GY$FA)\n# # A tibble: 14 x 8\n#    Env        FA1     FA2      FA3     FA4     FA5 Communality Uniquenesses\n#    <chr>    <dbl>   <dbl>    <dbl>   <dbl>   <dbl>       <dbl>        <dbl>\n#  1 E1    -0.881    0.327   0.00927 -0.0631  0.274        0.963      0.0369 \n#  2 E10   -0.942   -0.158  -0.0820   0.113   0.174        0.962      0.0380 \n#  3 E11   -0.929   -0.233  -0.0336  -0.242   0.110        0.989      0.0111 \n#  4 E12   -0.848    0.135   0.0263   0.0941  0.241        0.805      0.195  \n#  5 E13   -0.940    0.108  -0.0842  -0.0637 -0.235        0.961      0.0391 \n#  6 E14   -0.150   -0.123  -0.916   -0.0872  0.265        0.954      0.0463 \n#  7 E2    -0.198   -0.0521 -0.126   -0.969   0.0328       0.997      0.00266\n#  8 E3    -0.0806   0.910   0.341   -0.0173 -0.110        0.963      0.0370 \n#  9 E4     0.209    0.543  -0.272   -0.728  -0.120        0.957      0.0433 \n# 10 E5    -0.777    0.392  -0.269   -0.0470 -0.267        0.904      0.0963 \n# 11 E6    -0.524    0.569  -0.309   -0.174  -0.238        0.781      0.219  \n# 12 E7    -0.244    0.342  -0.520    0.297   0.619        0.918      0.0820 \n# 13 E8     0.00161 -0.0589 -0.914   -0.226  -0.143        0.911      0.0891 \n# 14 E9    -0.0794  -0.291  -0.0183  -0.0539  0.927        0.954      0.0463\nprint(fato$GY$env_strat)\n# # A tibble: 14 x 6\n#    Env   Factor  Mean   Min   Max    CV\n#    <chr> <chr>  <dbl> <dbl> <dbl> <dbl>\n#  1 E1    FA1     2.52 1.97   2.90 13.3 \n#  2 E10   FA1     2.18 1.54   2.57 14.4 \n#  3 E11   FA1     1.37 0.899  1.68 16.4 \n#  4 E12   FA1     1.61 1.02   2    20.3 \n#  5 E13   FA1     2.91 1.83   3.52 16.8 \n#  6 E5    FA1     3.91 3.37   4.81 10.7 \n#  7 E3    FA2     4.06 3.43   4.57  8.22\n#  8 E6    FA2     2.66 2.34   2.98  7.95\n#  9 E14   FA3     1.78 1.43   2.06 11.7 \n# 10 E8    FA3     2.54 2.05   2.88 10.5 \n# 11 E2    FA4     3.18 2.61   3.61  8.25\n# 12 E4    FA4     3.68 3.02   4.27 11.5 \n# 13 E7    FA5     1.99 1.39   2.55 16.8 \n# 14 E9    FA5     3.06 1.94   3.72 15.6\nstat_ge <- ge_stats(data_ge, ENV, GEN, REP, GY)\nsummary(stat_ge, export = TRUE)"},{"path":"interaction.html","id":"o-modelo-ammi","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7 O modelo AMMI","text":"O modelo linear mais simples com efeito de interação usado na análise de EMA é\\[\r\n{y_{ijk}} = {\\rm{ }}\\mu {\\rm{ }} + \\mathop \\alpha \\nolimits_i  + \\mathop \\tau \\nolimits_j  + \\mathop {(\\alpha \\tau )}\\nolimits_{ij}  + \\mathop \\gamma \\nolimits_{jk}  + {\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\r\n\\]onde \\({y_{ijk}}\\) é variável resposta observada k-ésimo bloco -ésimo genótipo j-ésimo ambiente (= 1, 2, …, g; j = 1, 2, …, e; k = 1, 2, …, b); \\(\\mu\\) é média geral; \\(\\mathop\\alpha\\nolimits_i\\) é o efeito principal genótipo ; \\(\\mathop \\tau \\nolimits_j\\) é o principal efeito ambiente j; \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) é o efeito de interação genótipo com o ambiente j; \\(\\mathop \\gamma \\nolimits_{jk}\\) é o efeito bloco k ambiente j; e \\({\\rm{ }}\\mathop \\varepsilon \\nolimits_{ijk}\\) é o erro aleatório assumindo \\(..d \\sim N(0, \\sigma^2 )\\).Métodos que combinam diferentes princípios estatísticos ganharam espaço na análise de  por volta da década 1960, com destaque especial ao estudo de Gollob (1968), que propôs um método que combina os benefícios da análise de fatores e análise de variância em um único método para estudar estabilidade . Naquela época este método era conhecido como FANOVA. Atualmente este mesmo método foi popularizado por Gauch (1988) com o acrônimo AMMI.análise AMMI utiliza análise aditiva de variância aos fatores principais (genótipo e ambiente) e decomposição por valores singulares ao residual modelo aditivo, isto é, o efeito da interação genótipo-vs-ambiente somado ao erro experimental. Esta matriz dos efeitos não aditivos, então, pode ser aproximadamente exibida por meio de biplots  Gabriel (1971). Este método tem ganhado destaque nas últimas décadas, principalmente devido rápida evolução computacional, o que tornou possível complexas decomposições de matrizes de alta ordem.De posse de uma matriz de dupla entrada oriunda de ensaios multiambientes, estimativa da variável resposta -ésimo genótipo j-ésimo ambiente é obtida utilizando AMMI de acordo com o seguinte modelo:\\[\r\n{y_{ij}} = \\mu  + {\\alpha_i} + {\\tau_j} + \\sum\\limits_{k = 1}^k {{\\lambda _k}{a_{ik}}} {t_{jk}} + {\\rho _{ij}} + {\\varepsilon _{ij}}\r\n\\]onde \\({\\lambda_k}\\) é o valor singular para o k-ésimo eixo componente principal; \\(a_{ik}\\) é o -ésimo elemento k-ésimo autovetor de genótipos; \\(t_{jk}\\) é o j-ésimo elemento k-ésimo autovetor de ambientes. Um resíduo \\(\\rho _{ij}\\) permanece, se todos os k-PCAs não são considerados, onde k = \\(min(G-1; E-1)\\).","code":""},{"path":"interaction.html","id":"ajuste-do-modelo","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7.1 Ajuste do modelo","text":"O modelo AMMI é ajustado com função performs_ammi(). O primeiro argumento é os dados, nosso exemplo data_ge. Os próximos argumentos (ENV, GEN e REP) são os nomes das colunas que contém os níveis dos fatores ambiente, genótipo, repetição, respectivamente. argumento resp são declaradas variáveis resposta. Uma única variável pode ser analizada (como em nosso exemplo) ou, um vetor de variáveis, usando, por exemplo resp = c(GY, HM).Note que os argumentos inseridos na função obedecem ordem dos argumentos requiridos na função [veja args(waas)]. Se obedecida esta ordem de avaliação, não é necessário declarar qual argumento está sendo inserido. Por exemplo, se mudássemos ordem de entrada, teríamos um código semelhante waas(data_ge, gen = GEN, env =  ENV, REP, resp = c(PH, ED, TKW, NKR)).","code":"\n\nAMMI_model <- performs_ammi(data_ge, ENV, GEN, REP, GY)\n# variable GY \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df  Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV  13 279.574 21.5057   62.33 0.00e+00          .           .\n#   REP(ENV)  28   9.662  0.3451    3.57 3.59e-08          .           .\n#        GEN   9  12.995  1.4439   14.93 2.19e-19          .           .\n#    GEN:ENV 117  31.220  0.2668    2.76 1.01e-11          .           .\n#        PC1  21  10.749  0.5119    5.29 0.00e+00       34.4        34.4\n#        PC2  19   9.924  0.5223    5.40 0.00e+00       31.8        66.2\n#        PC3  17   4.039  0.2376    2.46 1.40e-03       12.9        79.2\n#        PC4  15   3.074  0.2049    2.12 9.60e-03        9.8          89\n#        PC5  13   1.446  0.1113    1.15 3.18e-01        4.6        93.6\n#        PC6  11   0.932  0.0848    0.88 5.61e-01          3        96.6\n#        PC7   9   0.567  0.0630    0.65 7.53e-01        1.8        98.4\n#        PC8   7   0.362  0.0518    0.54 8.04e-01        1.2        99.6\n#        PC9   5   0.126  0.0252    0.26 9.34e-01        0.4         100\n#  Residuals 252  24.367  0.0967      NA       NA          .           .\n#      Total 536 389.036  0.7258      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!"},{"path":"interaction.html","id":"analise-residual","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7.2 Analise residual","text":"O pacote metan conta com uma opção para análise residual modelo AMMI ajustado. Gráficos podem ser obtidos utilizando o seguinte comando.figura acima, obtida com função autoplot(), mostra 4 gráficos. Os dois primeiros são os mais importantes. O primeiro (Residual vs fitted) pode ser utilizado para identificar homogeneidade das variâncias. Uma distribuição aleatória dos pontos gráfico deve ser observada. Quando um padrão de distibuição é observado –como, por exemplo, distribuição dos pontos em forma de funil– uma investigação deve ser realizada, pois este padrão indica possiblidade de heterogeneidade das variâncias. O segundo gráfico (Normal Q-Q) nos informa quanto normalidade dos resíduos, ou seja, é desejado que os pontos sejam distribuídos ao redor da linha diagonal.","code":"\nplot(AMMI_model)"},{"path":"interaction.html","id":"escolha-do-número-de-termos-multiplicativos","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7.3 Escolha do número de termos multiplicativos","text":"Conforme já discutido, análise AMMI aplica técnica de decomposição por valores singulares na matriz dos efeitos não aditivos modelo (). Logo, esta matriz pode ser aproximada pela pelo seguinte modelo: \\(= U \\lambda V^T\\), onde onde U é uma matriz g \\(\\times\\) e contendo os vetores singulares de \\(AA^T\\) e formam base ortonormal para os efeitos de genótipos; \\(V^T\\) é uma matriz e \\(\\times\\) e que contém os vetores singulares de \\(\\mathbf{^TA}\\) e formam base ortonormal para os efeitos de ambientes; e \\(\\lambda\\) é uma matriz diagonal e \\(\\times\\) e contendo k-valores singulares de \\(^TA\\) , onde k = \\(min(G-1; E-1)\\). Assim, diferentes modelos (dependendo número de termos multiplicativos utilizados) podem ser utilziados para predizer o rendimento genótipo ambiente j. tabela abaixo mostra os possíveis modelos. modelo AMMI0 apenas os efeitos aditivos são considerados. modelo AMMI1, o primeiro termo multiplicativo é considerado, e assim por diante, até o modelo AMMIF, onde \\(min(G-1;E-1)\\) termos são considerados.escolha número de termos multiplicativos ser utilizado é baseada em basicamente dois critérios de sucesso de análise: Postdiscritive sucess e Predictive sucess. Por definição, Predictive sucess significa literalmente afirmação prévia que acontecerá em algum momento futuro. Neste contexto, testes de validação cruzada (cross-validation) podem ser utilizadas para avaliar o sucesso preditivo dos membros de modelos da familia AMMI (Tiago Olivoto, Lúcio, et al. 2019a). Por outro lado, Postdiscritive sucess significa fazer uma afirmação ou dedução sobre algo que aconteceu passado. Na escolha número de termos multiplicativos da análise AMMI este sucesso pode ser calculado utilizando testes como o proposto por Gollob (1968).","code":""},{"path":"interaction.html","id":"postdiscritive-sucess","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7.3.1 Postdiscritive sucess","text":"objeto anova  gerado pela função waas()  testes de hipóteses são realizados e probabilidades de erro são atribuídas para cada modelo considerando distribuição de graus de liberdade proposto por Gollob (1968). Assim é possível identificar qual é o número ideal de termos ser considerado na predição. Em nosso exemplo, dois termos foram significativos 5% de probabilidade de erro.","code":""},{"path":"interaction.html","id":"predictive-sucess","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.7.3.2 Predictive sucess","text":"O pacote metan fornece uma solução completa para validaçao cruzada modelo AMMI. Utilizando função cv_ammif() , por exemplo, é possível realizar um teste de cross-validation para família de modelos AMMI (AMMI0-AMMIF) usando dados com repetições. Automaticamente, primeira validação é realizada considerando AMMIF (todos possíveis IPCAs são usados). Considerando esse modelo, o conjunto de dados original é dividido em dois conjuntos de dados: dados de modelagem e dados de validação. O conjunto de dados “modelagem” possui todas combinações (genótipo vs ambiente) com R-1 repetições. O conjunto de dados “validação” tem uma repetição. O diagrama abaixo representa o procedimento realizado.divisão conjunto de dados em dados de modelagem e validação depende design informado. Considerando um delineamento de blocos completos casualizados (DBC), blocos completos são aleatoriamente selecionados dentro de ambientes, como mostrado por Tiago Olivoto, Lúcio, et al. (2019a). O bloco restante serve dados de validação. Se design = \"CRD\" informado, assim declarando que um delineamento intericamente casualizado (DIC)  foi usado, observações são aleatoriamente selecionadas para cada tratamento (combinação genótipo-vs-ambiente). Este é o mesmo procedimento sugerido por Gauch (1988). Os valores estimados para o membro da família AMMI em estudo são então comparados com os dados de “validação” e um erro de predição \\(\\hat{z}_{ij}\\) é estimado para cada tratamento. raiz quadrada quadrado médio da diferença de predição (RMSPD) é calculado. Este procedimento é repetido n vezes, utilizando o argumento nboot = n. Ao final procedimento, o algorítimo armazena n estimativas RMSPD para o modelo em questão, e um novo modelo é então testado seguindo os mesmos passos.Como exemplo, função cv_ammi() é usada para calcular um procedimento de validação cruzada para os modelos AMMI0, AMMI2 e AMMIF (9 eixos).AMMI0 para AMMIF\r\nfunção cv_ammif() é usada para calcular um procedimento de validação cruzada para todos os membros modelo da família AMMI. Nesse caso, AMMI0-AMMI9.Validação cruzada para previsão de BLUP\r\nfunção cv_blup () fornece uma validação cruzada de dados baseados em replicação usando modelos mistos. Por padrão, blocos completos são selecionados aleatoriamente para cada ambiente. Usando o argumento `random ’, é possível escolher os efeitos aleatórios modelo, como mostrado abaixo.Validação cruzada para previsão de BLUP\r\nfunção cv_blup () fornece uma validação cruzada de dados baseados em replicação usando modelos mistos. Por padrão, blocos completos são selecionados aleatoriamente para cada ambiente. Usando o argumento `random ’, é possível escolher os efeitos aleatórios modelo, como mostrado abaixo.Genótipo e genótipo versus ambiente como efeitos aleatóriosGenótipo e genótipo versus ambiente como efeitos aleatóriosAmbiente, replicação dentro ambiente e interação como efeitos aleatóriosUm modelo aleatório (todos os termos como efeitos aleatórios)","code":"\nAMMI0 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 0) # AMMI0\nAMMI2 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 2) # AMMI2\nAMMI9 <- cv_ammi(data_ge, ENV, GEN, REP, GY, naxis = 9) # AMMIF\nAMMIF <- cv_ammif(data_ge, ENV, GEN, REP, GY)\nBLUP_g <- cv_blup(dados_ge, ENV, GEN, REP, GY, random = \"gen\")\nBLUP_e <- cv_blup(dados_ge, ENV, GEN, REP, GY, random = \"env\")\nBLUP_ge <- cv_blup(dados_ge, ENV, GEN, REP, GY, random = \"all\")"},{"path":"interaction.html","id":"imprimindo-os-meios-das-estimativas-rmspd","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.8 Imprimindo os meios das estimativas RMSPD","text":"tabela acima mostra estatísticas descritivas (média, desvio padrão, erro padrão da média e quantis 2,5% e 97,5%) das 100 estimativas RMSPD para cada modelo, e são apresentadas partir modelo mais preciso (menor média RMSPD) para o modelo menos preciso (maior média RMSPD).","code":"\nbind_mod <- bind_cv(AMMIF, BLUP_g, bind = \"means\")\nprint(bind_mod$RMSPD)\n# # A tibble: 11 x 6\n#    MODEL        mean     sd      se  Q2.5 Q97.5\n#    <fct>       <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#  1 BLUP_g_RCBD 0.405 0.0249 0.00176 0.357 0.453\n#  2 AMMI2       0.411 0.0242 0.00171 0.369 0.460\n#  3 AMMI4       0.416 0.0234 0.00166 0.373 0.462\n#  4 AMMI3       0.416 0.0215 0.00152 0.377 0.457\n#  5 AMMI5       0.422 0.0232 0.00164 0.377 0.464\n#  6 AMMI6       0.422 0.0219 0.00155 0.381 0.462\n#  7 AMMI7       0.425 0.0199 0.00141 0.381 0.458\n#  8 AMMI8       0.427 0.0204 0.00145 0.387 0.462\n#  9 AMMIF       0.429 0.0213 0.00150 0.390 0.469\n# 10 AMMI1       0.430 0.0251 0.00178 0.384 0.479\n# 11 AMMI0       0.430 0.0283 0.00200 0.370 0.485"},{"path":"interaction.html","id":"plotagem-dos-valores-rmspd","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9 Plotagem dos valores RMSPD","text":"Os valores das estimativas RMSPD obtidas processo de validação cruzada podem ser plotados usando função plot ().Seis estatísticas são mostradas neste boxplot. média (losango preto), mediana (linha preta), dobradiças inferior e superior que correspondem ao primeiro e terceiro quartis (percentis 25 e 75, respectivamente). O bigode superior se estende da dobradiça até o maior valor não mais que \\(1,5 \\times {IQR}\\) partir da dobradiça (em que IQR é o intervalo entre quartis). O bigode mais baixo se estende da dobradiça ao menor valor, máximo \\(1,5 \\times {IQR}\\) da dobradiça. Dados além final dos bigodes são considerados pontos extremos. Se condição violin = TRUE, uma plotagem de violino é adicionada junto com o boxplot. Um gráfico de violino é uma exibição compacta de uma distribuição contínua exibida da mesma maneira que um gráfico de caixa.","code":"library(metan)\r\nbind1 <- bind_cv(AMMI0, AMMI2, AMMI9)\r\nbind2 <- bind_cv (AMMIF, BLUP_g, BLUP_e, BLUP_ge)\r\na <- plot(bind1, violino = TRUE)\r\nb <- plot(bind2,\r\n          width.boxplot = 0.6,\r\n          order_box = TRUE,\r\n          plot_theme = theme_metan_minimal ())\r\narrange_ggplot(a, b, tag_levels = list(letters[1:2]))"},{"path":"interaction.html","id":"valores-estimados-pelo-modelo-ammi","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.1 Valores estimados pelo modelo AMMI","text":"Em nosso exemplo, o modelo AMMI2 foi o que apresentou o menor RMSPD, sendo então o mais indicado para estimar variável GY. estimativa considerando dois termos multiplicativos pode realizada utilizando função predict(), tendo como argumentos o modelo AMMI ajustado (AMMI_model) e o número de termos multiplicativos considerados na estimação (naxis). seguintes variáveis são retornadas: ENV é o ambiente; GEN é o genótipo; Y é o valor observado; resOLS é o residual (\\(\\hat{z}_{ij}\\)) estimado pelos Mínimos Quadrados Ordinários, onde \\(\\hat{z}_{ij} = y_{ij} - \\bar{y}_{.} - \\bar{y}_{.j} + \\bar{y}_{..}\\); Ypred é o valor estimado pelos mínimos quadrados ordinários (\\(\\hat{y}_{ij} = y_{ij} -\\hat{z}_{ij}\\)); ResAMMI é o residual estimado pelo modelo AMMI (\\(\\hat{}_{ij}\\)) considerando o número de termos multiplicativos informado na função (neste caso 2), onde \\(\\hat{}_{ij} = \\lambda_1a_{i1}t_{j1}\\); YpredAMMI é o valor estimado pelo modelo AMMI \\(\\hat{ya}_{ij} = \\bar{y}_{.} + \\bar{y}_{.j} - \\bar{y}_{..}+\\hat{}_{ij}\\); e AMMI0 é o valor estimado quando nenhum termo multiplicativo é usado, ou seja, \\(\\hat{y}_{ij} = \\bar{y}_{.} + \\bar{y}_{.j} - \\bar{y}_{..}\\).","code":"\npredicted <- predict(AMMI_model, naxis = 2)\npredicted$GY\n# # A tibble: 140 x 8\n#    ENV   GEN       Y RESIDUAL Ypred ResAMMI[,1] YpredAMMI[,1] AMMI0\n#    <fct> <fct> <dbl>    <dbl> <dbl>       <dbl>         <dbl> <dbl>\n#  1 E1    G1     2.37  -0.0843  2.45     0.0693           2.52  2.45\n#  2 E1    G10    1.97  -0.344   2.32    -0.360            1.96  2.32\n#  3 E1    G2     2.90   0.311   2.59     0.0735           2.66  2.59\n#  4 E1    G3     2.89   0.0868  2.80    -0.00963          2.79  2.80\n#  5 E1    G4     2.59   0.100   2.49     0.0144           2.50  2.49\n#  6 E1    G5     2.19  -0.196   2.38    -0.0317           2.35  2.38\n#  7 E1    G6     2.30  -0.0797  2.38     0.0238           2.40  2.38\n#  8 E1    G7     2.77   0.186   2.59     0.186            2.77  2.59\n#  9 E1    G8     2.90   0.0493  2.85     0.0852           2.94  2.85\n# 10 E1    G9     2.33  -0.0307  2.36    -0.0515           2.31  2.36\n# # ... with 130 more rows"},{"path":"interaction.html","id":"índices-de-estabilidade-baseados-em-ammi","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.2 Índices de estabilidade baseados em AMMI","text":"(Tiago Olivoto, Lúcio, et al. 2019a) demonstraram que média ponderada dos escores absolutos (WAAS, Weighted Average Absolute Scores), pode ser utilizada como um índice quantitativo de estabilidade na análise AMMI. Utilizando função get_model_data() é possível obter facilmente este índice para diversas variáveis com poucas linhas de código. Veja o exemplo abaixo, com quatro variáveis. Este índice também é computado em uma estrutura de modelo misto. Veja o exemplo aqui.Além índice WAAS mostrado acima, os seguintes índices de estabilidade baseados em AMMI podem ser calculados usando função AMMI_indexes():AMMI stability value, ASV, (Purchase, Hatting, Deventer 2000).\\[\r\nASV = \\sqrt {{{\\left[ {\\frac{{IPCA{1_{ss}}}}{{IPCA{2_{ss}}}} \\times \\left( {IPCA{1_{score}}} \\right)} \\right]}^2} + {{\\left( {IPCA{2_{score}}} \\right)}^2}}\r\n\\]Soma dos valores absolutos dos escores IPCA, SIPC\\[\r\nSIP{C_i} = \\sum\\nolimits_{k = 1}^P {\\left| {\\mathop {\\lambda }\\nolimits_k^{0.5} {a_{ik}}} \\right|}\r\n\\]Média dos autovetores elevados ao quadrado, EV\\[\r\nE{V_i} = \\sum\\nolimits_{k = 1}^P {\\mathop \\nolimits_{ik}^2 } /P\r\n\\]descritos por Sneller, Kilgore-Norquest, Dombek (1997), onde P é o número de IPCA retido por meio de testes Fvalor absoluto da contribuição relativa dos IPCAs para interação (Zali et al. 2012).\\[\r\nZ{a_i} = \\sum\\nolimits_{k = 1}^P {{\\theta _k}{a_{ik}}}\r\n\\]Onde \\({\\theta _k}\\) é o percentual da soma de quadrados explicada pelo k -ésimo IPCA. índices de selecção simultâneas (SSI) são calculados pela soma dos ranques dos índices Za ASV, SIPC e EV e o ranque da variável dependente (Farshadfar 2008) que resulta em ssiASV, ssiSIPC, ssiEV, e ssiZa, respectivamente.função AMMI_index () tem dois argumentos. O primeiro (x) é o modelo, que deve ser um objeto da classe waas. O segundo, (order.y) é ordem para variável resposta. Por padrão, ele é definido como nulo, o que significa que variável resposta é ordenada em ordem decrescente. Se x é uma lista com mais de uma variável, então order.y deve ser um vetor com o mesmo comprimento de x. Cada elemento vetor deve ser um dos “h” ou “l”. Se “h” usado, variável resposta será ordenada em ordem decrescente. Se “l” usado, variável resposta será ordenada em ordem crescente da média dos genótipos. Usando o operador %>%  é possível estruturar uma sequência lógica de operações. Vamos construir esse modelo.","code":"\nwaas(data_ge2, ENV, GEN, REP,\n     resp = c(PH, ED, TKW, NKR)) %>%\n get_model_data(what = \"WAAS\")\n# variable PH \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  7.719  2.5728 127.913 4.25e-07          .           .\n#   REP(ENV)   8  0.161  0.0201   0.897 5.22e-01          .           .\n#        GEN  12  1.865  0.1554   6.929 6.89e-09          .           .\n#    GEN:ENV  36  5.397  0.1499   6.686 5.01e-14          .           .\n#        PC1  14  4.466  0.3190  14.230 0.00e+00       82.8        82.8\n#        PC2  12  0.653  0.0545   2.430 8.40e-03       12.1        94.9\n#        PC3  10  0.277  0.0277   1.240 2.76e-01        5.1         100\n#  Residuals  96  2.153  0.0224      NA       NA          .           .\n#      Total 191 22.692  0.1188      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable ED \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  306.0  101.99  43.386 2.70e-05          .           .\n#   REP(ENV)   8   18.8    2.35   0.906 5.15e-01          .           .\n#        GEN  12  212.9   17.74   6.838 8.95e-09          .           .\n#    GEN:ENV  36  398.2   11.06   4.263 7.60e-09          .           .\n#        PC1  14  212.2   15.16   5.840 0.00e+00       53.3        53.3\n#        PC2  12  134.7   11.23   4.330 0.00e+00       33.8        87.1\n#        PC3  10   51.3    5.13   1.980 4.38e-02       12.9         100\n#  Residuals  96  249.1    2.59      NA       NA          .           .\n#      Total 191 1583.2    8.29      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable TKW \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  37013   12338   11.13 3.16e-03          .           .\n#   REP(ENV)   8   8869    1109    1.21 3.03e-01          .           .\n#        GEN  12  44633    3719    4.05 4.41e-05          .           .\n#    GEN:ENV  36 164572    4571    4.98 1.73e-10          .           .\n#        PC1  14 104276    7448    8.11 0.00e+00       63.4        63.4\n#        PC2  12  33361    2780    3.03 1.20e-03       20.3        83.6\n#        PC3  10  26935    2694    2.93 3.00e-03       16.4         100\n#  Residuals  96  88171     918      NA       NA          .           .\n#      Total 191 507829    2659      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable NKR \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  237.0   79.01  15.843 0.000997          .           .\n#   REP(ENV)   8   39.9    4.99   0.635 0.746348          .           .\n#        GEN  12  227.8   18.99   2.418 0.008726          .           .\n#    GEN:ENV  36  602.7   16.74   2.132 0.001839          .           .\n#        PC1  14  337.4   24.10   3.070 0.000600         56          56\n#        PC2  12  192.2   16.02   2.040 0.028500       31.9        87.9\n#        PC3  10   73.1    7.31   0.930 0.509500       12.1         100\n#  Residuals  96  753.7    7.85      NA       NA          .           .\n#      Total 191 2463.8   12.90      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!\n# Class of the model: waas\n# Variable extracted: WAAS\n# # A tibble: 13 x 5\n#    GEN      PH    ED   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl>\n#  1 H1    0.318 0.667 2.72  0.929\n#  2 H10   0.230 0.973 2.15  0.506\n#  3 H11   0.201 0.649 1.26  0.836\n#  4 H12   0.364 0.315 0.558 0.228\n#  5 H13   0.363 0.838 0.514 0.946\n#  6 H2    0.342 1.08  4.41  0.404\n#  7 H3    0.374 0.486 4.10  0.252\n#  8 H4    0.294 0.378 3.07  0.281\n#  9 H5    0.168 0.567 0.738 0.611\n# 10 H6    0.270 0.409 1.64  1.60 \n# 11 H7    0.228 0.384 3.44  0.518\n# 12 H8    0.315 0.653 4.91  0.941\n# 13 H9    0.146 0.907 5.50  0.888\nstab_indexes <- \n  data_ge %>%\n  waas(ENV, GEN, REP, GY, verbose = FALSE) %>%\n  AMMI_indexes()\nprint(stab_indexes)\n# Variable GY \n# ---------------------------------------------------------------------------\n# AMMI-based stability indexes\n# ---------------------------------------------------------------------------\n# # A tibble: 10 x 7\n#    GEN       Y   ASV  SIPC     EV     ZA  WAAS\n#    <chr> <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>\n#  1 G1     2.60 0.346 0.463 0.0149 0.100  0.151\n#  2 G10    2.47 1.23  2.07  0.210  0.437  0.652\n#  3 G2     2.74 0.249 1.54  0.179  0.216  0.283\n#  4 G3     2.96 0.113 0.552 0.0207 0.0797 0.106\n#  5 G4     2.64 0.594 1.04  0.0521 0.219  0.326\n#  6 G5     2.54 0.430 0.997 0.0433 0.186  0.270\n#  7 G6     2.53 0.265 1.14  0.0911 0.172  0.233\n#  8 G7     2.74 0.663 1.79  0.191  0.303  0.428\n#  9 G8     3.00 0.574 1.18  0.0669 0.225  0.327\n# 10 G9     2.51 0.983 1.50  0.131  0.336  0.507"},{"path":"interaction.html","id":"biplots","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.3 Biplots","text":"O pacote metan conta com gráficos gerados pelo pacote ggplot2, o que lhe confere um alto nível de personalização. função utilizada para obtenção dos diferentes tipos de biplots será plot_scores(). Para maiores detalhes veja ?plot_._scores.","code":""},{"path":"interaction.html","id":"biplot-tipo-2-gy-x-pc1","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.3.1 Biplot tipo 2: GY x PC1","text":"O biplot conhecido como AMMI1 é confeccionado utilizando o argumento type = 2 na função plot_scores(). O biplot AMMI1 é utilizado para identificar tanto estabilidade quando produtividade dos genótipos. Neste tipo de biplot, os genótipos com escores PC1 próximos de zero e à direita da linha vertical, são considerados os mais estáveis e com rendimento superior média geral.\r\nFigure 14.1: Biplot AMMI1 gerado pelo pacote metan\r\n","code":"\np3 <-plot_scores(AMMI_model)\np4 <-plot_scores(AMMI_model,\n                 col.segm.env = \"transparent\") +\n                 theme_gray() +\n                 theme(legend.position = c(0.1, 0.9),\n                       legend.background = element_rect(fill = NA))\n\narrange_ggplot(p3, p4, tag_levels = list(c(\"p3\",\"p4\")))"},{"path":"interaction.html","id":"biplot-tipo-1-ipca1-x-ipca2","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.3.2 biplot tipo 1: IPCA1 x IPCA2","text":"O biplot conhecido como AMMI2 é confeccionado utilizando o argumento type = 1 (padrão) na função plot_scores(). O biplot AMMI2 representa os dois primeiros IPCAs oriundos da decomposição por valor singular da matriz dos efeitos da interação e é utilizado para realizar inferências quanto aos padrões da interação genótipo vs ambiente. Neste caso, os dois primeiros IPCAs explicam 66.2% da da soma de quadrados da interação.\r\nFigure 14.2: Biplot AMMI2, gerado pelo pacote metan\r\n","code":"\n\np1 <-plot_scores(AMMI_model, type = 2)\np2 <-plot_scores(AMMI_model,\n                 type = 2,\n                 polygon = TRUE,\n                 col.gen = \"black\",\n                 col.env = \"gray70\",\n                 col.segm.env = \"gray70\",\n                 axis.expand = 1.5)\narrange_ggplot(p1, p2, tag_levels = list(c(\"p1\",\"p2\")))"},{"path":"interaction.html","id":"rendimento-nominal-x-ipca1","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.9.3.3 Rendimento nominal x IPCA1","text":"Com o objetivo de identificar possíveis mega-ambientes, bem como visualizar o padrão -won-conjunto de dados, um gráfico com o rendimento nominal (\\(\\mathop {\\hat y} \\nolimits_ {ij}^*\\) ) em função dos escores PCA1 dos ambientes é também confecionado pela função plot_scores() utilizando o argumento type = 4. Neste gráfico, cada genótipo é representado por uma linha reta com equação \\(\\hat y_{ij}^* = \\mu_i + PCA1_i \\times PCA1_j\\) , onde \\(\\hat y_{ij}^*\\) é o rendimento nominal para o genótipo ambiente J; \\(\\mu\\) é média geral genótipo ; \\(PC{1_i}\\) é o escore PCA1 genótipo e \\(PC{1_j}\\) é o escore PCA1 ambiente j. O genótipo vencedor em um determinado ambiente possui o maior rendimento nominal nesse ambiente.\r\nFigure 14.3: Gráfico tipo ‘-won-’ baseado modelo AMMI\r\n","code":"\nplot_scores(AMMI_model,\n            type = 4,\n            size.tex.pa = 2,\n            x.lab = \"Rendimento nominal (Mg/ha)\",\n            y.lab = \"Escore dos ambientes no PCA1\")"},{"path":"interaction.html","id":"o-modelo-gge","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10 O modelo GGE","text":"O modelo GGE (Genotype plus Genotype-vs-Environment interaction) tem sido amplamente utilizado para avaliação de genótipos e identificação de mega-ambientes em ensaios multi-ambientais (MET). Este modelo considera um biplot que é construído pelos dois primeiros componentes principais (PC1 e PC2) derivados da decomposição por valores singulares de dados oriundos de um MET centrados ambiente (Yan et al. 2007).Comunmente, o rendimento médio genótipo ambiente j é descrito pelo seguinte modelo linear geral, ignorando quaisquer erros aleatórios\\[\r\n\\hat y_{ij} + \\mu + \\alpha_i + \\beta_j + \\phi_{ij}\r\n\\]onde \\(\\hat y_{ij}\\) é o rendimento médio genótipo ambiente j, \\(= 1, ... g; j = 1, ... e\\) sendo g e e o número de genótipos e ambientes, respectivamente; \\(\\mu\\) é média geral; \\(\\alpha_i\\) é o efeito principal genótipo ; \\(\\beta_j\\) é o efeito principal ambiente j e \\(\\phi_{ij}\\) é o efeito de interação entre genótipo e o ambiente j. Quando \\(\\phi_{ij}\\) é submetido Decomposição por Valor Singular (SVD), temos o bem conhecido modelo AMMI, visto anteriormente. modelo GGE, o termo \\(\\alpha_i\\) é deletado modelo acima, permitindo que variação explicada por este termo seja absorvida por \\(\\phi_{ij}\\). Em seguida, esta matriz de dados –agora centrada ambiente– é submetida SVD (Yan et al. 2007; Yan Kang 2003). Explicitamente, temos\\[\r\n{\\phi_{ij} =  \\hat y_{ij}} - \\mu - \\beta_j  = \\sum\\limits_{k = 1}^p \\xi_{ik}^*\\eta_{jk}^*\r\n\\]onde \\(\\xi_{ik}^* = \\lambda_k^ \\alpha \\xi_{ik}\\); \\(\\eta_{jk}^* = \\lambda_k^{1-\\alpha}\\eta_{jk}\\) sendo \\(\\lambda_k\\) o k-ésimo autovalor da SVD (\\(k = 1, ... p\\)), com \\(p \\le min (e, g)\\); \\(\\alpha\\) é o fator de partição valor singular para o Componente Principal (PC) k (Yan 2002); \\(\\xi_{ik}^*\\) e \\(\\eta_{jk}^*\\) são os escores PC k para genótipo e ambiente j, respectivamente.função gge() pacote metan é usada para ajustar o modelo GGE. De acordo com Yan Kang (2003), função suporta quatro métodos de centralização de dados, dois métodos de escalonamento de dados e três opções para particionamento de valor singular:","code":""},{"path":"interaction.html","id":"data-centering","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.1 Data centering","text":"0 ou none: para dados não centralizados;1 ou global: para dados centralizados globalmente (E + G + GE);2 ou environment: (padrão), para dados centrados ambiente (G + GE);3 ou double: para dados centrados duplamente (GE). Um biplot não pode ser produzido sem modelos produzidos sem centralização.","code":""},{"path":"interaction.html","id":"data-scaling","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.2 Data scaling","text":"0 ou none: (padrão) para nenhum escalonamento;1 ou sd: Cada valor é dividido pelo desvio padrão seu ambiente correspondente (coluna). Isso colocará todos os ambientes com aproximadamente o mesmo intervalo de valores.","code":""},{"path":"interaction.html","id":"singular-value-partition","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.3 Singular value partition","text":"1 ou genotype: O valor singular é inteiramente particionado nos autovetores de Genótipo (\\(\\alpha = 1\\)), Também chamado de row metric preserving;2 ou environment: (padrão) O valor singular é inteiramente particionado nos autovetores de ambiente (\\(\\alpha = 0\\)), também chamado de column metric preserving;3 ou symmetrical: O valor singular é simetricamente dividida nos autovetores de genótipo e ambiente (\\(\\alpha = 0,5\\)). Esta partição é mais frequentemente usada na análise AMMI.","code":""},{"path":"interaction.html","id":"ajustando-o-modelo-gge","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.4 Ajustando o modelo GGE","text":"Para ajustar o modelo GGE, usaremos os dados em data_ge, que contém dados rendimento de grãos avaliados em 10 genótipos conduzidos em 14 ambientes. Primeiro de tudo, vamos criar uma tabela bidirecional para esses dados usando função make_mat(). função gge() ajusta um modelo GGE baseado em uma tabela bidirecional (ge_table em nosso caso) com genótipos nas linhas e ambientes em colunas, ou em um data.frame contendo pelo menos colunas para genótipos, ambientes e variável() resposta.\r\nO modelo acima foi ajustado considerando () column metric preserving (onde o valor singular é inteiramente particionado nos autovetores ambiente); (ii) environment centered (o biplot conterá uma informação mista de G + GEI); e nenhum método de escalonamento. Para alterar estas configurações padrão, use os argumentos svp, centering e scaling, respectivamente. Por favor, note que segundo exemplo o argumento table foi definido como TRUE para indicar que os dados de entrada são uma tabela bidirecional.","code":"\nge_table = make_mat(data_ge, GEN, ENV, GY)\nprint.data.frame(ge_table, digits = 3)\n#       E1  E10   E11  E12  E13  E14   E2   E3   E4   E5   E6   E7   E8   E9\n# G1  2.37 2.31 1.356 1.34 3.00 1.53 3.04 4.08 3.49 4.17 2.81 1.90 2.27 2.78\n# G10 1.97 1.54 0.899 1.02 1.83 1.86 3.15 4.11 4.27 3.37 2.48 2.24 2.70 3.15\n# G2  2.90 2.30 1.491 1.99 3.03 1.43 3.23 4.57 3.72 3.83 2.54 1.99 2.05 3.36\n# G3  2.89 2.34 1.568 1.76 3.47 2.06 3.61 4.13 4.13 4.13 2.98 2.16 2.85 3.29\n# G4  2.59 2.17 1.370 1.53 2.64 1.86 3.19 3.85 3.30 3.78 2.70 1.98 2.30 3.72\n# G5  2.19 2.14 1.326 1.69 2.57 1.78 3.14 3.74 3.38 3.47 2.43 1.66 2.71 3.30\n# G6  2.30 2.21 1.501 1.39 2.91 1.80 3.29 3.43 3.40 3.57 2.34 1.76 2.54 3.04\n# G7  2.77 2.44 1.364 1.95 3.18 1.94 2.61 4.10 3.02 4.05 2.67 2.55 2.58 3.14\n# G8  2.90 2.57 1.683 2.00 3.52 1.99 3.44 4.11 4.14 4.81 2.91 2.26 2.88 2.83\n# G9  2.33 1.74 1.125 1.41 2.95 1.57 3.09 4.51 3.90 3.93 2.77 1.39 2.49 1.94\n# Usando um data frame\ngge_model <- gge(data_ge, ENV, GEN, GY)"},{"path":"interaction.html","id":"valores-estimados-pelo-modelo-gge","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.5 Valores estimados pelo modelo GGE","text":"","code":"\npredicted <- predict(gge_model, naxis = 2)\nprint(predicted$GY, digits = 3)\n# # A tibble: 10 x 14\n#       E1   E10   E11   E12   E13   E14    E2    E3    E4    E5    E6    E7    E8\n#  * <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#  1  2.51  2.14 1.35   1.59  2.95  1.76  3.20  4.14  3.78  3.97  2.70  1.95  2.56\n#  2  1.93  1.62 0.988  1.07  2.07  1.66  3.08  3.99  3.78  3.28  2.43  1.69  2.48\n#  3  2.67  2.33 1.47   1.75  3.09  1.82  3.19  4.04  3.59  4.03  2.70  2.08  2.54\n#  4  2.83  2.44 1.56   1.88  3.40  1.83  3.26  4.18  3.73  4.32  2.83  2.10  2.59\n#  5  2.50  2.25 1.39   1.64  2.71  1.83  3.09  3.81  3.32  3.65  2.52  2.11  2.47\n#  6  2.32  2.06 1.27   1.46  2.52  1.78  3.09  3.87  3.46  3.54  2.49  1.99  2.47\n#  7  2.40  2.11 1.31   1.52  2.65  1.78  3.12  3.92  3.52  3.66  2.54  2.00  2.49\n#  8  2.78  2.49 1.56   1.88  3.17  1.87  3.17  3.93  3.40  4.04  2.68  2.21  2.52\n#  9  2.99  2.54 1.64   2.00  3.71  1.84  3.33  4.33  3.89  4.61  2.96  2.11  2.64\n# 10  2.27  1.78 1.15   1.30  2.82  1.64  3.28  4.43  4.28  4.03  2.79  1.65  2.62\n# # ... with 1 more variable: E9 <dbl>"},{"path":"interaction.html","id":"visualizando-o-biplot","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6 Visualizando o Biplot","text":"função genérica plot() é usada para gerar um biplot usando como entrada o modelo ajustado da classe gge. O tipo de biplot é escolhido pelo argumento type na função. Dez tipos de biplots estão disponíveis de acordo com Yan Kang (2003).type = 1 Um biplot básico.type = 2 Desempenho médio vs. estabilidadetype = 3 Que ganhou onde.type = 4 Descriminação vs. representatividadetype = 5 Examinar um ambiente.type = 6 Ranquear os ambientes.type = 7 Examinar um genótipo.type = 8 Ranquear os genótipos.type = 9 Comparar dois genótipos.type = 10 Relação entre os ambientes.Neste material, para cada tipo de biplot, dois gráficos são produzidos. Um com configurações padrão e outro para mostrar algumas opções gráficas da função.","code":""},{"path":"interaction.html","id":"biplot-tipo-1-um-biplot-básico","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.1 Biplot tipo 1: Um biplot básico","text":"Esta é configuração padrão gráfico da função, portanto, este biplot é produzido apenas chamando plot(model), como mostrado abaixo.\r\n","code":"\np1 <- plot(gge_model)\np2 <- plot(gge_model,\n           col.gen = \"blue\",\n           size.text.env = 2)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-2-desempenho-médio-vs.-estabilidade","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.2 Biplot tipo 2: Desempenho médio vs. estabilidade","text":"Neste biplot, visualização da média e da estabilidade dos genótipos é obtida desenhando uma coordenada média de ambiente (AEC) biplot obtido com row metric preserving. Primeiro, um ambiente médio, representado pelo pequeno círculo, é definido pelas médias dos escores PC1 e PC2 dos ambientes. linha que passa pela origem biplot e pelo AEC pode ser chamada de média. projeções de marcadores genotípicos nesse eixo deve, portanto, aproximar o rendimento médio dos genótipos. Assim, o G8 foi claramente o genótipo de maior rendimento, em média.ordenada de AEC é linha que passa pela origem biplot e é perpendicular à abscissa AEC. Portanto, se abscissa AEC representa o G, ordenada AEC deve aproximar o GEI associado cada genótipo, que é uma medida de variabilidade ou instabilidade dos genótipos (Yan et al. 2007). Uma projeção maior na ordenada AEC, independentemente da direção, significa maior instabilidade. Em nosso exemplo, o G3 foi o mais estável e o segundo genótipo mais produtivo, enquanto o G9 apresentou alta instabilidade.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <-plot(gge_model, type = 2)\np2 <-plot(gge_model,\n          type = 2,\n          col.gen = \"black\",\n          col.env = \"gray\",\n          axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-3-quem-ganhou-onde","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.3 Biplot tipo 3: quem ganhou onde","text":"Neste biplot (obtido com particionamento de valores singulares simétrico) um polígono é desenhado juntando os genótipos (G7, G8, G9, G10 e G4) que estão localizadas mais distante da origem biplot fazendo com que todos os outros genótipos fiquem contidos polígono. Os genótipos vértex têm os vetores mais longos, em suas respectivas direções, que é uma medida da capacidade de resposta aos ambientes. Estes genótipos estão, portanto, entre os genótipos mais responsivos. Todos os outros genótipos são menos responsivos em suas respectivas direções.linhas perpendiculares aos lados polígono dividem o biplot em setores. Cada setor tem um genótipo vértice. Por exemplo, o setor com o genótipo-vértice G4 pode ser referido como o setor G4. Um ambiente (E9), foi enquadrado neste setor. Como regra geral, o genótipo vértex é o genótipo de mais alto rendimento em todos os ambientes que compartilham o setor com ele (Yan et al. 2007). Neste caso, G4 teve o maior rendimento em E9, como mostrado na tabela acima.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <-plot(gge_model, type = 3)\np2 <-plot(gge_model,\n          type = 3,\n          size.shape.win = 5,\n          large_label = 6,\n          col.gen = \"black\",\n          col.env = \"gray\",\n          annotation = FALSE,\n          title = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-4-discriminação-vs.-representatividade","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.4 Biplot tipo 4: Discriminação vs. representatividade","text":"","code":"\np1 <-plot(gge_model, type = 4)\np2 <-plot(gge_model,\n          type = 4,\n          plot_theme = theme_gray())+\n  theme(legend.position = \"bottom\")\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-5-examinar-um-ambiente","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.5 Biplot tipo 5: Examinar um ambiente","text":"identificação de genótipos mais adaptados um ambiente pode ser facilmente alcançada utilizando o biplot GGE. Por exemplo, para visualizar o desempenho de diferentes genótipos em um dado ambiente, por exemplo, E10, simplesmente desenhe uma linha que passa pela origem biplot e o marcador E10. Os genótipos podem ser classificados de acordo com suas projeções eixo E10 com base em seu desempenho neste ambiente, na direção apontada pela seta. Em nosso exemplo, E10, o genótipo de maior rendimento foi o genótipo G8 e o genótipo de menor rendimento foi G10. ordem dos genótipos foi G8 > G7 > G3 > G2 > G4 > G1 > G6 > G5 > G9 > G10.\r\n","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <-plot(gge_model, type = 5, sel_env = \"E10\")\np2 <-plot(gge_model,\n          type = 5,\n          sel_env = \"E10\",\n          col.gen = \"black\",\n          col.env = \"black\",\n          size.text.env = 10,\n          axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-6-ranquear-os-ambientes","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.6 Biplot tipo 6: ranquear os ambientes","text":"Neste biplot o ambiente “ideal” é usado como o centro de um conjunto de linhas concêntricas que servem para medir distância entre um ambiente e o ambiente “ideal”. Como o foco principal neste biplot são os ambientes, partição de valor singular usada é “ambiente” (padrão). Pode ser visto que E13 é o mais próximo ambiente ideal e, portanto, é o mais desejável de todos os 14 ambientes. E4 e E9 foram os ambientes de teste menos desejados.\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY)\np1 <- plot(gge_model, type = 6)\np2 <- plot(gge_model,\n           type = 6,\n           col.gen = \"black\",\n           col.env = \"black\",\n           size.text.env = 10,\n           axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-7-examinar-um-genótipo","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.7 Biplot tipo 7: examinar um genótipo","text":"Semelhante à visualização dos desempenhos genotípicos em um determinado ambiente (biplot 5), visualização da média e da estabilidade dos genótipos é obtida desenhando-se uma AEC biplot com foco genótipo, ou row metric preserving (Yan et al. 2007).\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <- plot(gge_model, type = 7, sel_gen = \"G8\")\np2 <- plot(gge_model,\n           type = 7,\n           sel_gen = \"G8\",\n           col.gen = \"black\",\n           col.env = \"black\",\n           size.text.env = 10,\n           axis_expand = 1.5)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-8-ranquear-os-genótipos","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.8 Biplot tipo 8: ranquear os genótipos","text":"Este biplot compara todos os genótipos com o genótipo “ideal”. O genótipo ideal, representado pelo pequeno círculo com uma seta apontando para ele, é definido como tendo o maior rendimento em todos os ambientes. Ou seja, tem o maior rendimento médio e é absolutamente estável. Os genótipos são classificados com base em sua distância genótipo ideal (Yan et al. 2007). Em nosso exemplo, o G3 e o G8 superaram os outros genótipos.\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"genotype\")\np1 <- plot(gge_model, type = 8)\np2 <- plot(gge_model,\n           type = 8,\n           col.gen = \"black\",\n           col.env = \"gray\",\n           size.text.gen = 6)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-9-comparar-dois-genótipos","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.9 Biplot tipo 9: comparar dois genótipos","text":"Para comparar dois genótipos, por exemplo, G10 e G8, desenhe uma linha de conexão para conectá-los e trace uma linha perpendicular que passa pela origem biplot e é perpendicular à linha de conexão. Vemos que um ambiente, E9, está mesmo lado da linha perpendicular G10, e os outros 13 ambientes estão outro lado da linha perpendicular, junto com o G8. Isso indica que G10 produziu mais que o G8 E9, mas G8 produziu mais que o G10 nos outros 13 ambientes (Yan et al. 2007).\r\n","code":"\ngge_model <- gge(data_ge, ENV, GEN, GY, svp = \"symmetrical\")\np1 <- plot(gge_model, type = 9, sel_gen1 = \"G8\", sel_gen2 = \"G10\")\np2 <- plot(gge_model,\n           type = 9,\n           sel_gen1 = \"G8\",\n           sel_gen2 = \"G10\",\n           col.gen = \"black\",\n           title = FALSE,\n           annotation = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"biplot-tipo-10-relação-entre-os-ambientes","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.10.6.10 Biplot tipo 10: relação entre os ambientes","text":"","code":"\ngge_model <-gge(data_ge, ENV, GEN, GY)\np1 <-plot(gge_model, type = 10)\np2 <-plot(gge_model,\n          type = 10,\n          col.gen = \"black\",\n          title = FALSE,\n          annotation = FALSE)\narrange_ggplot(p1, p2)"},{"path":"interaction.html","id":"modelos-mistos-na-avaliação-de-met","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11 Modelos mistos na avaliação de MET","text":"Assumindo \\(\\mathop\\alpha\\nolimits_i\\) e \\(\\mathop {(\\alpha \\tau )}\\nolimits_{ij}\\) como sendo de efeitos aleatórios, o modelo em  pode ser convenientemente reescrito utilizando o seguinte modelo linear misto:\r\n\\[ \\label{meq}\r\n{\\boldsymbol{y  = X\\beta  + Zu + \\varepsilon }}\r\n\\]onde y é um vetor \\(n[ = \\sum\\nolimits_{j = 1}^e {(gb)]} \\times 1\\), \\({\\boldsymbol{y}} = {\\rm{ }}{\\left[ {{y_{111}},{\\rm{ }}{y_{112}},{\\rm{ }} \\ldots ,{\\rm{ }}{y_{geb}}} \\right]^\\prime }\\); \\(\\boldsymbol{\\beta}\\) é um vetor \\(eb \\times 1\\) de efeitos fixos, \\({\\boldsymbol{\\beta }} = [\\mathop \\gamma \\nolimits_{11} ,\\mathop \\gamma \\nolimits_{12} ,...,\\mathop \\gamma \\nolimits_{eb} ]'\\); u é um vetor \\(m[ = g + ge] \\times 1\\) de efeitos aleatórios, \\({\\boldsymbol{u}} = {\\rm{ }}{\\left[ {{\\alpha _1},{\\alpha _2},...,{\\alpha _g},\\mathop {(\\alpha \\tau )}\\nolimits_{11} ,\\mathop {(\\alpha \\tau )}\\nolimits_{12} ,...,\\mathop {(\\alpha \\tau )}\\nolimits_{ge} } \\right]^\\prime }\\); X é uma matriz delineamento de dimensão \\(n \\times eb\\) relacionando y \\({\\boldsymbol{\\beta }}\\); Z é uma matriz delineamento de dimensão \\(m\\times n\\) relacionando y \\(\\boldsymbol{u}\\); e \\({\\boldsymbol{\\varepsilon }}\\) é um vetor n×1 de erros aleatórios, \\({\\boldsymbol{\\varepsilon }} = {\\rm{ }}{\\left[ {{y_{111}},{\\rm{ }}{y_{112}},{\\rm{ }} \\ldots ,{\\rm{ }}{y_{geb}}} \\right]^\\prime }\\)Os vetores aleatórios \\({\\boldsymbol{\\beta }}\\) e \\(\\boldsymbol{u}\\) são assumidos como normais e independentemente distribuídos com média zero e matrizes de variância-covariância G e R, respectivamente, de tal forma que\\[\r\n\\left[ \\begin{array}{l}{\\boldsymbol{u}}\\\\{\\boldsymbol{\\varepsilon }}\\end{array} \\right]\\sim N\\left( {\\left[ \\begin{array}{l}{\\boldsymbol{0}}\\\\{\\boldsymbol{0}}\\end{array} \\right]{\\boldsymbol{,}}\\left[ {\\begin{array}{*{20}{c}}{\\boldsymbol{G}}&{\\boldsymbol{0}}\\\\{\\boldsymbol{0}}&{\\boldsymbol{R}}\\end{array}} \\right]} \\right)\r\n\\]matriz de variância-covariância G tem muitas formas possíveis. estrutura mais simples -e opção padrão na maioria dos pacotes estatísticos para modelos mistos- são os componentes de variância, de modo que\\[\r\n{\\boldsymbol{G}} = \\left[ {\\begin{array}{*{20}{c}}{\\mathop {\\hat \\sigma }\\nolimits_\\alpha ^2 {{\\boldsymbol{}}_g}}&0\\\\0&{\\mathop {\\hat \\sigma }\\nolimits_{\\alpha \\tau }^2 {{\\boldsymbol{}}_{ge}}}\\end{array}} \\right]\r\n\\]e \\({\\boldsymbol{R}} = \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2 {{\\boldsymbol{}}_n}\\), sendo \\(\\mathop {\\hat \\sigma }\\nolimits_\\alpha ^2\\), \\(\\mathop {\\hat \\sigma }\\nolimits_{\\alpha \\tau }^2\\) e \\(\\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2\\) variâncias para genótipo, interação genótipo-ambiente e erros aleatórios, respectivamente.Os vetores \\({\\boldsymbol{\\beta }}\\) e \\(\\boldsymbol{u}\\) são estimados considerando bem conhecida equação de modelo misto Henderson (1975).\\[ \\label{ad}\r\n\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{\\hat \\beta }}}\\\\{{\\boldsymbol{\\hat u}}}\\end{array}} \\right]{\\boldsymbol{ = }}{\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{\\ - {\\boldsymbol{1}}}}{\\boldsymbol{X}}}&{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{Z}}}\\\\{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{X}}}&{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{Z + }}{{\\boldsymbol{G}}^{ - {\\boldsymbol{1}}}}}\\end{array}} \\right]^ - }\\left[ {\\begin{array}{*{20}{c}}{{\\boldsymbol{X'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{y}}}\\\\{{\\boldsymbol{Z'}}{{\\boldsymbol{R}}^{ - {\\boldsymbol{1}}}}{\\boldsymbol{y}}}\\end{array}} \\right]\r\n\\]Onde o sobescrito \\(^{-1}\\) e \\(^-\\) representam inversas e inversas generalizadas das matrizes, respectivamente. estimativa dos componentes de variância em \\({\\boldsymbol{\\hat G}}\\) e \\({\\boldsymbol{\\hat R}}\\), pode ser realizada sem maiores problemas utilizando ANOVA  convencional quando os dados são balanceados. Quando este pressuposto não é cumprido, estimativa baseada em Restricted Maximum Likelihood (REML)  utilizando o algoritmo Expectation-Maximization (Dempster, Laird, Rubin 1977) é mais indicada.Desde década de 1990, os modelos mistos vêm ganhando cada vez mais espaço na avaliação de MET, pois permitem estimativa de parâmetros genéticos  e ambientais, bem como predição dos valores genotípicos de forma não-viciada (Smith, Cullis, Thompson 2005). Da mesma forma, modelos mistos reduzem os ruídos de análises realizadas com dados desbalanceados e também de variáveis que não assumem aditividade, características frequentemente observadas em MET (Hu 2015). Na avaliação dos dados oriundos de MET é de interesse pesquisador predizer o “verdadeiro” rendimento \\(w_{ij}\\) dado os rendimentos observados \\(y_{ij}\\). Quando um fator é considerado fixo, inferências são limitadas apenas aos níveis testados deste fator e os efeitos são estimados por Best Linear Unbiased Estimator, ou BLUE . Para efeitos aleatórios, onde deseja-se expandir inferências para uma população de tratamentos (ou ambientes), predição é realizada por Best Linear Unbiased Predictor, ou BLUP  (Henderson 1975).\r\nEq. ","code":""},{"path":"interaction.html","id":"ajustando-o-modelo","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.1 Ajustando o modelo","text":"função waasb()  pacote metan será utilizada para análise dos dados de nosso exemplo utilizando o modelo misto descrito acima, considerando como aleatório os efeitos de genótipo e da interação genótipo-vs-ambiente.função get_model_data() pode ser utilizada para extrair importantes informações modelo ajustado com função waasb().Resumo experimentoTeste de razão de máxima verossimilhanças (LRT)O teste LRT  indicou diferenças significativas para os efeitos aleatórios de genótipo e interação genótipo-vs-ambiente. Assim, utilização de modelos mistos é indicada para predição rendimento em nosso exemplo.Componentes da variância","code":"\n\nBLUP_model <- waasb(data_ge, ENV, GEN, REP,\n                    resp = c(GY, HM),\n                    verbose = FALSE)\nget_model_data(BLUP_model, \"details\")\n# # A tibble: 14 x 3\n#    Parameters GY                  HM              \n#    <chr>      <chr>               <chr>           \n#  1 Mean       \"2.67\"              \"48.09\"         \n#  2 SE         \"0.05\"              \"0.21\"          \n#  3 SD         \"0.92\"              \"4.37\"          \n#  4 CV         \"34.56\"             \"9.09\"          \n#  5 Min        \"0.67 (G10 in E11)\" \"38 (G2 in E14)\"\n#  6 Max        \"5.09 (G8 in E5)\"   \"58 (G8 in E11)\"\n#  7 MinENV     \"E11 (1.37)\"        \"E14 (41.03)\"   \n#  8 MaxENV     \"E3 (4.06)\"         \"E11 (54.2)\"    \n#  9 MinGEN     \"G10 (2.47) \"       \"G2 (46.66) \"   \n# 10 MaxGEN     \"G8 (3) \"           \"G5 (49.3) \"    \n# 11 wresp      \"50\"                \"50\"            \n# 12 mresp      \"100\"               \"100\"           \n# 13 Ngen       \"10\"                \"10\"            \n# 14 Nenv       \"14\"                \"14\"\nget_model_data(BLUP_model, \"lrt\")\n# # A tibble: 4 x 8\n#   VAR   model    npar logLik   AIC   LRT    Df `Pr(>Chisq)`\n#   <chr> <chr>   <int>  <dbl> <dbl> <dbl> <dbl>        <dbl>\n# 1 GY    GEN        44  -224.  537. 19.3      1     1.11e- 5\n# 2 GY    GEN:ENV    44  -237.  562. 44.8      1     2.15e-11\n# 3 HM    GEN        44  -867. 1821.  7.86     1     5.07e- 3\n# 4 HM    GEN:ENV    44  -894. 1876. 62.8      1     2.27e-15\nget_model_data(BLUP_model, \"vcomp\")\n# # A tibble: 3 x 3\n#   Group        GY    HM\n#   <chr>     <dbl> <dbl>\n# 1 GEN      0.0280 0.490\n# 2 GEN:ENV  0.0567 2.19 \n# 3 Residual 0.0967 2.84"},{"path":"interaction.html","id":"análise-dos-resíduos-2","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.2 Análise dos resíduos","text":"função autoplot() também pode ser utilizada para investigação dos pressupostos modelo neste exemplo. Dois gráficos que são gerados mas não mostrados por padrão são vistos neste exemplo.","code":"\nplot(BLUP_model)"},{"path":"interaction.html","id":"parâmetros-genéticos","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.3 Parâmetros genéticos","text":"Além dos componentes de variância  para os efeitos aleatórios declarados, alguns importantes parâmetros genéticos  são mostrados. Heribatility é herdabilidade sentido amplo (\\(\\mathop h\\nolimits_g^2\\)) estimada por \\(\\mathop h\\nolimits_g^2 = \\mathop \\sigma \\nolimits_g^2 /\\left( {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 + \\mathop \\sigma \\nolimits_e^2 } \\right)\\) onde \\((\\mathop \\sigma \\nolimits_g^2 )\\) é variância genotípica; \\((\\mathop \\sigma \\nolimits_i^2 )\\) é variância da interação GE; e \\((\\mathop \\sigma \\nolimits_e^2 )\\) é variância residual. GEIr2 é o coeficiente de determinação efeito da interação GE (\\(\\mathop r\\nolimits_i^2\\) ) estimado por \\(\\mathop r\\nolimits_i^2 = \\mathop \\sigma \\nolimits_i^2 /\\left( {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 + \\mathop \\sigma \\nolimits_e^2 } \\right)\\); Heribatility means é herdabilidade da média assumindo ausência de valores perdidos \\((\\mathop h\\nolimits_{gm}^2 )\\), estimada por \\(\\mathop h\\nolimits_{gm}^2 = \\mathop \\sigma \\nolimits_g^2 /\\left[ {\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2 /+ \\mathop \\sigma \\nolimits_e^2 /\\left( {ab} \\right)} \\right]\\), onde e b são o número de ambientes e blocos, respectivamente; Accuracy é acurácia de seleção (Ac), estimada por \\(Ac = \\sqrt{\\mathop h\\nolimits_{gm}^2}\\) ; rge é correlação genótipo-ambiente \\((\\mathop r\\nolimits_{ge})\\) estimada por \\(\\mathop r\\nolimits_{ge} = \\mathop \\sigma \\nolimits_g^2 /\\left({\\mathop \\sigma \\nolimits_g^2 + \\mathop \\sigma \\nolimits_i^2} \\right)\\); CVg é o coeficiente de variação genotípico, estimado por \\({\\left({\\mathop \\sigma \\nolimits_g^2 /\\mu } \\right)^{0.5}} \\times 100\\), onde \\(\\mu\\) é média geral; CVr é o coeficiente de variação residual, estimado por \\({\\left( {\\mathop \\sigma \\nolimits_e^2 /\\mu} \\right)^{0.5}} \\times 100\\) ; CV ratio é razão entre o coeficiente de variação genotípico e residual.","code":"\nget_model_data(BLUP_model, \"genpar\")\n# Class of the model: waasb\n# Variable extracted: genpar\n# # A tibble: 9 x 3\n#   Parameters              GY     HM\n#   <chr>                <dbl>  <dbl>\n# 1 Phenotypic variance  0.181 5.52  \n# 2 Heritability         0.154 0.0887\n# 3 GEIr2                0.313 0.397 \n# 4 h2mg                 0.815 0.686 \n# 5 Accuracy             0.903 0.828 \n# 6 rge                  0.370 0.435 \n# 7 CVg                  6.26  1.46  \n# 8 CVr                 11.6   3.50  \n# 9 CV ratio             0.538 0.415"},{"path":"interaction.html","id":"médias-preditas","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.4 Médias preditas","text":"Imprimindo os BLUPs preditos para genótiposblupGEN mostra média predita para os genótipos testados. BLUPg é o efeito genotípico \\((\\hat{g}_{})\\) estimado por \\(\\hat{g}_{} = h_g^2(\\bar{y}_{.}-\\bar{y}_{..})\\) onde \\(h_g^2\\) é o efeito shrinkage para genótipo, estimado por \\(\\mathop h\\nolimits_g^2 = (\\mathop {\\hat \\sigma }\\nolimits_i^2 + E\\mathop {\\hat \\sigma }\\nolimits_g^2 )/(\\mathop {\\hat \\sigma }\\nolimits_i^2 + \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2 + E\\mathop {\\hat \\sigma }\\nolimits_g^2 ).\\). Predicted é média predita por \\(\\hat{g}_{}+\\mu\\) onde \\(\\mu\\) é média geral. LL e UL são os limites inferiore e superior, respectivamente estimado por \\((\\hat{g}_{}+\\mu)\\pm{CI}\\). \\(CI\\) é o intervalo de confiança para predição BLUP, assumindo uma dada probabilidade de erro, onde \\(CI = t\\times\\sqrt{((1-Ac)\\times{\\mathop \\sigma \\nolimits_g^2)}}\\) onde \\(t\\) é o valor t-Student para um teste bilateral uma dada probabilidade de erro; \\(Ac\\) é acurácia de seleçao; e \\(\\mathop \\sigma \\nolimits_g^2\\) é variância genotípica.plotando os BLUPs preditos para genótipos\r\nFigure 14.4: Médias preditas para genótipos considerando um modelo misto\r\nImprimindo os BLUPs estimados para combinações genótipo x ambiente (primeiras 10 entradas)saída acima os BLUPs  estimados para combinações genótipo x ambiente. BLUPg é o efeito genotípico, descrito acima; BLUPge é o efeito genotípico genótipo ambiente j \\((\\hat{g}_{ij})\\) estimado por \\(\\hat{g}_{ij} = h_g^2(\\bar{y}_{.}-\\bar{y}_{..})+h_{ge}^2(y_{ij}-\\bar{y}_{.}-\\bar{y}_{.j}+\\bar{y}_{..})\\), onde \\(h_{ge}^2\\) é o efeito shrinkage para interação GE, estimado por \\(\\mathop h\\nolimits_{ge}^2 = \\mathop {\\hat \\sigma }\\nolimits_i^2 /(\\mathop {\\hat \\sigma }\\nolimits_i^2 + \\mathop {\\hat \\sigma }\\nolimits_\\varepsilon ^2)\\); BLUPg+ge é \\(BLUP_g+BLUP_{ge}\\); Predicted é média predita (\\(\\hat{y}_{ij}\\)) estimada por \\(\\hat{y}_{ij} = \\bar{y}_{.j}+BLUP_{g+ge}\\).Para obter os valores acima para cada variável modelo basta utilizar função get_model_data(), por exemplo:get_model_data(BLUP_model, \"blupg\") para médias preditas de genótipos;get_model_data(BLUP_model, \"blupge\") para médias preditas de genótipos em ambientes;","code":"\n\nprint(BLUP_model$GY$BLUPgen)\n# # A tibble: 10 x 6\n#     Rank GEN     BLUPg Predicted    LL    UL\n#    <dbl> <fct>   <dbl>     <dbl> <dbl> <dbl>\n#  1     1 G8     0.269       2.94  2.78  3.11\n#  2     2 G3     0.229       2.90  2.74  3.07\n#  3     3 G2     0.0570      2.73  2.57  2.90\n#  4     4 G7     0.0543      2.73  2.56  2.89\n#  5     5 G4    -0.0264      2.65  2.48  2.81\n#  6     6 G1    -0.0575      2.62  2.45  2.78\n#  7     7 G5    -0.112       2.56  2.40  2.73\n#  8     8 G6    -0.114       2.56  2.39  2.73\n#  9     9 G9    -0.134       2.54  2.37  2.71\n# 10    10 G10   -0.166       2.51  2.34  2.67\np1 <-plot_blup(BLUP_model)\np2 <-plot_blup(BLUP_model,\n               col.shape  =  c(\"gray20\", \"gray80\"),\n               y.lab = \"Genótipos\",\n               x.lab = \"Rendimento de grãos predito\") + coord_flip()\n\narrange_ggplot(p1, p2, tag_levels = list(c(\"p1\", \"p2\")))\n\nprint(BLUP_model$GY$BLUPint)\n# # A tibble: 420 x 7\n#    ENV   GEN   REP     BLUPg  BLUPge `BLUPg+ge` Predicted\n#    <fct> <fct> <fct>   <dbl>   <dbl>      <dbl>     <dbl>\n#  1 E1    G1    1     -0.0575 -0.0621    -0.120       2.46\n#  2 E1    G1    2     -0.0575 -0.0621    -0.120       2.44\n#  3 E1    G1    3     -0.0575 -0.0621    -0.120       2.31\n#  4 E1    G2    1      0.0570  0.207      0.264       2.84\n#  5 E1    G2    2      0.0570  0.207      0.264       2.82\n#  6 E1    G2    3      0.0570  0.207      0.264       2.69\n#  7 E1    G3    1      0.229   0.0885     0.318       2.89\n#  8 E1    G3    2      0.229   0.0885     0.318       2.87\n#  9 E1    G3    3      0.229   0.0885     0.318       2.75\n# 10 E1    G4    1     -0.0264  0.0601     0.0337      2.61\n# # ... with 410 more rows"},{"path":"interaction.html","id":"índices-de-estabilidade-baseados-em-blup","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.5 Índices de estabilidade baseados em BLUP","text":"","code":""},{"path":"interaction.html","id":"wsb","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.5.1 O índice WAASB","text":"Recentemente, (Tiago Olivoto, Lúcio, et al. 2019a) propuseram um índice de estabilidade, WAASB (Weighted Average Absolute Scores), que combina características modelo AMMI e BLUP. O índice é baseado na média ponderada dos escores absolutos obtidos pela decomposição por valores singulares da matriz BLUP dos efeitos da interação em um modelo de efeito misto, conforme seguite equação.\\[\r\n        WAASB_i  =\r\n        \\sum_{k = 1}^{p} |IPCA_{ik} \\times EP_k|/ \\sum_{k = 1}^{p}EP_k\r\n\\]onde \\(WAASB_i\\) é média ponderada dos escores absolutos genótipo ; \\(IPCA_{ik}\\) é o escore genótipo k-esimo IPCA; e \\(EP_k\\) é variância explicada pelo k IPCA para \\(k = 1,2, .., p\\), sendo \\(p = min (g-1; e-1)\\). O genótipo mais estável é aquele com o menor valor de WAASB (Tiago Olivoto, Lúcio, et al. 2019a).Devido aplicação da ténica de decomposição por valores singulares, é possível confecção de biplots semelhantes ao método AMMI, considerando agora, um modelo de efeito misto. Para isto, função plot_scores  é utilizada utilizando como argumento de o modelo ajustado BLUP_model. Tiago Olivoto, Lúcio, et al. (2019a) e Tiago Olivoto, Lúcio, et al. (2019b) propuseram utilização índice WAASB na ordenada biplot AMMI1, substituindo os valores IPCA1 pelos valores WAASB. Este biplot é criado utilizando type = 3 na função.\r\nFigure 14.5: Biplot AMMI2 gerado pelo pacote metan\r\nOs quadrantes propostos nesta interpretação representam quatro classificações propostas por Tiago Olivoto, Lúcio, et al. (2019a) em relação à interpretação conjunta da produtividade e estabilidade. Os genótipos ou ambientes incluídos quadrante podem ser considerados genótipos instáveis –ou ambientes com alta capacidade de discriminação, mas com produtividade abaixo da média geral. quadrante II estão incluídos os genótipos instáveis, embora com produtividade acima da média geral. Os ambientes incluídos neste quadrante merecem atenção especial, pois, além de fornecerem altas magnitudes da variável resposta, apresentam boa capacidade de discriminação. Os genótipos dentro quadrante III têm baixa produtividade, mas podem ser considerados estáveis devido aos valores mais baixos WAASB. Quanto menor esse valor, mais estável o genótipo pode ser considerado. Os ambientes incluídos neste quadrante podem ser considerados pouco produtivos e com baixa capacidade de discriminação. Os genótipos dentro quadrante IV são altamente produtivos e estáveis.","code":"\np5 = plot_scores(BLUP_model, type = 3)\np6 = plot_scores(BLUP_model, type = 3) +\n                 theme_gray() +\n                 theme(legend.position = c(0.1, 0.9),\n                       legend.background = element_rect(fill = NA))\n\narrange_ggplot(p5, p6, tag_levels = list(c(\"p5\",\"p6\")))"},{"path":"interaction.html","id":"o-índice-waasby","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.5.2 O índice WAASBY","text":"Um novo índice de superioridade que considera tanto estabilidade quanto produtividade para classificação dos genótipos também foi proposto por Tiago Olivoto, Lúcio, et al. (2019a), considerando seguinte equação.\\[\r\nWAASB{Y_i} = \\frac{{\\left( {r{G_i} \\times {\\theta _Y}} \\right) + \\left( {r{W_i} \\times {\\theta _S}} \\right)}}{{{\\theta _Y} + {\\theta _S}}}\r\n\\]onde \\(WAASBY_i\\) é o índice de superioridade para o genótipo que pondera entre desempenho e estabilidade; \\(rG_i\\) e \\(rW_i\\) são os valores escalonados (0-100) para GY e WAASB, respectivamente; \\(\\theta_Y\\) e \\(\\theta_S\\) são os pesos para GY e WAASB, respectivamente.Este índice permite atribuir pesos para estabilidade e produtividade na classificação dos genótipos. Para isto, o argumento wresp da função waasb()  é usado. Por exemplo, se o objetivo é selecionar genótipos com alto rendimento (independentemente de sua estabilidade), então o wresp = 100 deve ser usado. Nesse caso, classificação índice WAASBY corresponderá perfeitamente à classificação da variável de resposta. Por outro lado, visando selecionar genótipos altamente estáveis (independentemente da produtividade), então o wresp = 0 deve ser usado. Nesse caso, classificação índice WAASBY corresponderá perfeitamente à classificação índice WAASB. Qualquer valor entre 0 e 100 pode ser usado argumento wresp para ponderar entre o desempenho médio e estabilidade. Em nosso exemplo, o índice WAASBY foi calculado considerando wresp = 50 (padrão da função), o que significa pesos iguais para desempenho e estabilidade médios. Para plotar os valores de WAASBY, o código seguir é usado.O reescalonamento da variável resposta e índice WAASB é necessário para que eles sejam diretamente comparáveis. Por padrão, variável resposta é reescalonada de forma que o valor máximo (genótipo com maior média) seja 100 e o valor mínimo (genótipo com menor média) seja 0. Digamos que para uma determinada variável, menores valores são desejados, então o argumento mresp = 100 (padrão) deve ser ajustado para mresp = 0.Par obter o índice WAASBY para múltiplas variáveis, por exemplo, basta utilizar função get_model_data(), como mostrado abaixo.","code":"\np1 <- plot_waasby(BLUP_model)\np2 <- plot_waasby(BLUP_model,\n                  col.shape = c(\"black\", \"gray\")) +\n  coord_flip()\nwaas(data_ge2, ENV, GEN, REP,\n     resp = c(PH, ED, TKW, NKR),\n     wresp = rep(65, 4)) %>%\n get_model_data(what = \"WAASY\")\n# variable PH \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  7.719  2.5728 127.913 4.25e-07          .           .\n#   REP(ENV)   8  0.161  0.0201   0.897 5.22e-01          .           .\n#        GEN  12  1.865  0.1554   6.929 6.89e-09          .           .\n#    GEN:ENV  36  5.397  0.1499   6.686 5.01e-14          .           .\n#        PC1  14  4.466  0.3190  14.230 0.00e+00       82.8        82.8\n#        PC2  12  0.653  0.0545   2.430 8.40e-03       12.1        94.9\n#        PC3  10  0.277  0.0277   1.240 2.76e-01        5.1         100\n#  Residuals  96  2.153  0.0224      NA       NA          .           .\n#      Total 191 22.692  0.1188      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable ED \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  306.0  101.99  43.386 2.70e-05          .           .\n#   REP(ENV)   8   18.8    2.35   0.906 5.15e-01          .           .\n#        GEN  12  212.9   17.74   6.838 8.95e-09          .           .\n#    GEN:ENV  36  398.2   11.06   4.263 7.60e-09          .           .\n#        PC1  14  212.2   15.16   5.840 0.00e+00       53.3        53.3\n#        PC2  12  134.7   11.23   4.330 0.00e+00       33.8        87.1\n#        PC3  10   51.3    5.13   1.980 4.38e-02       12.9         100\n#  Residuals  96  249.1    2.59      NA       NA          .           .\n#      Total 191 1583.2    8.29      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable TKW \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  37013   12338   11.13 3.16e-03          .           .\n#   REP(ENV)   8   8869    1109    1.21 3.03e-01          .           .\n#        GEN  12  44633    3719    4.05 4.41e-05          .           .\n#    GEN:ENV  36 164572    4571    4.98 1.73e-10          .           .\n#        PC1  14 104276    7448    8.11 0.00e+00       63.4        63.4\n#        PC2  12  33361    2780    3.03 1.20e-03       20.3        83.6\n#        PC3  10  26935    2694    2.93 3.00e-03       16.4         100\n#  Residuals  96  88171     918      NA       NA          .           .\n#      Total 191 507829    2659      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# variable NKR \n# ---------------------------------------------------------------------------\n# AMMI analysis table\n# ---------------------------------------------------------------------------\n#     Source  Df Sum Sq Mean Sq F value   Pr(>F) Proportion Accumulated\n#        ENV   3  237.0   79.01  15.843 0.000997          .           .\n#   REP(ENV)   8   39.9    4.99   0.635 0.746348          .           .\n#        GEN  12  227.8   18.99   2.418 0.008726          .           .\n#    GEN:ENV  36  602.7   16.74   2.132 0.001839          .           .\n#        PC1  14  337.4   24.10   3.070 0.000600         56          56\n#        PC2  12  192.2   16.02   2.040 0.028500       31.9        87.9\n#        PC3  10   73.1    7.31   0.930 0.509500       12.1         100\n#  Residuals  96  753.7    7.85      NA       NA          .           .\n#      Total 191 2463.8   12.90      NA       NA       <NA>        <NA>\n# ---------------------------------------------------------------------------\n# \n# All variables with significant (p < 0.05) genotype-vs-environment interaction\n# Done!\n# Class of the model: waas\n# Variable extracted: WAASY\n# # A tibble: 13 x 5\n#    GEN      PH    ED   TKW   NKR\n#    <chr> <dbl> <dbl> <dbl> <dbl>\n#  1 H1     73.6 78.1   84.5  42.7\n#  2 H10    22.0 18.8   34.8  56.3\n#  3 H11    42.5 39.2   56.6  56.3\n#  4 H12    27.5 51.5   41.0  35  \n#  5 H13    49.2 60.2   70.7  25.1\n#  6 H2     66.3 54.9   62.5  51.6\n#  7 H3     59.4 57.9   52.1  48.5\n#  8 H4     68.6 59.4   59.8  98.7\n#  9 H5     85.3 61.6   69.9  75.2\n# 10 H6     67.5 95.7   90.3  34.5\n# 11 H7     41.4 62.8   55.8  42.6\n# 12 H8     11.6 32.2   17.3  29.2\n# 13 H9     45.5  7.93   0    48.4"},{"path":"interaction.html","id":"diferentes-cenários-para-a-estimativa-do-waasby","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.5.3 Diferentes cenários para a estimativa do WAASBY","text":"exemplo seguir, aplicaremos função wsmp()  (weighting stability mean performance) ao modelo previamente ajustado BLUP_model visando planejar diferentes cenários de estimação WAASBY alterando os pesos atribuídos à estabilidade e ao desempenho médio. O número de cenários é definido pelo argumento increment. Por padrão, vinte e um cenários diferentes são simulados. Neste caso, o índice de superioridade WAASBY é calculado considerando os seguintes pesos: estabilidade = 100; desempenho médio = 0. Em outras palavras, apenas estabilidade é considerada para classificação dos genótipos. Na próxima iteração, os pesos se tornam 95/5 (uma vez que increment = 5). terceiro cenário, os pesos se tornam 90/10, e assim por diante até esses pesos se tornarem 0/100. Na última iteração, classificação genótipo para o WAASBY corresponde perfeitamente às classificações da variável resposta.função genérica plot()  é então usada para plotar o objeto. Dois heatmaps são criados. O primeiro tipo mostra classificação genótipo dependendo número de eixos de componentes principais usados para estimar o índice WAASB. Um dendrograma baseado na distância euclidiana é usado para agrupar classificação dos genótipos. O segundo tipo mostra classificação genótipo dependendo da relação WAASB/GY. classificações obtidas cenário 100/0 consideram exclusivamente estabilidade para o ranking de genótipos. Por outro lado, o cenário 0/100 considera exclusivamente produtividade para o ranking de genótipos.","code":"\nscenarios <- wsmp(BLUP_model)\np1 <- plot(scenarios)\np2 <- plot(scenarios, type = 2)\narrange_ggplot(p1, p2, tag_levels = list(c(\"p1\", \"p2\")))"},{"path":"interaction.html","id":"performance-e-média-harmônica-dos-valores-genotípicos","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.11.5.4 Performance e média harmônica dos valores genotípicos","text":"função Resende_indexes() computa os índices Média Harmônica dos Valores Genotípicos (MHVG), Performance Relativa dos Valores Genotípicos (PRVG) e Média Harmônica da Performance Relativa dos Valores Genotipicos (MHPRVG), conforme descrito em Colombari Filho et al. (2013). Estes índices são calculados para cada genótipo de acordo com seguintes equações.\\[\r\nMHVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {\\frac{1}{{G{v_{ij}}}}}\r\n\\]\\[\r\nPRVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {G{v_{ij}}/{\\mu_j}}\r\n\\]\\[\r\nMHPRVG_i = \\frac{1}{e}\\sum\\limits_{j = 1}^e {\\frac{1}{{G{v_{ij}}/{\\mu_j}}}}\r\n\\]onde e é o número de ambientes incluídos na análise, \\(Gv_{ij}\\) é o valor genotípico (BLUP) para o genótipo ambiente j.","code":"\nIndexes <- Resende_indexes(BLUP_model)\nprint(Indexes$GY)\n# # A tibble: 10 x 10\n#    GEN       Y  HMGV HMGV_R  RPGV RPGV_Y RPGV_R HMRPGV HMRPGV_Y HMRPGV_R\n#    <fct> <dbl> <dbl>  <dbl> <dbl>  <dbl>  <dbl>  <dbl>    <dbl>    <dbl>\n#  1 G1     2.60  2.32      6 0.969   2.59      6  0.967     2.59        6\n#  2 G10    2.47  2.11     10 0.913   2.44     10  0.896     2.40       10\n#  3 G2     2.74  2.47      4 1.03    2.74      4  1.02      2.73        4\n#  4 G3     2.96  2.68      2 1.11    2.96      2  1.10      2.95        2\n#  5 G4     2.64  2.39      5 0.990   2.65      5  0.988     2.64        5\n#  6 G5     2.54  2.30      8 0.954   2.55      7  0.952     2.55        8\n#  7 G6     2.53  2.30      7 0.954   2.55      8  0.952     2.55        7\n#  8 G7     2.74  2.52      3 1.04    2.77      3  1.03      2.76        3\n#  9 G8     3.00  2.74      1 1.13    3.01      1  1.12      3.00        1\n# 10 G9     2.51  2.18      9 0.926   2.48      9  0.917     2.45        9"},{"path":"interaction.html","id":"ammi-ou-blup-decisão-em-cada-caso","chapter":"Capítulo 14 Interação genótipo-vs-ambiente","heading":"14.12 AMMI ou BLUP? Decisão em cada caso!","text":"Vimos anteriormente que um membro da família de modelos AMMI (AMMI2) é o mais preciso na predição da variável resposta em nosso exemplo. Após analizar-mos os mesmos dados utilizando modelos mistos, nos surge seguinte questão. Qual é o método mais preciso para predizer variável resposta em nosso exemplo? resposta esta pergunda será dada agora. O pacote metan também conta com uma função para cross-validation considerando o modelo misto acima. idéia é simples. Relizaremos uma validação cruzada semelhante àquela realizada modelo AMMI e compararemos o RMSPD dos valores preditos utilizando BLUP com aqueles obtidos pelo modelo AMMI.função cv_blup() realiza uma validação cruzada para experimentos com repetição usando modelos mistos. Por padrão, blocos completos são selecionados aleatoriamente em cada ambiente. O procedimento para calcular o RMSPD é idêntico ao apresentado na análise AMMI.Para unir os resultados obtidos pelas funções cv_ammif() e cv_blup() função bind_cv(). Usando o argumento bind = \"means\" é possível concatenar os valores médios obtidos por cada modelo.   Um gráfico boxplot também pode ser obtido utilizando o argumento bind = \"boot\" (padrão).","code":"\nvalida_blup <- cv_blup(data_ge, ENV, GEN, REP, GY, nboot = 20)\nval_means <- bind_cv(AMMIF, valida_blup, bind = \"means\")\nval_means$RMSPD\n# # A tibble: 11 x 6\n#    MODEL        mean     sd      se  Q2.5 Q97.5\n#    <fct>       <dbl>  <dbl>   <dbl> <dbl> <dbl>\n#  1 BLUP_g_RCBD 0.408 0.0232 0.00518 0.370 0.439\n#  2 AMMI2       0.411 0.0242 0.00171 0.369 0.460\n#  3 AMMI4       0.416 0.0234 0.00166 0.373 0.462\n#  4 AMMI3       0.416 0.0215 0.00152 0.377 0.457\n#  5 AMMI5       0.422 0.0232 0.00164 0.377 0.464\n#  6 AMMI6       0.422 0.0219 0.00155 0.381 0.462\n#  7 AMMI7       0.425 0.0199 0.00141 0.381 0.458\n#  8 AMMI8       0.427 0.0204 0.00145 0.387 0.462\n#  9 AMMIF       0.429 0.0213 0.00150 0.390 0.469\n# 10 AMMI1       0.430 0.0251 0.00178 0.384 0.479\n# 11 AMMI0       0.430 0.0283 0.00200 0.370 0.485\nval_plot <- bind_cv(AMMIF, valida_blup)\np1 <-plot(val_plot)\np2 <-plot(val_plot,\n          width.boxplot = 0.6,\n          col.boxplot = \"cyan\")\narrange_ggplot(p1, p2)"},{"path":"referências.html","id":"referências","chapter":"Capítulo 15 Referências","heading":"Capítulo 15 Referências","text":"Altman, N., M. Krzywinski. 2017. “Interpreting P values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.Anderson, T. W. 2003. introduction multivariate statistical analysis. 3rd ed. Wiley-Interscience.Annicchiarico, P. 1992. “Cultivar adaptation recommendation alfalfa trials Northern Italy.” Journal Genetics Breeding 46: 269–78.Baker, Monya. 2016. “Statisticians issue warning misuse P values.” Nature 531 (7593): 151–51. https://doi.org/10.1038/nature.2016.19503.Bartlett, M. S. 1947. “Use Transformations.” Biometrics 3 (1): 39–52. https://doi.org/10.2307/3001536.Bates, D. M., D. G. Watts. 1988. Nonlinear Regression Analysis Applications. 2nd ed. Wiley Series Probability Statistics. Hoboken, NJ, USA: John Wiley & Sons, Inc. https://doi.org/10.1002/9780470316757.Blalock, H. M. 1963. “Correlated independent variables: problem multicollinearity.” Social Forces 42 (2): 233–37. https://doi.org/10.1093/sf/42.2.233.Blanca, M. J., R. Alarcón, J. Arnau, R. Bono, R. Bendayan. 2017. “Non-normal data: ANOVA still valid option?” Psicothema 29 (4): 552–57. https://doi.org/10.7334/psicothema2016.383.Box, G. E. P., D. R. Cox. 1964. “Analysis Transformations.” Journal Royal Statistical Society. Series B (Methodological) 211-252: 211–52. https://doi.org/10.2307/2984418.Breslow, N. E., D. G. Clayton. 1993. “Approximate Inference Generalized Linear Mixed Models.” Journal American Statistical Association 88 (421): 9–25. https://doi.org/10.2307/2290687.Casella, George. 2008. Statistical Design. Springer Texts Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-75965-4.Charrad, Malika, Nadia Ghazzali, Véronique Boiteau, Azam Niknafs. 2014. “<b>NbClust<\/b> : <>R<\/> Package Determining Relevant Number Clusters Data Set.” Journal Statistical Software 61 (6): 1–36. https://doi.org/10.18637/jss.v061.i06.Chawla, D. S. 2017. “Big names statistics want shake much-maligned P value.” Nature 548 (7665): 16–17. https://doi.org/10.1038/nature.2017.22375.Cochran, W. G. 1940. “Analysis Variance Experimental Errors Follow Poisson Binomial Laws.” Annals Mathematical Statistics 11 (3): 335–47. https://doi.org/10.1214/aoms/1177731871.Colombari Filho, J. M., M. D. V. Resende, O. P. Morais, . P. Castro, É. P. Guimarães, J. . Pereira, M. M. Utumi, F. Breseghello. 2013. “Upland rice breeding Brazil: simultaneous genotypic evaluation stability, adaptability grain yield.” Euphytica 192 (1): 117–29. https://doi.org/10.1007/s10681-013-0922-2.Dempster, . P., N. M. Laird, D. B. Rubin. 1977. “Maximum likelihood incomplete data via EM algorithm.” Journal Royal Statistical Society, Series B 39 (1): 1–38. https://www.jstor.org/stable/2984875.Draper, Norman R, Harry Smith. 1998. Applied regression analysis. John Wiley & Sons. https://books.google.com.br/books?hl=pt-BR{\\&}lr={\\&}id=uSReBAAAQBAJ{\\&}oi=fnd{\\&}pg=PT12{\\&}ots=P9bwDDSasT{\\&}sig=4RbKqw-ZEgE40xtra-EsI9dLDPk.Eberhart, S. ., W. . Russell. 1966. “Stability parameters comparing varieties.” Crop Science 6 (1): 36–40. https://doi.org/10.2135/cropsci1966.0011183X000600010011x.Eisenhart, C. 1947. “assumptions underlying analysis variance.” Biometrics 3 (1): 1–21. http://www.ncbi.nlm.nih.gov/pubmed/20240414.Farshadfar, E. 2008. “Incorporation AMMI stability value grain yield single non-parametric index (GSI) bread wheat.” Pakistan Journal Biological Sciences 11 (14): 1791–6. https://doi.org/10.3923/pjbs.2008.1791.1796.Ferreira, D. F. 2009. Estatistica Basica. Viçosa, MG.: UFV.Ferreira, E. B., P. P. Cavalcanti, D. . Nogueira. 2018. “ExpDes: Experimental Designs.” https://cran.r-project.org/web/packages/ExpDes/index.html.Field, ., J. Miles, Z. Field. 2012. Discovering Statistics Using R. SAGE Publications Ltd. https://us.sagepub.com/en-us/sam/discovering-statistics-using-r/book236067.Fisher, R. . 1925. Statistical methods research workers. 11th ed. Edinburgh: Oliver; Boyd.———. 1935. design experiments. Edinburgh: Oliver; Boyd.Fisher, R. ., W. . Mackenzie. 1923. “Studies crop variation. II. manurial response different potato varieties.” Journal Agricultural Science 13 (03): 311–20. https://doi.org/10.1017/S0021859600003592.Friedman, M. 1937. “Use Ranks Avoid Assumption Normality Implicit Analysis Variance.” Journal American Statistical Association 32 (200): 675–701. https://doi.org/10.2307/2279372.Gabriel, K. R. 1971. “biplot graphic display matrices application principal component analysis.” Biometrika 58 (3): 453–67. https://doi.org/10.2307/2334381.Galton, Francis. 1888. “Co-relations measurement, chiefly anthropometric data.” Proceedings Royal Society London 45 (273-279): 135–45. http://rspl.royalsocietypublishing.org/content/45/273-279/135.full.pdf.Gauch, H. G. 1988. “Model selection validation yield trials interaction.” Biometrics 44 (3): 705–15. https://doi.org/10.2307/2531585.Gollob, H. F. 1968. “statistical model combines features factor analytic analysis variance techniques.” Psychometrika 33 (1): 73–115. https://doi.org/10.1007/BF02289676.Graham, Michael H. 2003. “Confronting Multicollinearity Ecological Multiple Regression.” Ecology 84 (11): 2809–15. https://doi.org/10.1890/02-3114.Halkidi, Maria, Yannis Batistakis, Michalis Vazirgiannis. 2001. “Clustering Validation Techniques.” Journal Intelligent Information Systems 17 (2/3): 107–45. https://doi.org/10.1023/:1012801612483.Hartigan, J. ., M. . Wong. 1979. “Algorithm 136: K-Means Clustering Algorithm.” Applied Statistics 28 (1): 100–108. https://doi.org/10.2307/2346830.Henderson, C. R. 1949. “Estimation changes herd environment.” Journal Dairy Science 32: 706.———. 1950. “Estimation genetic parameters.” Annals Mathematical Statistics 21: 309–10.———. 1975. “Best linear unbiased estimation prediction selection model.” Biometrics 31 (2): 423–47. https://doi.org/10.2307/2529430.———. 1953. “Estimation Variance Covariance Components.” Biometrics 9 (2): 226–52. https://doi.org/10.2307/3001853.Hoerl, Arthur E, Robert W Kennard. 1976. “Ridge regression iterative estimation biasing parameter.” Communications Statistics - Theory Methods 5 (1): 77–88. https://doi.org/10.1080/03610927608827333.Hoerl, Arthur E., Robert W. Kennard. 1970. “Ridge Regression: Biased Estimation Nonorthogonal Problems.” Technometrics 12 (1): 55–67. https://doi.org/10.1080/00401706.1970.10488634.Hu, X. 2015. “comprehensive comparison ANOVA BLUP valuate location-specific genotype effects rape cultivar trials random locations.” Field Crops Research 179: 144–49. https://doi.org/10.1016/j.fcr.2015.04.023.Hubert, Lawrence, Phipps Arabie. 1985. “Comparing partitions.” Journal Classification 2 (1): 193–218. https://doi.org/10.1007/BF01908075.Kaiser, Henry F. 1961. “Note Guttman’s Lower Bound Number Common Factors.” British Journal Statistical Psychology 14 (1): 1–2. https://doi.org/10.1111/j.2044-8317.1961.tb00061.x.Koopman, B. O. 1936. “distributions admitting sufficient statistic.” Transactions American Mathematical Society 39 (3): 399–409. https://doi.org/10.1090/S0002-9947-1936-1501854-3.Kozak, M., H.-P. Piepho. 2017. “’s normal anyway? Residual plots telling significance tests checking ANOVA assumptions.” Journal Agronomy Crop Science 204: 86–98. https://doi.org/10.1111/jac.12220.Kruskal, W. H., W. . Wallis. 1952. “Use Ranks One-Criterion Variance Analysis.” Journal American Statistical Association 47 (260): 583–621. https://doi.org/10.2307/2280779.Krzanowski, W. J., Y. T. Lai. 1988. “Criterion Determining Number Groups Data Set Using Sum--Squares Clustering.” Biometrics 44 (1): 23–34. https://doi.org/10.2307/2531893.Krzywinski, M., N. Altman. 2013. “Significance, P values t-tests.” Nature Methods 10 (11): 1041–2. https://doi.org/10.1038/nmeth.2698.Kutner, Michael H., Chris Nachtsheim, John Neter, William Li. 2005. Applied linear statistical models.Langsrud, Ø. 2003. “ANOVA unbalanced data: Use Type II instead Type III sums squares.” Statistics Computing 13 (2): 163–67. https://doi.org/10.1023/:1023260610025.Laurent, R., P. Turk. 2013. “Effects Misconceptions Properties Friedman’s Test.” Communications Statistics - Simulation Computation 42 (7): 1596–1615. https://doi.org/10.1080/03610918.2012.671874.Lin, C. S., M. R. Binns. 1988. “superiority measure cultivar performance cultivar x location data.” Canadian Journal Plant Science 68 (1): 193–98. https://doi.org/10.4141/cjps88-018.Lucio, Alessandro Dal Col, Luis F Nunes, Francisco Rego. 2016. “Nonlinear regression plot size estimate green beans production.” Horticultura Brasileira 34 (4): 507–13. https://doi.org/10.1590/s0102-053620160409.Lúcio, Alessandro Dal Col, Luis Filipe Nunes, Francisco Rego. 2015. “Nonlinear models describe production fruit Cucurbita pepo Capiscum annuum.” Scientia Horticulturae 193: 286–93. https://doi.org/10.1016/j.scienta.2015.07.021.Lúcio, Alessandro D., Daniel Santos, Tiago Olivoto. 2017. “Variability Experimental Desing Trials Cucurbita pepo Capsicum annuum.” Journal Agricultural Science 9 (11): 58–75. https://doi.org/10.5539/jas.v9n11p58.Mayer, ., B. Nagengast, J. Fletcher, R. Steyer. 2014. “Analyzing average conditional effects multigroup multilevel structural equation models.” Frontiers Psychology 5 (304): 1–16. https://doi.org/10.3389/fpsyg.2014.00304.Miller, G. ., J. P. Chapman. 2001. “Misunderstanding analysis covariance.” Journal Abnormal Psychology 110 (1): 40–48. http://www.ncbi.nlm.nih.gov/pubmed/11261398.Milligan, Glenn W., Martha C. Cooper. 1985. “examination procedures determining number clusters data set.” Psychometrika 50 (2): 159–79. https://doi.org/10.1007/BF02294245.Mojena, R. 1977. “Hierarchical grouping methods stopping rules: evaluation.” Computer Journal 20 (4): 359–63. https://doi.org/10.1093/comjnl/20.4.359.Mora, F., L. M. Goncalves, C. . Scapim, E. N. Martins, M. F. P. S. Machado. 2008. “Generalized lineal models analysis binary data propagation experiments Brazilian orchids.” Brazilian Archives Biology Technology 51 (5): 963–70. https://doi.org/10.1590/S1516-89132008000500013.Murakami, D. M., C. D. Cruz. 2004. “Proposal methodologies environment stratification analysis genotype adaptability.” Crop Breeding Applied Biotechnology 4 (1): 7–11. https://doi.org/10.12702/1984-7033.v04n01a02.Nelder, J. ., R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal Royal Statistical Society. Series (General) 135 (3): 370–84. https://doi.org/10.2307/2344614.Niles, Henry E. 1922. “Correlation, Causation Wright’s Theory \"Path Coefficients\".” Genetics 7 (3): 258–73. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1200533/.Nuzzo, Regina. 2014. “Scientific method: Statistical errors.” Nature 506 (7487): 150–52. https://doi.org/10.1038/506150a.Olivoto, Tiago, Alessandro Dal’Col Lúcio. 2020. “metan: R package multi‐environment trial analysis.” Methods Ecology Evolution 11 (6): 783–89. https://doi.org/10.1111/2041-210X.13384.Olivoto, Tiago, Alessandro D. C. Lúcio, José . G. Silva, Volmir S. Marchioro, Velci Q. Souza, Evandro Jost. 2019a. “Mean Performance Stability Multi‐Environment Trials : Combining Features AMMI BLUP Techniques.” Agronomy Journal 111 (6): 2949–60. https://doi.org/10.2134/agronj2019.03.0220.Olivoto, Tiago, Alessandro D. C Lúcio, José . G. Silva, Bruno G. Sari, Maria . Diel. 2019b. “Mean Performance Stability Multi‐Environment Trials II: Selection Based Multiple Traits.” Agronomy Journal 111 (6): 2961–9. https://doi.org/10.2134/agronj2019.03.0221.Olivoto, Tiago, Alessandro D. C Lúcio, Velci Q. de Souza, Maicon Nardino, Maria . Diel, Bruno G. Sari, Dionatan .K. Krysczun, Daniela Meira, Carine Meier. 2018. “Confidence Interval Width Pearson’s Correlation Coefficient: Gaussian-Independent Estimator Based Sample Size Strength Association.” Agronomy Journal 110 (2): 503–10. https://doi.org/10.2134/agronj2017.09.0566.Olivoto, Tiago, Maicon Nardino, Ivan Ivan Ricardo Carvalho, Diego Nicolau Follmann, M. Ferrari, Alan J. de Pelegrin, V. Jardel Szareski, Antônio Costa de Oliveira, Braulio Otomar Caron, Velci Queiroz de Souza. 2017. “Optimal sample size data arrangement method estimating correlation matrices lesser collinearity: statistical focus maize breeding.” African Journal Agricultural Research 12 (2): 93–103. https://doi.org/10.5897/AJAR2016.11799.Olivoto, T., M. Nardino, . R. Carvalho, D. N. Follmann, M. Ferrari, V. J. Szareski, . J. de Pelegrin, V. Q. de Souza. 2017. “REML/BLUP sequential path analysis estimating genotypic values interrelationships among simple maize grain yield-related traits.” Genetics Molecular Research 16 (1): gmr16019525. https://doi.org/10.4238/gmr16019525.Olivoto, T., V. Q. Souza, M. Nardino, . R. Carvalho, M. Ferrari, . J. Pelegrin, V. J. Szareski, D. Schmidt. 2017. “Multicollinearity path analysis: simple method reduce effects.” Agronomy Journal 109 (1): 131–42. https://doi.org/10.2134/agronj2016.04.0196.Patterson, H. D., R. Thompson. 1971. “Recovery Inter-Block Information Block Sizes Unequal.” Biometrika 58 (3): 545–54. https://doi.org/10.2307/2334389.Pearson, K. 1920. “Notes History Correlation.” Biometrika 13 (1): 25–45. https://doi.org/10.2307/2331722.Piepho, H. P., R. N. Edmondson. 2018. “tutorial statistical analysis factorial experiments qualitative quantitative treatment factor levels.” Journal Agronomy Crop Science 204 (5): 429–55. https://doi.org/10.1111/jac.12267.Purchase, J. L., H. Hatting, C. S. van Deventer. 2000. “Genotype × environment interaction winter wheat (Triticum aestivum L.) South Africa: II. Stability analysis yield performance.” South African Journal Plant Soil 17 (3): 101–7. https://doi.org/10.1080/02571862.2000.10634878.Rencher, Alvin C., G. Bruce. Schaalje. 2008. Linear models statistics. John Wiley & Sons.Rodrigues-Soares, J. P, G. F. . Jesus, E. L. T. Gonçalves, K.TN. Moraes, E. C. Chagas, F. C. M. Chaves, M. . . Belo, Adolfo Jatobá, J. L. P. Mouriño, M. L. Martins. 2018. “Induced aerocystitis hemato-immunological parameters Nile tilapia fed supplemented diet essential oil Lippia alba.” Brazilian Journal Veterinary Research Animal Science 55 (1): 1–12. https://doi.org/10.11606/issn.1678-4456.bjvras.2018.136717.Rutherford, . 2001. Introducing ANOVA ANCOVA : GLM approach. London: SAGE.Sari, B. G. 2018. “Parametros biologicos da producao de tomateiro via modelo logistico.” PhD thesis, Universidade Federal de Santa Maria.Scheiner, S. M., J. Gurevitch. 2001. Design analysis ecological experiments. 2nd ed. New York: Oxford University Press.Schenider, P. R., P. S. P. Schenider, C. . M. Souza. 2009. Analise de regressao aplicada engenharia florestal. Santa Maria: FACOS, UFSM.Scott, . J., M. J. Symons. 1971. “Clustering Methods Based Likelihood Ratio Criteria.” Biometrics 27 (2): 387–97. https://doi.org/10.2307/2529003.Seber, G. . F., C. J. Wild. 2003. Nonlinear regression. John Wiley & Sons, Inc. https://www.wiley.com/en-us/Nonlinear+Regression-p-9780471471356.Senoglu, Birdal, Moti L. Tiku. 2001. “Analysis variance experimental design nonnormal error distributions.” Communications Statistics - Theory Methods 30 (7): 1335–52. https://doi.org/10.1081/STA-100104748.Silverman, B. W. 1998. Density Estimation Statistics Data Analysis. New York: Routledge. https://doi.org/10.1201/9781315140919.Smith, . B., B. R. Cullis, R. Thompson. 2005. “analysis crop cultivar breeding evaluation trials: overview current mixed model approaches.” Journal Agricultural Science 143 (06): 449–62. https://doi.org/10.1017/S0021859605005587.Snedecor, G. W., W. G. Cochran. 1967. Statistical methods. 6th ed. Ames: Iowa State University Press.Sneller, C. H., L. Kilgore-Norquest, D. Dombek. 1997. “Repeatability yield stability statistics soybean.” Crop Science 37 (2): 383–90. https://doi.org/10.2135/cropsci1997.0011183X003700020013x.Stevens, James (James Paul). 2009. Applied multivariate statistics social sciences. Routledge.Stroup, W. W. 2013. Generalized linear mixed models : modern concepts, methods applications. Boca Raton,FL.: CRC Press.———. 2015. “Rethinking Analysis Non-Normal Data Plant Soil Science.” Agronomy Journal 107 (2): 811–27. https://doi.org/10.2134/agronj2013.0342.Suzuki, R., H. Shimodaira. 2006. “Pvclust: R package assessing uncertainty hierarchical clustering.” Bioinformatics 22 (12): 1540–2. https://doi.org/10.1093/bioinformatics/btl117.Wickham, Hadley. 2009. Ggplot2 : elegant graphics data analysis. Springer. https://doi.org/10.1007/978-0-387-98141-3.Wilkinson, L. 2005. grammar graphics. Springer.Wolfinger, R., M. O’connell. 1993. “Generalized linear mixed models pseudo-likelihood approach.” Journal Statistical Computation Simulation 48 (3-4): 233–43. https://doi.org/10.1080/00949659308811554.Wright, Sewall. 1921. “Correlation causation.” Journal Agricultural Research 20 (7): 557–85. http://www.ssc.wisc.edu/soc/class/soc952/Wright/Wright{\\_}Correlation Causation.pdf.———. 1923. “Theory Path Coefficients Reply Niles’s Criticism.” Genetics 8 (3): 239–55. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1200747/.Yan, Weikai. 2002. “Singular-Value Partitioning Biplot Analysis Multienvironment Trial Data.” Agronomy Journal 94 (5): 990–96. https://doi.org/10.2134/agronj2002.0990.Yan, Weikai., Manjit S. Kang. 2003. GGE biplot analysis: graphical tool breeders, geneticists, agronomists. CRC Press.Yan, W., M. S. Kang, B. Ma, S. Woods, P. L. Cornelius. 2007. “GGE Biplot vs. AMMI analysis genotype--environment data.” Crop Science 47 (2): 641–53. https://doi.org/10.2135/cropsci2006.06.0374.Yates, F. 1940. “recovery inter-block information balanced incomplete block designs.” Annals Eugenics 10 (1): 317–25. https://doi.org/10.1111/j.1469-1809.1940.tb02257.x.Zali, H., E. Farshadfar, S. H. Sabaghpour, R. Karimizadeh. 2012. “Evaluation genotype × environment interaction chickpea using measures stability AMMI model.” Annals Biological Research 3 (7): 3126–36. http://eprints.icrisat.ac./7173/.Zoz, T, F Steiner, Zoz, D. D. Castagnara, T. W. Witt, M. D. Zanotto, D. L. Auld. 2018. “Effect row spacing plant density grain yield yield components Crambe abyssinica Hochst.” Semina: Ciencias Agrarias 39 (1): 393–402. https://doi.org/10.5433/1679-0359.2018v39n1p393.","code":""},{"path":"resposta-dos-exercícios.html","id":"resposta-dos-exercícios","chapter":"A Resposta dos exercícios","heading":"A Resposta dos exercícios","text":"","code":""},{"path":"resposta-dos-exercícios.html","id":"exerc1","chapter":"A Resposta dos exercícios","heading":"A.1 Exercício 1","text":"O resultado foi o mesmo, pois embora se tenha invertido o valor dos números, segundo exemplo se declarou qual argumento o numero pertencial.Pois o argumento (x > 10) faz com que ocorra um erro e função não seja executada.O argumento ‘eleva’ não está correto. Ele deve ser ou ‘quadrado’ ou ‘cubo’.","code":"\nF2(2, 3)\n# [1] 8\nF2(y = 3, x =2)\n# [1] 8\nF3(20)\n# Error in F3(20): O argumento x = 20 é inválido. 'x' precisa ser maior que 10\nelevar(12, eleva = \"cubico\")\n# Error in elevar(12, eleva = \"cubico\"): O argumento eleva = cubico deve ser ou 'quadrado' ou 'cubo'\nmega = function(jogos, numeros = 6){\n  if(!numeros %in% c(6:15)){\n    stop(\"O numero deve ser entre 6 e 15\")\n  }\n  result = list()\nfor(i in 1:jogos){\nresult[[i]] = sort(\n  sample(1:60, size = numeros, replace = FALSE)\n  )\n}\n  return(do.call(rbind, result))\n}\n\n# 4 jogos\nmega(5, 10)\n#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n# [1,]    1    3    8   13   22   27   33   40   54    59\n# [2,]    6   16   21   22   38   39   41   42   51    53\n# [3,]    2    4   11   13   25   26   27   28   36    49\n# [4,]   10   12   21   22   32   35   46   47   59    60\n# [5,]    1    2    6   10   21   28   33   35   46    60"},{"path":"resposta-dos-exercícios.html","id":"exerc2","chapter":"A Resposta dos exercícios","heading":"A.2 Exercício 2","text":"","code":"\nmaize %>%\n  mutate(MGRA_kg = MGRA / 1000) %>%\n  select(AMB, HIB, REP, MGRA_kg) %>%\n  top_n(5, MGRA_kg)"},{"path":"resposta-dos-exercícios.html","id":"exerc3","chapter":"A Resposta dos exercícios","heading":"A.3 Exercício 3","text":"","code":"\nmaize %>%\n  group_by(HIB) %>%\n  summarise(MGRA_mean = mean(MGRA)) %>%\n  mutate(Rank = rank(MGRA_mean)) %>%\n  arrange(Rank)\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc4","chapter":"A Resposta dos exercícios","heading":"A.4 Exercício 4","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH, colour = AMB, size = APLA)) +\ngeom_point()\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc5","chapter":"A Resposta dos exercícios","heading":"A.5 Exercício 5","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH, colour = GEN)) +\n     geom_point() +\n     facet_wrap(~AMB)+\n     my_theme()\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc6","chapter":"A Resposta dos exercícios","heading":"A.6 Exercício 6","text":"","code":"\nggplot(dados_gg, aes(x = RG, y = PH)) +\n     geom_point(aes(colour = AMB))+\n     geom_smooth(method = \"lm\", se = F)+\n     my_theme()+\n     labs(x = \"Rendimento de grãos\", y = \"Peso hectolitro\")\n  "},{"path":"resposta-dos-exercícios.html","id":"exerc7","chapter":"A Resposta dos exercícios","heading":"A.7 Exercício 7","text":"","code":"\nmeans = qualitativo %>% \n  group_by(HIBRIDO) %>% \n  summarise(RG = mean(RG)) %>%\n  mutate(letras = \"a\")\nggplot(means, aes(x = HIBRIDO, y = RG)) +\n  geom_bar(stat = \"identity\", col = \"black\", fill = \"orange\", width = 0.5)+\n  scale_y_continuous(expand = expand_scale(c(0, .1)))+\n  geom_text(aes(label = letras), hjust = -1, size = 3.5)+\n  geom_hline(yintercept = mean(means$RG), linetype = \"dashed\")+\n  coord_flip()"},{"path":"resposta-dos-exercícios.html","id":"exerc8","chapter":"A Resposta dos exercícios","heading":"A.8 Exercício 8","text":"","code":"\nplot_lines(quantitativo, x = DOSEN, y = RG, fit = 2, col = F)"},{"path":"resposta-dos-exercícios.html","id":"exerc9","chapter":"A Resposta dos exercícios","heading":"A.9 Exercício 9","text":"","code":"\nwith(quantitativo, dbc(DOSEN, BLOCO, RG, quali = FALSE))"},{"path":"resposta-dos-exercícios.html","id":"exerc10","chapter":"A Resposta dos exercícios","heading":"A.10 Exercício 10","text":"","code":"\nres = tibble(Convencional = residuals(convencional),\n             Transformado = residuals(transform),\n             Generalizado = residuals(general, type = \"deviance\"))\nshapiro.test(res$Convencional)\nshapiro.test(res$Transformado)\nshapiro.test(res$Generalizado)"},{"path":"resposta-dos-exercícios.html","id":"exerc11","chapter":"A Resposta dos exercícios","heading":"A.11 Exercício 11","text":"","code":"\nplot_factbars(FAT1_CI,\n              HIBRIDO,\n              FONTEN,\n              resp = RG,\n              palette = \"Greys\")"},{"path":"resposta-dos-exercícios.html","id":"exerc12","chapter":"A Resposta dos exercícios","heading":"A.12 Exercício 12","text":"","code":"\nNUPEC_1 <- \n  FAT2_CI %>%\n  filter(HIBRIDO == \"NUPEC_1\")\nggplot(NUPEC_1, aes(x = DOSEN, y = RG)) +\ngeom_point()+\nstat_smooth(method = \"lm\", formula = as.formula(\"y ~ poly(x, 2)\")) +\ngeom_vline(xintercept = 50, linetype = \"dashed\", col = \"gray\") +\ngeom_vline(xintercept = 48, col = \"gray\")"},{"path":"resposta-dos-exercícios.html","id":"exerc13","chapter":"A Resposta dos exercícios","heading":"A.13 Exercício 13","text":"","code":"\ncovar_mat = maize %>%\n  split_factors(ENV, keep_factors = TRUE) %>%\n  covcor_design(gen = GEN,\n                rep = REP,\n                resp = c(PH, EH, NKE, TKW),\n                type = \"rcov\")"},{"path":"tabela-de-distribuições.html","id":"tabela-de-distribuições","chapter":"B Tabela de distribuições","heading":"B Tabela de distribuições","text":"","code":""},{"path":"tabela-de-distribuições.html","id":"distribuição-f","chapter":"B Tabela de distribuições","heading":"B.1 Distribuição F","text":"Tabela 1. Limite unilateral da cauda direita da distribuição F de Fisher-Snedecor, 0,01 de probabilidade de erro\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 2. Limite unilateral da cauda direita da distribuição F de Fisher-Snedecor, 0,05 de probabilidade de erro\\(~\\)\r\n\\(~\\)\r\n\\(~\\)","code":""},{"path":"tabela-de-distribuições.html","id":"tabela-para-o-teste-tukey","chapter":"B Tabela de distribuições","heading":"B.2 Tabela para o teste Tukey","text":"Tabela 3. Valores da estatística q para teste de Tukey\\(~\\)\r\n\\(~\\)\r\n\\(~\\)","code":""},{"path":"tabela-de-distribuições.html","id":"distribuição-t","chapter":"B Tabela de distribuições","heading":"B.3 Distribuição t","text":"Tabela 4. Valores críticos da distribuição t de Student bicaudal em diferentes probabilidades\\(~\\)\r\n\\(~\\)\r\n\\(~\\)","code":""},{"path":"tabela-de-distribuições.html","id":"distribuição-normal-padrão","chapter":"B Tabela de distribuições","heading":"B.4 Distribuição normal padrão","text":"Tabela 5. Valores da distribuição normal padrão. Primeira decimal de Z nas linhas e segunda decimal de Z nas colunas\\(~\\)\r\n\\(~\\)\r\n\\(~\\)","code":""},{"path":"tabela-de-distribuições.html","id":"distribuição-qui-quadrado","chapter":"B Tabela de distribuições","heading":"B.5 Distribuição qui-quadrado","text":"Tabela 6. Valores críticos (função inversa) em relação cauda esquerda da distribuição Chi-quadrado\\(~\\)\r\n\\(~\\)\r\n\\(~\\)Tabela 7. Valores críticos (função inversa) em relação cauda direita da distribuição Chi-quadrado","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
